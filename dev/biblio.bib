@book{aitchison2003,
  title = {The {{Statistical Analysis}} of {{Compositional Data}}},
  author = {Aitchison, J.},
  year = {2003},
  publisher = {{Blackburn Press}},
  address = {{Caldwell, NJ}},
  abstract = {This book was originally published in 1986. It is reprinted here with a new foreword, extensive postscript detailing developments in the field since publication and a selection of more recent literature references. A recent excursion on the web in search of "compositional data" produced over 3000 entries within a great variety of disciplines. In agriculture, land use compositions; in archaeology, chemical compositions of ceramics; in developmental biology, shape analysis relating (head, trunk, leg) compositions to height; in economics, household budget patterns; in environometrics, pollutant compositions; in geology, major oxide compositions of rocks and sediment (sand, silt, clay) compositions; in literary studies, sentence compositions; in manufacturing, global car production compositions; in medicine, blood, urine and renal calculi compositions; in ornithology, plumage and artefact colour compositions of the greater bower bird and sea bird time budgets; in palaeontology, zonal pollen compositions; in psephology, US Presidential election voting proportions; in psychology and sociology, time budgets of various groups; in waste disposal studies, waste composition. There can be little doubt that appropriate statistical analysis of such compositions is a requirement of many problems in many disciplines. This book provides a clear and systematic account of statistical methods designed to meet the special needs of the compositional data analyst. From the motivation of a number of practical examples from different disciplines and from a re-examination of the difficulties inherent in the inappropriate standard methods, the author argues that any successful statistical methodology must be based on the simple perception that only the relative magnitudes of the components of a composition matter, not their absolute values. "All along Aitchison motivates his mathematics by showing how it can answer the real questions of scientists . . . An enthralling instance of statistics at the service of modern science." The Times Higher Education Supplement "This book is the definitive work in its area and is likely to be the standard reference for decades to come." Mathematical Reviews "This splendid monograph develops sensible models for distributions on simplexes." Journal of the Royal Statistical Society "It is very well written and provides the only unified approach in existence to the analysis of compositional data." International Statistical Institute},
  isbn = {978-1-930665-78-1},
  langid = {english}
}

@incollection{alt1984,
  title = {Multivariate Quality Control},
  booktitle = {The Encyclopedia of Statistical Sciences},
  author = {Alt, Frederick A.},
  editor = {Johnson, N. L. and Kotz, S. and Read, C. R.},
  year = {1984},
  volume = {6},
  pages = {110--122},
  publisher = {{John Wiley \& Sons}},
  address = {{New York}}
}

@misc{anhoej2021,
  title = {{\pkg{qicharts}}: {{Quality}} Improvement Charts},
  author = {Anhoej, Jacob},
  year = {2021},
  note = {\proglang{R} package version 0.5.8},
  url = {https://CRAN.R-project.org/package=qicharts}
}

@misc{anhoej2023,
  title = {{\pkg{qicharts2}}: {{Quality}} Improvement Charts},
  author = {Anhoej, Jacob},
  year = {2021},
  keywords = {todo},
  note = {\proglang{R} package version 0.7.4},
  url = {https://CRAN.R-project.org/package=qicharts2}
}

@article{apley2008,
  title = {Robustness {{Comparison}} of {{Exponentially Weighted Moving-Average Charts}} on {{Autocorrelated Data}} and on {{Residuals}}},
  author = {Apley, Daniel W. and Lee, Hyun Cheol},
  year = {2008},
  journal = {Journal of Quality Technology},
  volume = {40},
  number = {4},
  pages = {428--447},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2008.11917747},
  urldate = {2023-08-02},
  abstract = {Control charting of autocorrelated data has been the subject of extensive research over the past two decades. A standard approach is to apply an exponentially weighted moving average (EWMA) chart to either the autocorrelated data or to the residuals of an autoregressive moving-average (ARMA) model of the process. Numerous empirical studies have demonstrated that many control charts for autocorrelated data, residual-based charts in particular, lack robustness to ARMA modeling errors. In this article, we quantify and corroborate these empirical findings by developing analytical expressions for the sensitivity of EWMA control charts applied to residuals and to autocorrelated data. The analytical results provide insight into the mechanisms behind the (lack of) robustness and also provide a basis for comparing the robustness of the two approaches. One conclusion is that, although the residual-based EWMA may lack robustness, it is generally more robust than the EWMA applied to the autocorrelated data.},
  keywords = {Autoregressive Moving-Average Model,Control Chart,Model Error,Robustness,Statistical Process Control,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Apley_Lee_2008_Robustness_Comparison_of_Exponentially_Weighted_Moving-Average_Charts_on.pdf}
}

@article{arnoldi1951,
  title = {The {{Principle}} of {{Minimized Iterations}} in the {{Solution}} of the {{Matrix Eigenvalue Problem}}},
  author = {Arnoldi, W. E.},
  year = {1951},
  journal = {Quarterly of Applied Mathematics},
  volume = {9},
  number = {1},
  eprint = {43633863},
  eprinttype = {jstor},
  pages = {17--29},
  publisher = {{Brown University}},
  issn = {0033-569X},
  urldate = {2023-10-10}
}

@software{bates2023,
  author       = {Douglas Bates and
                  Phillip Alday and
                  Dave Kleinschmidt and
                  José Bayoán Santiago Calderón and
                  Likan Zhan and
                  Andreas Noack and
                  Milan Bouchet-Valat and
                  Alex Arslan and
                  Tony Kelman and
                  Antoine Baldassari and
                  Benedikt Ehinger and
                  Daniel Karrasch and
                  Elliot Saba and
                  Jacob Quinn and
                  Michael Hatherly and
                  Morten Piibeleht and
                  Patrick Kofod Mogensen and
                  Simon Babayan and
                  Tim Holy and
                  Yakir Luc Gagnon and
                  Yoni Nazarathy},
  title = {{{JuliaStats}}/\pkg{{{MixedModels}}.jl}: V4.22.3},
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v4.22.3},
  doi          = {10.5281/zenodo.10268806},
  url          = {https://doi.org/10.5281/zenodo.10268806}
},

@misc{Loess.jl,
  author       = {Andreas Noack and
                  Daniel C Jones and
                  Alex Arslan and
                  Evan Fields
                  },
  title = {{{JuliaStats}}/\pkg{{{Loess}}.jl}: V0.6.3},
  year = {2013},
  howpublished = {\url{https://github.com/JuliaStats/Loess.jl}}
},

@misc{Distributions.jl-2019,
  author       = {Dahua Lin and
                  John Myles White and
                  Simon Byrne and
                  Douglas Bates and
                  Andreas Noack and
                  John Pearson and
                  Alex Arslan and
                  Kevin Squire and
                  David Anthoff and
                  Theodore Papamarkou and
                  Mathieu Besançon and
                  Jan Drugowitsch and
                  Moritz Schauer and
                  other contributors},
  title        = {{JuliaStats/\pkg{Distributions.jl}: a \proglang{Julia} package for probability distributions and associated functions}},
  month        = jul,
  year         = 2019,
  doi          = {10.5281/zenodo.2647458},
  url          = {https://doi.org/10.5281/zenodo.2647458}
}

@article{besancon2021,
  title = {\pkg{Distributions.jl}: {{Definition}} and {{Modeling}} of {{Probability Distributions}} in the {{JuliaStats Ecosystem}}},
  shorttitle = {Distributions.Jl},
  author = {Besan{\c c}on, Mathieu and Papamarkou, Theodore and Anthoff, David and Arslan, Alex and Byrne, Simon and Lin, Dahua and Pearson, John},
  year = {2021},
  journal = {Journal of Statistical Software},
  volume = {98},
  pages = {1--30},
  issn = {1548-7660},
  doi = {10.18637/jss.v098.i16},
  urldate = {2023-10-23},
  abstract = {Random variables and their distributions are a central part in many areas of statistical methods. The Distributions.jl package provides Julia users and developers tools for working with probability distributions, leveraging Julia features for their intuitive and flexible manipulation, while remaining highly efficient through zero-cost abstractions.},
  copyright = {Copyright (c) 2021 Mathieu Besan\c{c}on, Theodore Papamarkou, David Anthoff, Alex Arslan, Simon Byrne, Dahua Lin, John Pearson},
  langid = {english},
  keywords = {distributions,inference,interface,Julia,KDE,mixture,modeling,probabilistic programming,sampling},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Besancon_et_al_2021_Distributions.pdf}
}

@article{besl1992,
  title = {A Method for Registration of 3-{{D}} Shapes},
  author = {Besl, P.J. and McKay, Neil D.},
  year = {1992},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {14},
  number = {2},
  pages = {239--256},
  issn = {1939-3539},
  doi = {10.1109/34.121791},
  urldate = {2023-10-10},
  abstract = {The authors describe a general-purpose, representation-independent method for the accurate and computationally efficient registration of 3-D shapes including free-form curves and surfaces. The method handles the full six degrees of freedom and is based on the iterative closest point (ICP) algorithm, which requires only a procedure to find the closest point on a geometric entity to a given point. The ICP algorithm always converges monotonically to the nearest local minimum of a mean-square distance metric, and the rate of convergence is rapid during the first few iterations. Therefore, given an adequate set of initial rotations and translations for a particular class of objects with a certain level of 'shape complexity', one can globally minimize the mean-square distance metric over all six degrees of freedom by testing each initial registration. One important application of this method is to register sensed data from unfixtured rigid objects with an ideal geometric model, prior to shape inspection. Experimental results show the capabilities of the registration algorithm on point sets, curves, and surfaces.{$<>$}},
  file = {/home/dede/Zotero/storage/8TKGCM9D/121791.html}
}

@article{bezanson2017,
  title = {\proglang{Julia}: {{A Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  year = {2017},
  journal = {SIAM Review},
  volume = {59},
  number = {1},
  pages = {65--98},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/141000671},
  urldate = {2023-07-02},
  abstract = {JuMP is an open-source modeling language that allows users to express a wide range of optimization problems (linear, mixed-integer, quadratic, conic-quadratic, semidefinite, and nonlinear) in a high-level, algebraic syntax. JuMP takes advantage of advanced features of the Julia programming language to offer unique functionality while achieving performance on par with commercial modeling tools for standard tasks. In this work we will provide benchmarks, present the novel aspects of the implementation, and discuss how JuMP can be extended to new problem classes and composed with state-of-the-art tools for visualization and interactivity.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bezanson_et_al_2017_Julia.pdf}
}

@book{bonferroni1936,
  title = {{Teoria statistica delle classi e calcolo delle probabilit\`a}},
  author = {Bonferroni, Carlo E.},
  year = {1936},
  publisher = {{Seeber}},
  langid = {italian}
}

@book{box2015,
  title = {Time {{Series Analysis}}: {{Forecasting}} and {{Control}}},
  shorttitle = {Time {{Series Analysis}}},
  author = {Box, George E. P. and Jenkins, Gwilym M. and Reinsel, Gregory C. and Ljung, Greta M.},
  year = {2015},
  edition = {5th edition},
  publisher = {{John Wiley \& Sons}},
  address = {{Hoboken, New Jersey}},
  abstract = {Praise for the Fourth Edition "The book follows faithfully the style of the original edition. The approach is heavily motivated by real-world time series, and by developing a complete approach to model building, estimation, forecasting and control."\rule{1em}{1pt}Mathematical ReviewsBridging classical models and modern topics, the Fifth Edition of Time Series Analysis: Forecasting and Control maintains a balanced presentation of the tools for modeling and analyzing time series. Also describing~ the latest developments that have occurred in the field over the past decade through applications from areas such as business, finance, and engineering, the Fifth Edition continues to serve as one of the most influential and prominent works on the subject.Time Series Analysis: Forecasting and Control, Fifth Edition provides a clearly written exploration of the key methods for building, classifying, testing, and analyzing stochastic models for time series and describes their use in five important areas of application: forecasting; determining the transfer function of a system; modeling the effects of intervention events; developing multivariate dynamic models; and designing simple control schemes.~ Along with these classical uses, the new edition covers modern topics with new features that include:A redesigned chapter on multivariate time series analysis with an expanded treatment of Vector Autoregressive, or VAR models, along with a discussion of the analytical tools needed for modeling vector time seriesAn expanded chapter on special topics covering~ unit root testing,~ time-varying volatility~ models such as ARCH and GARCH, nonlinear time series models, and long memory modelsNumerous examples drawn from finance, economics, engineering, and other related fieldsThe use of the publicly available R software for graphical illustrations and numerical calculations along with scripts that demonstrate the use of R for model building and forecastingUpdates to literature references throughout and new end-of-chapter exercisesStreamlined chapter introductions and revisions that update and enhance the expositionTime Series Analysis: Forecasting and Control, Fifth Edition is a valuable real-world reference for researchers and practitioners in time series analysis, econometrics, finance, and related fields. The book is also an excellent textbook for beginning graduate-level courses in advanced statistics, mathematics, economics, finance, engineering, and physics.},
  isbn = {978-1-118-67502-1},
  langid = {english}
}

@article{brook1972,
  title = {An {{Approach}} to the {{Probability Distribution}} of {{Cusum Run Length}}},
  author = {Brook, D. and Evans, D. A.},
  year = {1972},
  journal = {Biometrika},
  volume = {59},
  number = {3},
  eprint = {2334805},
  eprinttype = {jstor},
  pages = {539--549},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2334805},
  urldate = {2023-10-18},
  abstract = {The classical method of studying a cumulative sum control scheme of the decision interval type has been to regard the scheme as a sequence of sequential tests, to determine the average sample number for these component tests and hence to study the average run length for the scheme. A different approach in which the operation of the scheme is regarded as forming a Markov chain is set out. The transition probability matrix for this chain is obtained and then the properties of this matrix used to determine not only the average run lengths for the scheme, but also moments and percentage points of the run-length distribution and exact probabilities of run length. The method may be used with any discrete distribution and also, as an accurate approximation, with any continuous distribution for the random variable which is to be controlled. Examples are given for the cases of a Poisson random variable and a normal random variable.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Brook_Evans_1972_An_Approach_to_the_Probability_Distribution_of_Cusum_Run_Length.pdf}
}

@article{bugatti2022,
  title = {Towards Real-Time in-Situ Monitoring of Hot-Spot Defects in {{L-PBF}}: A New Classification-Based Method for Fast Video-Imaging Data Analysis},
  shorttitle = {Towards Real-Time in-Situ Monitoring of Hot-Spot Defects in {{L-PBF}}},
  author = {Bugatti, Matteo and Colosimo, Bianca Maria},
  year = {2022},
  journal = {Journal of Intelligent Manufacturing},
  volume = {33},
  number = {1},
  pages = {293--309},
  issn = {1572-8145},
  doi = {10.1007/s10845-021-01787-y},
  urldate = {2022-09-21},
  abstract = {The increasing interest towards additive manufacturing (AM) is pushing the industry to provide new solutions to improve process stability. Monitoring is a key tool for this purpose but the typical AM fast process dynamics and the high data flow required to accurately describe the process are pushing the limits of standard statistical process monitoring (SPM) techniques. The adoption of novel smart data extraction and analysis methods are fundamental to monitor the process with the required accuracy while keeping the computational effort to a reasonable level for real-time application. In this work, a new framework for the detection of defects in metal additive manufacturing processes via in-situ high-speed cameras is presented: a new data extraction method is developed to efficiently extract only the relevant information from the regions of interest identified in the high-speed imaging data stream and to reduce the dimensionality of the anomaly detection task performed by three competitor machine learning classification methods. The defect detection performance and computational speed of this approach is carefully evaluated through computer simulations and experimental studies, and directly compared with the performance and computational speed of other existing methods applied on the same reference dataset. The results show that the proposed method is capable of quickly detecting the occurrence of defects while keeping the high computational speed that would be required to implement this new process monitoring approach for real-time defect detection.},
  langid = {english},
  keywords = {Image-based process monitoring,In-situ defect detection,Laser Powder Bed Fusion (L-PBF),Machine learning,Neural network,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bugatti_Colosimo_2022_Towards_real-time_in-situ_monitoring_of_hot-spot_defects_in_L-PBF.pdf}
}

@article{buhlmann2002,
  title = {Bootstraps for {{Time Series}}},
  author = {B{\"u}hlmann, Peter},
  year = {2002},
  journal = {Statistical Science},
  volume = {17},
  number = {1},
  eprint = {3182810},
  eprinttype = {jstor},
  pages = {52--72},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  urldate = {2022-05-07},
  abstract = {We review and compare block, sieve and local bootstraps for time series and thereby illuminate theoretical aspects of the procedures as well as their performance on finite-sample data. Our view is selective with the intention of providing a new and fair picture of some particular aspects of bootstrapping time series. The generality of the block bootstrap is contrasted with sieve bootstraps. We discuss implementational advantages and disadvantages. We argue that two types of sieve often outperform the block method, each of them in its own important niche, namely linear and categorical processes. Local bootstraps, designed for nonparametric smoothing problems, are easy to use and implement but exhibit in some cases low performance.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bühlmann_2002_Bootstraps_for_Time_Series.pdf}
}

@article{bui2022a,
  title = {Analyzing {{Nonparametric Part-to-Part Variation}} in {{Surface Point Cloud Data}}},
  author = {Bui, Anh Tuan and Apley, Daniel W.},
  year = {2022},
  journal = {Technometrics},
  volume = {64},
  number = {4},
  pages = {457--474},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2021.1883482},
  urldate = {2023-10-10},
  abstract = {Surface point cloud data from three-dimensional optical scanners provide rich information about the surface geometry of scanned parts and potential variation in the surfaces from part-to-part. It is challenging, however, to make full use of these data for statistical process control purposes to identify sources of variation that manifest in a more complex nonparametric manner than variation in some prespecified set of geometric features of each part. We develop a framework for identifying nonparametric variation patterns that uses dissimilarity representation of the data and dissimilarity-based manifold learning, which helps discover a low-dimensional implicit manifold parameterization of the variation. Visualizing how the parts change as the manifold parameters are varied helps build an understanding of the physical characteristic of the variation. We also discuss using the nominal surface of parts when it is accessible to improve the computational expense and visualization aspects of the framework. Our approaches clearly reveal the nature of the variation patterns in a real cylindrical-part machining example and a simulated square head bolt example.},
  keywords = {Dissimilarity representation,Manifold learning,Phase I analysis,Point cloud distance,Statistical process control}
}

@article{capezza2020,
  title = {Control Charts for Monitoring Ship Operating Conditions and {{CO2}} Emissions Based on Scalar-on-Function Regression},
  author = {Capezza, Christian and Lepore, Antonio and Menafoglio, Alessandra and Palumbo, Biagio and Vantini, Simone},
  year = {2020},
  journal = {Applied Stochastic Models in Business and Industry},
  volume = {36},
  number = {3},
  pages = {477--500},
  issn = {1526-4025},
  doi = {10.1002/asmb.2507},
  urldate = {2023-10-19},
  abstract = {To respond to the compelling air pollution programs, shipping companies are nowadays setting-up on their fleets modern multisensor systems that stream massive amounts of observational data, which can be considered as varying over a continuous domain. Motivated by this context, a novel procedure is proposed, which extends classical multivariate techniques to the monitoring of multivariate functional data and a scalar quality characteristic related to them. The proposed procedure is shown to be also applicable in real time and is illustrated by means of a real-case study in the maritime field on the continuous monitoring of operating conditions (ie, the multivariate functional data) and total CO2 emissions (ie, the scalar quality characteristic) at each voyage of a cruise ship. The real-time monitoring is particularly helpful for promptly supporting managerial decision making by indicating if and when an anomaly occurs during the navigation.},
  copyright = {\textcopyright{} 2020 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {functional data analysis,multivariate functional principal component analysis,profile monitoring,statistical process monitoring},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capezza_et_al_2020_Control_charts_for_monitoring_ship_operating_conditions_and_CO2_emissions_based.pdf;/home/dede/Zotero/storage/TGSVWCKP/asmb.html}
},

@Manual{capezza2023a,
  title = {\pkg{funcharts}: {{Functional Control Charts}}},
  year = {2023},
  author = {Christian Capezza and Fabio Centofanti and Antonio Lepore and Alessandra Menafoglio and Biagio Palumbo and Simone Vantini},
  url = {https://CRAN.R-project.org/package=funcharts},
  note = {\proglang{R} package version 1.3.2},
}

@article{capezza2023,
  title = {\pkg{funcharts}: Control Charts for Multivariate Functional Data in {\proglang{R}}},
  shorttitle = {Funcharts},
  author = {Capezza, Christian and Centofanti, Fabio and Lepore, Antonio and Menafoglio, Alessandra and Palumbo, Biagio and Vantini, Simone},
  year = {2023},
  journal = {Journal of Quality Technology},
  volume = {55},
  number = {5},
  pages = {566--583},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2023.2219012},
  urldate = {2023-11-21},
  abstract = {Modern statistical process monitoring (SPM) applications focus on profile monitoring, i.e., the monitoring of process quality characteristics that can be modeled as profiles, also known as functional data. Despite the large interest in the profile monitoring literature, there is still a lack of software to facilitate its practical application. This article introduces the funcharts \proglang{R} package that implements recent developments on the SPM of multivariate functional quality characteristics, possibly adjusted by the influence of additional variables, referred to as covariates. The package also implements the real-time version of all control charting procedures to monitor profiles partially observed up to an intermediate domain point. The package is illustrated both through its built-in data generator and a real-case study on the SPM of Ro-Pax ship CO2 emissions during navigation, which is based on the ShipNavigation data set provided in the Supplementary Material.},
  keywords = {functional data analysis,multivariate functional linear regression,profile monitoring,R,statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capezza_et_al_2023_funcharts.pdf}
}

@article{capizzi2003,
  title = {An {{Adaptive Exponentially Weighted Moving Average Control Chart}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2003},
  journal = {Technometrics},
  volume = {45},
  number = {3},
  eprint = {25047047},
  eprinttype = {jstor},
  pages = {199--207},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  urldate = {2020-10-28},
  abstract = {Lucas and Saccucci showed that exponentially weighted moving average (EWMA) control charts can be designed to quickly detect either small or large shifts in the mean of a sequence of independent observations. But a single EWMA chart cannot perform well for small and large shifts simultaneously. Furthermore, in the worst-case situation, this scheme requires a few observations to overcome its initial inertia. The main goal of this article is to suggest an adaptive EWMA (AEWMA) chart that weights the past observations of the monitored process using a suitable function of the current "error." The resulting scheme can be viewed as a smooth combination of a Shewhart chart and an EWMA chart. A design procedure for the new control schemes is suggested. Comparisons of the standard and worst-case average run length profiles of the new scheme with those of different control charts show that AEWMA schemes offer a more balanced protection against shifts of different sizes.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2003_An_Adaptive_Exponentially_Weighted_Moving_Average_Control_Chart2.pdf}
}

@article{capizzi2008,
  title = {Practical {{Design}} of {{Generalized Likelihood Ratio Control Charts}} for {{Autocorrelated Data}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2008},
  journal = {Technometrics},
  volume = {50},
  number = {3},
  pages = {357--370},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/004017008000000280},
  urldate = {2022-05-03},
  abstract = {Control charts based on generalized likelihood ratio (GLR) tests are attractive from both theoretical and practical points of view. In particular, in the case of an autocorrelated process, the GLR test uses the information contained in the time-varying response after a change and, as shown by Apley and Shi, is able to outperfom traditional control charts applied to residuals. In addition, a GLR chart provides estimates of the magnitude and the time of occurrence of the change. In this article we present a practical approach to implementating GLR charts for monitoring an autoregressive moving average process assuming that only a phase I sample is available. The proposed approach, based on automatic time series identification, estimates the GLR control limits through stochastic approximation using bootstrap resampling and thus is able to take into account the uncertainty about the underlying model. A Monte Carlo study shows that our methodology can be used to design, in a semiautomatic fashion, a GLR chart with a prescribed rate of false alarms when as few as 50 phase I observations are available. A real example is used to illustrate the designing procedure.},
  keywords = {Automatic modeling,Autoregressive moving average model,Bootstrap,done,Statistical process control,Stochastic approximation,Uncertainty modeling},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2008_Practical_Design_of_Generalized_Likelihood_Ratio_Control_Charts_for.pdf;/home/dede/Zotero/storage/8CXVWLV6/004017008000000280.html}
}

@article{capizzi2009,
  title = {Bootstrap-{{Based Design}} of {{Residual Control Charts}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2009},
  journal = {IIE Transactions},
  volume = {41},
  number = {4},
  pages = {275--286},
  publisher = {{Taylor \& Francis}},
  issn = {0740-817X},
  doi = {10.1080/07408170802120059},
  urldate = {2021-11-23},
  abstract = {One approach to monitoring autocorrelated data consists in applying a control chart to the residuals of a time series model estimated from process observations. Recent research shows that the impact of estimation error on the run length properties of the resulting charts is not negligible. In this paper a general strategy for implementing residual-based control schemes is investigated. The designing procedure uses the AR-sieve approximation assuming that the process allows an autoregressive representation of order infinity. The run length distribution is estimated using bootstrap resampling in order to account for uncertainty in the estimated parameters. Control limits that satisfy a given constraint on the false alarm rate are computed via stochastic approximation. The proposed procedure is investigated for three residual-based control charts: generalized likelihood ratio, cumulative sum and exponentially weighted moving average. Results show that the bootstrap approach safeguards against an undesirably high rate of false alarms. In addition, the out-of-control bootstrap chart sensitivity seems to be comparable to that of charts designed under the assumption that the estimated model is equal to the true generating process. [Supplementary materials are available for this article. Go to the publisher's online edition of IIE Transactions for the following free supplemental resource: Appendix]},
  keywords = {autocorrelated data,control charts,done,On-line quality control,sieve bootstrap,time series,uncertainty modeling},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2009_Bootstrap-based_design_of_residual_control_charts.pdf;/home/dede/Zotero/storage/VPPPQH29/07408170802120059.html}
}

@article{capizzi2012a,
  title = {An {{Enhanced Control Chart}} for {{Start-Up Processes}} and {{Short Runs}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2012},
  journal = {Quality Technology \& Quantitative Management},
  volume = {9},
  number = {2},
  pages = {189--202},
  issn = {1684-3703},
  doi = {10.1080/16843703.2012.11673285},
  urldate = {2021-04-17},
  langid = {english},
  keywords = {toRead},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2012_An_Enhanced_Control_Chart_for_Start-Up_Processes_and_Short_Runs.pdf}
}

@article{capizzi2016,
  title = {Efficient Control Chart Calibration by Simulated Stochastic Approximation},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2016},
  journal = {IIE Transactions},
  volume = {48},
  number = {1},
  pages = {57--65},
  publisher = {{Taylor \& Francis}},
  issn = {0740-817X},
  doi = {10.1080/0740817X.2015.1055392},
  urldate = {2023-10-21},
  abstract = {The accurate determination of control limits is crucial in statistical process control. The usual approach consists in computing the limits so that the in-control run-length distribution has some desired properties; for example, a prescribed mean. However, as a consequence of the increasing complexity of process data, the run-length of many control charts discussed in the recent literature can be studied only through simulation. Furthermore, in some scenarios, such as profile and autocorrelated data monitoring, the limits cannot be tabulated in advance, and when different charts are combined, the control limits depend on a multidimensional vector of parameters. In this article, we propose the use of stochastic approximation methods for control chart calibration and discuss enhancements for their implementation (e.g., the initialization of the algorithm, an adaptive choice of the gain, a suitable stopping rule for the iterative process, and the advantages of using multicore workstations). Examples are used to show that simulated stochastic approximation provides a reliable and fully automatic approach for computing the control limits in complex applications. An \proglang{R} package implementing the algorithm is available in the supplemental materials.},
  keywords = {control charts,done,multi-chart schemes,statistical process control,Stochastic approximation,stochastic root finding},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2016_Efficient_control_chart_calibration_by_simulated_stochastic_approximation2.pdf}
}

@article{capizzi2017,
  title = {Phase {{I Distribution-Free Analysis}} of {{Multivariate Data}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2017},
  journal = {Technometrics},
  volume = {59},
  number = {4},
  pages = {484--495},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2016.1272494},
  urldate = {2020-10-22},
  abstract = {In this study, a new distribution-free Phase I control chart for retrospectively monitoring multivariate data is developed. The suggested approach, based on the multivariate signed ranks, can be applied to individual or subgrouped data for detection of location shifts with an arbitrary pattern (e.g., isolated, transitory, sustained, progressive, etc.). The procedure is complemented with a LASSO-based post-signal diagnostic method for identification of the shifted variables. A simulation study shows that the method compares favorably with parametric control charts when the process is normally distributed, and largely outperforms other multivariate nonparametric control charts when the process distribution is skewed or heavy-tailed. An \proglang{R} package can be found in the supplementary material.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2017_Phase_I_Distribution-Free_Analysis_of_Multivariate_Data.pdf;/home/dede/Zotero/storage/BRPHH56T/00401706.2016.html}
}

@article{centofanti2021a,
  title = {Functional Regression Control Chart},
  author = {Centofanti, Fabio and Lepore, Antonio and Menafoglio, Alessandra and Palumbo, Biagio and Vantini, Simone},
  year = {2021},
  journal = {Technometrics},
  volume = {63},
  number = {3},
  pages = {281--294},
  publisher = {{Taylor \& Francis}},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Centofanti_et_al_2021_Functional_regression_control_chart.pdf}
}

@article{chakraborti2001,
  title = {Nonparametric {{Control Charts}}: {{An Overview}} and {{Some Results}}},
  shorttitle = {Nonparametric {{Control Charts}}},
  author = {Chakraborti, S. and Van Der Laan, P. and Bakir, S. T.},
  year = {2001},
  journal = {Journal of Quality Technology},
  volume = {33},
  number = {3},
  pages = {304--315},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2001.11980081},
  urldate = {2023-07-03},
  abstract = {We present an overview of the literature on nonparametric or distribution-free control charts for univariate variables data. We highlight various advantages of these charts while pointing out some of the disadvantages of the more traditional, distribution-based control charts. Specific observations are made in the course of review of articles and constructive criticism is offered so that opportunities for further research can be identified. Connections to some areas of active research are made, such as sequential analysis, that are relevant to process control. We hope that this article leads to a wider acceptance of distribution-free control charts among practitioners and serves as an impetus to future research and development in this area.},
  keywords = {Change Point,Cumulative Sum Control Charts,Distribution-Free Procedures,done,Exponentially Weighted Moving Average Control Charts,Shewhart Control Charts},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chakraborti_et_al_2001_Nonparametric_Control_Charts.pdf}
}

@article{chakraborti2019,
  title = {Nonparametric (Distribution-{{Free}}) {{Control Charts}}: {{An Updated Overview}} and {{Some Results}}},
  shorttitle = {Nonparametric (Distribution-{{Free}}) {{Control Charts}}},
  author = {Chakraborti, Subha and Graham, Marien},
  year = {2019},
  journal = {Quality Engineering},
  volume = {31},
  number = {4},
  pages = {523--544},
  doi = {10.1080/08982112.2018.1549330},
  abstract = {Control charts that are based on assumption(s) of a specific form for the underlying process distribution are referred to as parametric control charts. There are many applications where there is insufficient information to justify such assumption(s) and, consequently, control charting techniques with a minimal set of distributional assumption requirements are in high demand. To this end, nonparametric or distribution-free control charts have been proposed in recent years. The charts have stable in-control properties, are robust against outliers and can be surprisingly efficient in comparison with their parametric counterparts. Chakraborti and some of his colleagues provided review papers on nonparametric control charts in 2001, 2007 and 2011, respectively. These papers have been received with considerable interest and attention by the community. However, the literature on nonparametric statistical process/quality control/monitoring has grown exponentially and because of this rapid growth, an update is deemed necessary. In this article, we bring these reviews forward to 2017, discussing some of the latest developments in the area. Moreover, unlike the past reviews, which did not include the multivariate charts, here we review both univariate and multivariate nonparametric control charts. We end with some concluding remarks.},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chakraborti_Graham_2019_Nonparametric_(distribution-free)_control_charts.pdf}
}

@article{champ1991,
  title = {A Generalized Quality Control Procedure},
  author = {Champ, Charles W. and Woodall, William H. and Mohsen, Hassan A.},
  year = {1991},
  journal = {Statistics \& Probability Letters},
  volume = {11},
  number = {3},
  pages = {211--218},
  issn = {0167-7152},
  doi = {10.1016/0167-7152(91)90145-H},
  urldate = {2023-10-23},
  abstract = {A new control chart procedure is given for which the Shewhart X-chart, the cumulative sum chart, and the exponentially weighted moving average chart are special cases. The run length properties for a one-sided generalized control chart are analyzed using an integral equation approach. A comparison of the average run lengths of various new control chart procedures is given.},
  keywords = {cumulative sum chart,exponentially weighted moving average chart,Shewhart -chart,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Champ_et_al_1991_A_generalized_quality_control_procedure.pdf;/home/dede/Zotero/storage/I5Z7XMQW/016771529190145H.html}
}

@article{chatterjee2009,
  title = {Distribution-{{Free Cumulative Sum Control Charts Using Bootstrap-Based Control Limits}}},
  author = {Chatterjee, Snigdhansu and Qiu, Peihua},
  year = {2009},
  journal = {The Annals of Applied Statistics},
  volume = {3},
  number = {1},
  pages = {349--369},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/08-AOAS197},
  urldate = {2022-04-30},
  abstract = {This paper deals with phase II, univariate, statistical process control when a set of in-control data is available, and when both the in-control and out-of-control distributions of the process are unknown. Existing process control techniques typically require substantial knowledge about the in-control and out-of-control distributions of the process, which is often difficult to obtain in practice. We propose (a) using a sequence of control limits for the cumulative sum (CUSUM) control charts, where the control limits are determined by the conditional distribution of the CUSUM statistic given the last time it was zero, and (b) estimating the control limits by bootstrap. Traditionally, the CUSUM control chart uses a single control limit, which is obtained under the assumption that the in-control and out-of-control distributions of the process are Normal. When the normality assumption is not valid, which is often true in applications, the actual in-control average run length, defined to be the expected time duration before the control chart signals a process change, is quite different from the nominal in-control average run length. This limitation is mostly eliminated in the proposed procedure, which is distribution-free and robust against different choices of the in-control and out-of-control distributions.},
  keywords = {Cumulative sum control charts,distribution-free procedures,done,nonparametric model,Resampling,robustness,statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chatterjee_Qiu_2009_Distribution-free_cumulative_sum_control_charts_using_bootstrap-based_control.pdf;/home/dede/Zotero/storage/E5XUXZES/08-AOAS197.html}
}

@book{chavel1984,
  title = {Eigenvalues in {{Riemannian Geometry}}},
  author = {Chavel, Isaac and Randol, Burton and Dodziuk, Jozef},
  year = {1984},
  publisher = {{Academic Press}},
  address = {{Orlando}},
  abstract = {The basic goals of the book are: (i) to introduce the subject to those interested in discovering it, (ii) to coherently present a number of basic techniques and results, currently used in the subject, to those working in it, and (iii) to present some of the results that are attractive in their own right, and which lend themselves to a presentation not overburdened with technical machinery.},
  isbn = {978-0-12-170640-1},
  langid = {english}
}

@book{chen2002,
  title = {Stochastic {{Approximation}} and {{Its Applications}}},
  author = {Chen, Han-Fu},
  year = {2002},
  edition = {},
  publisher = {{Springer-Verlag}},
  address = {{Dordrecht; Boston}},
  isbn = {978-1-4020-0806-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_2002_Stochastic_Approximation_and_Its_Applications.pdf}
}

@article{chen2008,
  title = {Extended {{Bayesian Information Criteria}} for {{Model Selection}} with {{Large Model Spaces}}},
  author = {Chen, Jiahua and Chen, Zehua},
  year = {2008},
  journal = {Biometrika},
  volume = {95},
  number = {3},
  eprint = {20441500},
  eprinttype = {jstor},
  pages = {759--771},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  urldate = {2023-10-10},
  abstract = {The ordinary Bayesian information criterion is too liberal for model selection when the model space is large. In this paper, we re-examine the Bayesian paradigm for model selection and propose an extended family of Bayesian information criteria, which take into account both the number of unknown parameters and the complexity of the model space. Their consistency is established, in particular allowing the number of covariates to increase to infinity with the sample size. Their performance in various situations is evaluated by simulation studies. It is demonstrated that the extended Bayesian information criteria incur a small loss in the positive selection rate but tightly control the false discovery rate, a desirable property in many applications. The extended Bayesian information criteria are extremely useful for variable selection in problems with a moderate sample size but with a huge number of covariates, especially in genome-wide association studies, which are now an active area in genetics research.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_Chen_2008_Extended_Bayesian_Information_Criteria_for_Model_Selection_with_Large_Model.pdf}
}

@article{chen2016a,
  title = {A {{Distribution-Free Multivariate Control Chart}}},
  author = {Chen, Nan and Zi, Xuemin and Zou, Changliang},
  year = {2016},
  journal = {Technometrics},
  volume = {58},
  number = {4},
  eprint = {44868867},
  eprinttype = {jstor},
  pages = {448--459},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  urldate = {2023-10-10},
  abstract = {Monitoring multivariate quality variables or data streams remains an important and challenging problem in statistical process control (SPC). Although the multivariate SPC has been extensively studied in the literature, designing distribution-free control schemes are still challenging and yet to be addressed well. This article develops a new nonparametric methodology for monitoring location parameters when only a small reference dataset is available. The key idea is to construct a series of conditionally distribution-free test statistics in the sense that their distributions are free of the underlying distribution given the empirical distribution functions. The conditional probability that the charting statistic exceeds the control limit at present given that there is no alarm before the current time point can be guaranteed to attain a specified false alarm rate. The success of the proposed method lies in the use of data-dependent control limits, which are determined based on the observations online rather than decided before monitoring. Our theoretical and numerical studies show that the proposed control chart is able to deliver satisfactory in-control run-length performance for any distributions with any dimension. It is also very efficient in detecting multivariate process shifts when the process distribution is heavy-tailed or skewed. Supplementary materials for this article are available online.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_et_al_2016_A_Distribution-Free_Multivariate_Control_Chart2.pdf}
}

@book{chung1996,
  title = {Spectral {{Graph Theory}}},
  author = {Chung, Fan R. K.},
  year = {1996},
  publisher = {{American Mathematical Society}},
  address = {{Providence, R.I}},
  abstract = {Beautifully written and elegantly presented, this book is based on 10 lectures given at the CBMS workshop on spectral graph theory in June 1994 at Fresno State University. Chung's well-written exposition can be likened to a conversation with a good teacher--one who not only gives you the facts, but tells you what is really going on, why it is worth doing, and how it is related to familiar ideas in other areas. The monograph is accessible to the nonexpert who is interested in reading about this evolving area of mathematics.},
  isbn = {978-0-8218-0315-8},
  langid = {english}
}

@article{cleveland1979a,
  title = {Robust {{Locally Weighted Regression}} and {{Smoothing Scatterplots}}},
  author = {Cleveland, William S.},
  year = {1979},
  journal = {Journal of the American Statistical Association},
  volume = {74},
  number = {368},
  pages = {829--836},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1979.10481038},
  urldate = {2023-10-21},
  abstract = {The visual information on a scatterplot can be greatly enhanced, with little additional cost, by computing and plotting smoothed points. Robust locally weighted regression is a method for smoothing a scatterplot, (x i , y i ), i = 1, \ldots, n, in which the fitted value at z k is the value of a polynomial fit to the data using weighted least squares, where the weight for (x i , y i ) is large if x i is close to x k and small if it is not. A robust fitting procedure is used that guards against deviant points distorting the smoothed points. Visual, computational, and statistical issues of robust locally weighted regression are discussed. Several examples, including data on lead intoxication, are used to illustrate the methodology.},
  keywords = {Graphics,Nonparametric regression,Robust estimation,Scatterplots,Smoothing}
}

@book{colosimo2006,
  title = {{Bayesian Process Monitoring, Control and Optimization}},
  author = {Colosimo, Bianca M. and del Castillo, Enrique},
  year = {2006},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  isbn = {978-1-58488-544-3},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Colosimo_Castillo_2006_Bayesian_Process_Monitoring,_Control_and_Optimization.pdf}
}

@article{colosimo2007,
  title = {On the Use of Principal Component Analysis to Identify Systematic Patterns in Roundness Profiles},
  author = {Colosimo, Bianca Maria and Pacella, Massimo},
  year = {2007},
  journal = {Quality and Reliability Engineering International},
  volume = {23},
  number = {6},
  pages = {707--725},
  issn = {1099-1638},
  doi = {10.1002/qre.878},
  urldate = {2023-07-02},
  abstract = {In many industrial applications, quality of products or processes is related to profiles. With reference to mechanical components, profiles and surfaces play a relevant role, as shown by the high number of geometric specifications characterizing most of the technical drawings. In this framework, an important step consists in identifying the systematic pattern which characterizes all the profiles machined while the process is in its standard or nominal state. With reference to this aim, this paper focuses on the use of principal component analysis (PCA) for profile data (Functional PCA). Since a usual objection to PCA is that principal components (PCs) are often difficult or impossible to interpret, this paper explores what types of profile features allow one to obtain interpretable PCs. Within the paper, a real case study related to roundness profiles of mechanical components is used as reference. In particular, functional PCA is applied to the set of real profile data to derive the significant PCs and the corresponding eigenfunctions. In order to gain insight into the information behind the retained PCs, both simulations and analytical results are used. In particular, the analytical results, outlined in the literature on functional data analysis, allow one to link the eigenfunctions to specific profile features, given that profile data admit an orthogonal basis series expansion. Copyright \textcopyright{} 2007 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {done,functional data analysis,functional PCA,geometric product specifications,principal component analysis,profile,profile monitoring,roundness},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Colosimo_Pacella_2007_On_the_use_of_principal_component_analysis_to_identify_systematic_patterns_in.pdf}
}

@article{colosimo2008,
  title = {Statistical {{Process Control}} for {{Geometric Specifications}}: {{On}} the {{Monitoring}} of {{Roundness Profiles}}},
  shorttitle = {Statistical {{Process Control}} for {{Geometric Specifications}}},
  author = {Colosimo, Bianca M. and Semeraro, Quirico and Pacella, Massimo},
  year = {2008},
  journal = {Journal of Quality Technology},
  volume = {40},
  number = {1},
  pages = {1--18},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2008.11917709},
  urldate = {2023-07-02},
  abstract = {Quality of mechanical components is critically related to both dimensional and geometric specifications. Traditionally, approaches for statistical process control (SPC) focused on the first type of specification only. When the quality of a manufactured product is related to geometric specifications (e.g., profile and form tolerances as straightness, roundness, cylindricity, flatness, etc.), the process should be considered in control if the relationship used to represent that profile or surface in the space is stable over time. This paper presents a novel method for monitoring bidimensional profiles. The proposed method is based on combining a spatial autoregressive regression (SARX) model (i.e., a regression model with spatial autoregressive errors) with control charting. To show the effectiveness of the proposed method, the approach is applied to real process data in which the roundness of items obtained by turning has to be monitored. A simulation study indicates that the proposed approach outperforms competing methods (based on monitoring the out-of-roundness value for each profile) in terms of the average number of samples required to detect out-of-control conditions arising in phase II and due to spindle-motion errors.},
  keywords = {Control Charts,done,Geometric Tolerance,Multivariate Control Chart,Profile Monitoring,Regression,Spatial Statistics,SPC},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Colosimo_et_al_2008_Statistical_Process_Control_for_Geometric_Specifications.pdf}
}

@article{colosimo2014,
  title = {From {{Profile}} to {{Surface Monitoring}}: {{SPC}} for {{Cylindrical Surfaces Via Gaussian Processes}}},
  shorttitle = {From {{Profile}} to {{Surface Monitoring}}},
  author = {Colosimo, Bianca M. and Cicorella, Paolo and Pacella, Massimo and Blaco, Marzia},
  year = {2014},
  journal = {Journal of Quality Technology},
  volume = {46},
  number = {2},
  pages = {95--113},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2014.11917956},
  urldate = {2023-07-12},
  abstract = {Quality of machined products is often related to the shapes of surfaces that are constrained by geometric tolerances. In this case, statistical quality monitoring should be used to quickly detect unwanted deviations from the nominal pattern. The majority of the literature has focused on statistical profile monitoring, while there is little research on surface monitoring. This paper faces the challenging task of moving from profile to surface monitoring. To this aim, different parametric approaches and control-charting procedures are presented and compared with reference to a real case study dealing with cylindrical surfaces obtained by lathe turning. In particular, a novel method presented in this paper consists of modeling the manufactured surface via Gaussian processes models and monitoring the deviations of the actual surface from the target pattern estimated in phase I. Regardless of the specific case study in this paper, the proposed approach is general and can be extended to deal with different kinds of surfaces or profiles.},
  keywords = {Cylindrical Surfaces,done,Geometric Specifications,GP Model,Kriging,Profile Monitoring,Spatial Statistics},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Colosimo_et_al_2014_From_Profile_to_Surface_Monitoring.pdf}
}

@article{colosimo2018,
  title = {Spatially Weighted {{PCA}} for Monitoring Video Image Data with Application to Additive Manufacturing},
  author = {Colosimo, Bianca M. and Grasso, Marco},
  year = {2018},
  journal = {Journal of Quality Technology},
  volume = {50},
  number = {4},
  pages = {391--417},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2018.1507563},
  urldate = {2023-07-02},
  abstract = {Machine vision systems for in-line process monitoring in advanced manufacturing applications have attracted an increasing interest in recent years. One major goal is to quickly detect and localize the onset of defects during the process. This implies the use of image-based statistical process monitoring approaches to detect both when and where a defect originated within the part. This study presents a spatiotemporal method based on principal component analysis (PCA) to characterize and synthetize the information content of image streams for statistical process monitoring. A spatially weighted version of the PCA, called ST-PCA, is proposed to characterize the temporal auto-correlation of pixel intensities over sequential frames of a video-sequence while including the spatial information related to the pixel location within the image. The method is applied to the detection of defects in metal additive manufacturing processes via in-situ high-speed cameras. A k-means clustering-based alarm rule is proposed to provide an identification of defects in both time and space. A comparison analysis based on simulated and real data shows that the proposed approach is faster than competitor methods in detecting the defects. A real case study in selective laser melting (SLM) of complex geometries is presented to demonstrate the performances of the approach and its practical use.},
  keywords = {image-based process monitoring,in-situ defect detection,k-means clustering,principal component analysis,selective laser melting,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Colosimo_Grasso_2018_Spatially_weighted_PCA_for_monitoring_video_image_data_with_application_to.pdf}
}

@article{colosimo2018a,
  title = {Editors' Note for Special Issue: ``{{Quality}} Engineering in Advanced Manufacturing''},
  shorttitle = {Editors' Note for Special Issue},
  author = {Colosimo, Bianca M. and Huang, Qiang and Dasgupta, Tirthankar and Tsung, Fugee},
  year = {2018},
  journal = {Journal of Quality Technology},
  volume = {50},
  number = {3},
  pages = {231--232},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2018.1488455},
  urldate = {2023-10-10},
  keywords = {todo}
}

@article{colosimo2018b,
  title = {Opportunities and Challenges of Quality Engineering for Additive Manufacturing},
  author = {Colosimo, Bianca M. and Huang, Qiang and Dasgupta, Tirthankar and Tsung, Fugee},
  year = {2018},
  journal = {Journal of Quality Technology},
  volume = {50},
  number = {3},
  pages = {233--252},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2018.1487726},
  urldate = {2023-10-10},
  abstract = {Additive manufacturing (AM), commonly known as three-dimensional printing, is widely recognized as a disruptive technology, and it has the potential to fundamentally change the nature of future manufacturing. Through building products layer by layer, AM represents a paradigm shift in manufacturing, with many industrial applications. It enables production of huge varieties of customized products with considerable geometric complexity, extended capabilities, and functional performances. Despite tremendous enthusiasm AM faces major research challenges for widespread adoption of this innovative technology. Specifically, addressing the unique challenges associated with quality engineering of AM processes is crucial to the eventual success of AM. This article presents an overview of quality-related issues for AM processes and products, focusing on opportunities and challenges in quality inspection, monitoring, control, optimization, and transfer learning as well as on building quality into the product through design.},
  keywords = {3D printing,calibration,compensation,in-situ monitoring,profile,shape,surface,transfer learning},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Colosimo_et_al_2018_Opportunities_and_challenges_of_quality_engineering_for_additive_manufacturing.pdf}
}

@article{colosimo2022,
  title = {Complex Geometries in Additive Manufacturing: {{A}} New Solution for Lattice Structure Modeling and Monitoring},
  shorttitle = {Complex Geometries in Additive Manufacturing},
  author = {Colosimo, Bianca Maria and Grasso, Marco and Garghetti, Federica and Rossi, Beatrice},
  year = {2022},
  journal = {Journal of Quality Technology},
  volume = {54},
  number = {4},
  pages = {392--414},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2021.1926377},
  urldate = {2023-07-12},
  abstract = {The production of novel types of complex shapes is nowadays enabled by new manufacturing paradigms such as additive manufacturing, also known as 3D printing. The continuous increase of shape complexity imposes new challenges in terms of inspection, product qualification and process monitoring methodologies. Previously proposed methods for 2.5D free-form surfaces are no longer applicable in the presence of this kind of new full 3D geometries. This paper aims to tackle this challenge by presenting a statistical quality monitoring approach for structures that cannot be described in terms of parametric models. The goal consists of identifying out-of-control geometrical distortions by analyzing either local variations within the part or changes from part to part. The proposed approach involves an innovative solution for modeling the deviation between the nominal geometry (the originating 3D model) and the real geometry (measured via x-ray computed tomography) by slicing the shapes and estimating the deviation slice by slice. 3D deviation maps are then transformed into 1D deviation profiles enabling the use of a profile monitoring scheme for local defect detection. The feasibility and potential of this method are demonstrated by focusing on a category of complex shapes where an elemental geometry regularly repeats in space. These shapes are known as lattice structures, or metamaterials, and their trabecular shape is thought to provide innovative mechanical and functional performance. The performance of the proposed method is shown in real and simulated case studies.},
  keywords = {3D printing,additive manufacturing,complex shape,lattice structure,profile monitoring,SPC,statistical quality monitoring},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Colosimo_et_al_2022_Complex_geometries_in_additive_manufacturing2.pdf}
}

@article{crosier1986,
  title = {A {{New Two-Sided Cumulative Sum Quality Control Scheme}}},
  author = {Crosier, Ronald B.},
  year = {1986},
  journal = {Technometrics},
  volume = {28},
  number = {3},
  pages = {187--194},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1986.10488126},
  urldate = {2023-09-13},
  abstract = {A new two-sided cumulative sum quality control scheme is proposed. The new scheme was developed specifically to be generalized to a multivariate cumulative sum quality control scheme. The multivariate version will be examined in a subsequent paper; this article evaluates the univariate version. A comparison of the conventional two-sided cumulative sum scheme and the proposed scheme indicates that the new scheme has slightly better properties (ratio of on-aim to off-aim average run lengths) than the conventional scheme. Steady state average run lengths are discussed. The new scheme and the conventional two-sided cumulative sum scheme have equivalent steady state average run lengths. Methods for implementing the fast initial response feature for the new cumulative sum scheme are given. A comparison of average run lengths for the conventional and proposed schemes with fast initial response features is also favorable to the new scheme. A Markov chain approximation is used to calculate the average run lengths of the new scheme.},
  keywords = {Average run length,Markov chain,Multivariate CUSUM,Steady state,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Crosier_1986_A_New_Two-Sided_Cumulative_Sum_Quality_Control_Scheme.pdf}
}

@article{crosier1988,
  title = {Multivariate {{Generalizations}} of {{Cumulative Sum Quality-Control Schemes}}},
  author = {Crosier, Ronald B.},
  year = {1988},
  journal = {Technometrics},
  volume = {30},
  number = {3},
  eprint = {1270083},
  eprinttype = {jstor},
  pages = {291--303},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1270083},
  urldate = {2020-11-26},
  abstract = {This article presents the design procedures and average run lengths for two multivariate cumulative sum (CUSUM) quality-control procedures. The first CUSUM procedure reduces each multivariate observation to a scalar and then forms a CUSUM of the scalars. The second CUSUM procedure forms a CUSUM vector directly from the observations. These two procedures are compared with each other and with the multivariate Shewhart chart. Other multivariate quality-control procedures are mentioned. Robustness, the fast initial response feature for CUSUM schemes, and combined Shewhart-CUSUM schemes are discussed.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Crosier_1988_Multivariate_Generalizations_of_Cumulative_Sum_Quality-Control_Schemes.pdf}
}

@article{crowder1989,
  title = {Design of {{Exponentially Weighted Moving Average Schemes}}},
  author = {Crowder, Stephen V.},
  year = {1989},
  journal = {Journal of Quality Technology},
  volume = {21},
  number = {3},
  pages = {155--162},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.1989.11979164},
  urldate = {2023-02-27},
  abstract = {Plots of optimal smoothing parameters and control limit constants are given which make the design of exponentially weighted moving average charts simple. A design strategy is reviewed.},
  keywords = {Average Run Length,done,Integral Equation,Sensitivity Analysis},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Crowder_1989_Design_of_Exponentially_Weighted_Moving_Average_Schemes.pdf}
}

@article{dai2011,
  title = {A {{New Adaptive CUSUM Control Chart}} for {{Detecting}} the {{Multivariate Process Mean}}},
  author = {Dai, Yi and Luo, Yunzhao and Li, Zhonghua and Wang, Zhaojun},
  year = {2011},
  journal = {Quality and Reliability Engineering International},
  volume = {27},
  number = {7},
  pages = {877--884},
  issn = {1099-1638},
  doi = {10.1002/qre.1177},
  urldate = {2023-06-21},
  abstract = {We propose a new multivariate CUSUM control chart, which is based on self adaption of its reference value according to the information from current process readings, to quickly detect the multivariate process mean shifts. By specifying the minimum magnitude of the process mean shift in terms of its non-centrality parameter, our proposed control chart can achieve an overall performance for detecting a particular range of shifts. This adaptive feature of our method is based on two EWMA operators to estimate the current process mean level and make the detection at each step be approximately optimal. Moreover, we compare our chart with the conventional multivariate CUSUM chart. The advantages of our control chart detection for range shifts over the existing charts are greatly improved. The Markovian chain method, through which the average run length can be computed, is also presented. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {average run length,done,exponentially weighted moving average,multivariate CUSUM,multivariate mean,statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Dai_et_al_2011_A_new_adaptive_CUSUM_control_chart_for_detecting_the_multivariate_process_mean.pdf;/home/dede/Zotero/storage/9KHHSZYY/qre.html}
}

@article{delcastillo2015,
  title = {Geodesic {{Gaussian Processes}} for the {{Parametric Reconstruction}} of a {{Free-Form Surface}}},
  author = {{del Castillo}, Enrique and Colosimo, Bianca M. and Tajbakhsh, Sam Davanloo},
  year = {2015},
  journal = {Technometrics},
  volume = {57},
  number = {1},
  pages = {87--99},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2013.879075},
  urldate = {2023-07-12},
  abstract = {q},
  keywords = {CAD,Manifold data analysis,Noncontact sensors,Parametric surface model},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/del_Castillo_et_al_2015_Geodesic_Gaussian_Processes_for_the_Parametric_Reconstruction_of_a_Free-Form.pdf}
}

@incollection{delcastillo2020,
  title = {Statistical {{Process Monitoring}} for {{Manifold Data}}},
  author = {{del Castillo}, Enrique and Zhao, Xueqi},
  year = {2020},
  pages = {1--8},
  doi = {10.1002/9781118445112.stat08276},
  abstract = {Manifold data is a fundamental type of complex data that are common in modern industry. It occurs when data lie on a lower dimensional, curved subspace or manifold. This article reviews the recent approaches for statistical process monitoring of a discrete-part manufacturing system based on the manifold data obtained from part metrology. The approaches reviewed are based on estimating the spectrum of the Laplace\textendash Beltrami (LB) operator of the scanned parts and using it in a multivariate nonparametric control chart for online process control. This intrinsic method has the computational advantage of avoiding the difficult part registration problem.},
  isbn = {978-1-118-44511-2},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/del_Castillo_Zhao_2020_Statistical_Process_Monitoring_for_Manifold_Data.pdf}
}

@article{delcastillo2020a,
  title = {Industrial Statistics and Manifold Data},
  author = {{del Castillo}, Enrique and Zhao, Xueqi},
  year = {2020},
  journal = {Quality Engineering},
  volume = {32},
  number = {2},
  pages = {155--167},
  publisher = {{Taylor \& Francis}},
  issn = {0898-2112},
  doi = {10.1080/08982112.2019.1641608},
  urldate = {2023-07-12},
  abstract = {Complex and not only big data exist everywhere in industry and how to control and optimize systems based on these data types is an important aspect of modern Quality Engineering. One fundamental type of complexity occurs when data lies on a lower dimensional, curved subspace or manifold. We review a new approach for statistical process monitoring of point cloud, mesh and voxel data based on intrinsic geometrical features of the 2-D manifold (surfaces) of scanned manufactured parts. Monitoring intrinsic properties avoids computationally expensive registration pre-processing of the data sets. We also present a review of recent approaches for analyzing and designing experiments where either the response or the covariates lie on manifolds.},
  keywords = {analysis of variance,design of experiments,Discussion of ``Industrial statistics and manifold data'',statistical modeling,statistical process control,statistics on manifolds},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/del_Castillo_Zhao_2020_Industrial_statistics_and_manifold_data.pdf;/home/dede/Zotero/storage/H8ME5B39/Rejoinder.pdf}
}

@article{duncan1956,
  title = {The {{Economic Design}} of {{X Charts Used}} to {{Maintain Current Control}} of a {{Process}}},
  author = {Duncan, Acheson J.},
  year = {1956},
  journal = {Journal of the American Statistical Association},
  volume = {51},
  number = {274},
  pages = {228--242},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1956.10501322},
  urldate = {2023-10-18},
  abstract = {This paper establishes a criterion that measures approximately the average net income of a process under surveillance of an X chart when the process is subject to random shifts in the process mean. The quality control rule assumed is that an assignable cause is looked for whenever a point falls outside the control limits. The criterion is for the case in which it is assumed that the process is not shut down while the search for the assignable cause is in progress, nor is the cost of adjustment or repair and the cost of bringing the process back into a state of control after the assignable cause is discovered charged to the control chart program. The paper shows how to determine the sample size, the interval between samples, and the control limits that will yield approximately maximum average net income. Numerical examples of optimum design are studied to see how variation in the various risk and cost factors affects the optimum. * The writer is greatly indebted to I. R. Savage and G. Greggory of Stanford University for their criticism and suggestions in preparation of this paper. The paper was completed while the writer was working at Stanford University under the auspices of the Office of Naval Research.}
}

@book{efron1993,
  title = {An {{Introduction}} to the {{Bootstrap}}},
  author = {Efron, Bradley and Tibshirani, R. J.},
  year = {1993},
  edition = {},
  publisher = {{Chapman and Hall/CRC}},
  address = {{New York}},
  abstract = {Statistics is a subject of many uses and surprisingly few effective practitioners. The traditional road to statistical knowledge is blocked, for most, by a formidable wall of mathematics. The approach in An Introduction to the Bootstrap avoids that wall. It arms scientists and engineers, as well as statisticians, with the computational techniques they need to analyze and understand complicated data sets.},
  isbn = {978-0-412-04231-7},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Efron_Tibshirani_1993_An_Introduction_to_the_Bootstrap.pdf}
}

@article{egozcue2006,
  title = {Hilbert {{Space}} of {{Probability Density Functions Based}} on {{Aitchison Geometry}}},
  author = {Egozcue, J. J. and {D{\'i}az{\textendash}Barrero}, J. L. and {Pawlowsky{\textendash}Glahn}, V.},
  year = {2006},
  journal = {Acta Mathematica Sinica},
  volume = {22},
  number = {4},
  pages = {1175--1182},
  issn = {1439-7617},
  doi = {10.1007/s10114-005-0678-2},
  urldate = {2023-10-14},
  abstract = {The set of probability functions is a convex subset of L1 and it does not have a linear space structure when using ordinary sum and multiplication by real constants. Moreover, difficulties arise when dealing with distances between densities. The crucial point is that usual distances are not invariant under relevant transformations of densities. To overcome these limitations, Aitchison's ideas on compositional data analysis are used, generalizing perturbation and power transformation, as well as the Aitchison inner product, to operations on probability density functions with support on a finite interval. With these operations at hand, it is shown that the set of bounded probability density functions on finite intervals is a pre\textendash Hilbert space. A Hilbert space of densities, whose logarithm is square\textendash integrable, is obtained as the natural completion of the pre\textendash Hilbert space.},
  langid = {english},
  keywords = {42B05,46C15,Aitchison distance,Bayes' theorem,Fourier coefficients,Haar basis,Least squares approximation,Simplex},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Egozcue_et_al_2006_Hilbert_Space_of_Probability_Density_Functions_Based_on_Aitchison_Geometry.pdf}
}

@book{ellis1994,
  title = {\proglang{Fortran 90} {{Programming}}},
  author = {Ellis, T. M. R. and Philips, Ivor R. and Lahey, Thomas M.},
  year = {1994},
  publisher = {{Addison-Wesley}},
  address = {{Wokingham, England; Reading, Mass}},
  abstract = {This complete tutorial by three authors who contributed to the development ofthe Fortran 90 standards places a special emphasis on science and engineeringapplications. Building on the lead author's auccessful work, Fortran 77, 2ndEd., the book provides Fortran 90.},
  isbn = {978-0-201-54446-6},
  langid = {english}
}

@article{english2000,
  title = {Detecting Changes in Autoregressive Processes with {{X}}\textasciimacron and {{EWMA}} Charts},
  author = {English, John R. and Lee, Sen-Chin and Martin, Terry W. and Tilmon, Chuck},
  year = {2000},
  journal = {IIE Transactions},
  volume = {32},
  number = {12},
  pages = {1103--1113},
  publisher = {{Taylor \& Francis}},
  issn = {0740-817X},
  doi = {10.1080/07408170008967465},
  urldate = {2023-10-21},
  abstract = {The traditional use of control charts necessarily assumes the independence of data. It is now recognized that many processes are autocorrelated thus violating the fundamental assumption of independence. As a result, there is a need for a broader approach to SPC when data are time-dependent or autocorrelated. This paper utilizes control charts with fixed control limits for residuals to monitor the performance of a process yielding time-dependent data subject to shifts in the mean and the autocorrelation structure. The effectiveness of the framework is evaluated by an average ran length study of both X\textasciimacron and EWMA charts using analytical and simulation techniques. Average run lengths are tabulated for various process disturbance scenarios, and recommendations for the most effective monitoring tool are made. The findings of this research present motivation to extend the traditional paradigms of a shifted process (e.g., mean and/or variance). The results show that decreases in the underlying time series parameters are practically impossible to detect with standard control charts. Furthermore, the practitioner is motivated to employ runs rules since the runs are more likely with time-dependent observations.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/ENGLISH_et_al_2000_Detecting_changes_in_autoregressive_processes_with_X¯and_EWMA_charts.pdf}
}

@article{erdman2008,
  title = {\pkg{bcp}: {{An \proglang{R} Package}} for {{Performing}} a {{Bayesian Analysis}} of {{Change Point Problems}}},
  shorttitle = {Bcp},
  author = {Erdman, Chandra and Emerson, John W.},
  year = {2008},
  journal = {Journal of Statistical Software},
  volume = {23},
  pages = {1--13},
  issn = {1548-7660},
  doi = {10.18637/jss.v023.i03},
  urldate = {2023-07-01},
  abstract = {Barry and Hartigan (1993) propose a Bayesian analysis for change point problems.  We provide a brief summary of selected work on change point problems, both preceding and following Barry and Hartigan. We outline Barry and Hartigan's approach and offer a new \proglang{R} package, bcp (Erdman and Emerson 2007), implementing their analysis.  We discuss two frequentist alternatives to the Bayesian analysis, the recursive circular binary segmentation algorithm (Olshen and Venkatraman 2004) and the dynamic programming algorithm of (Bai and Perron 2003). We illustrate the application of bcp with economic and microarray data from the literature.},
  copyright = {Copyright (c) 2007 Chandra Erdman, John W. Emerson},
  langid = {english},
  keywords = {todo}
}

@article{flach2017,
  title = {Multivariate Anomaly Detection for {{Earth}} Observations: A Comparison of Algorithms and Feature Extraction Techniques},
  shorttitle = {Multivariate Anomaly Detection for {{Earth}} Observations},
  author = {Flach, Milan and Gans, Fabian and Brenning, Alexander and Denzler, Joachim and Reichstein, Markus and Rodner, Erik and Bathiany, Sebastian and Bodesheim, Paul and Guanche, Yanira and Sippel, Sebastian and Mahecha, Miguel D.},
  year = {2017},
  journal = {Earth System Dynamics},
  volume = {8},
  number = {3},
  pages = {677--696},
  issn = {2190-4987},
  doi = {10.5194/esd-8-677-2017},
  urldate = {2023-10-20},
  abstract = {Abstract. Today, many processes at the Earth's surface are constantly monitored by multiple data streams. These observations have become central to advancing our understanding of vegetation dynamics in response to climate or land use change. Another set of important applications is monitoring effects of extreme climatic events, other disturbances such as fires, or abrupt land transitions. One important methodological question is how to reliably detect anomalies in an automated and generic way within multivariate data streams, which typically vary seasonally and are interconnected across variables. Although many algorithms have been proposed for detecting anomalies in multivariate data, only a few have been investigated in the context of Earth system science applications. In this study, we systematically combine and compare feature extraction and anomaly detection algorithms for detecting anomalous events. Our aim is to identify suitable workflows for automatically detecting anomalous patterns in multivariate Earth system data streams. We rely on artificial data that mimic typical properties and anomalies in multivariate spatiotemporal Earth observations like sudden changes in basic characteristics of time series such as the sample mean, the variance, changes in the cycle amplitude, and trends. This artificial experiment is needed as there is no gold standard for the identification of anomalies in real Earth observations. Our results show that a well-chosen feature extraction step (e.g., subtracting seasonal cycles, or dimensionality reduction) is more important than the choice of a particular anomaly detection algorithm. Nevertheless, we identify three~detection algorithms (k-nearest neighbors mean distance, kernel density estimation, a recurrence approach) and their combinations (ensembles) that outperform other multivariate approaches as well as univariate extreme-event detection methods. Our results therefore provide an effective workflow to automatically detect anomalies in Earth system science data.},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Flach_et_al_2017_Multivariate_anomaly_detection_for_Earth_observations.pdf}
}

@article{gahrooei2019,
  title = {An Adaptive Fused Sampling Approach of High-Accuracy Data in the Presence of Low-Accuracy Data},
  author = {Gahrooei, Mostafa Reisi and Paynabar, Kamaran and Pacella, Massimo and Colosimo, Bianca Maria},
  year = {2019},
  journal = {IISE Transactions},
  volume = {51},
  number = {11},
  pages = {1251--1264},
  publisher = {{Taylor \& Francis}},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gahrooei_et_al_2019_An_adaptive_fused_sampling_approach_of_high-accuracy_data_in_the_presence_of2.pdf}
}

@article{gan1991,
  title = {Computing the {{Percentage Points}} of the {{Run Length Distribution}} of an {{Exponentially Weighted Moving Average Control Chart}}},
  author = {Gan, F. F.},
  year = {1991},
  journal = {Journal of Quality Technology},
  volume = {23},
  number = {4},
  pages = {359--365},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.1991.11979353},
  urldate = {2023-09-13},
  abstract = {A computer program that computes the probability function of the run length of an exponentially weighted moving average control chart is presented. The percentage points of the run length distribution are then obtained from the probability function. A normal distribution is assumed.},
  keywords = {Normal Distribution,Numerical Integration,Probability Function,Statistical Process Control,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gan_1991_Computing_the_Percentage_Points_of_the_Run_Length_Distribution_of_an.pdf}
}

@article{gan1993,
  title = {An {{Optimal Design}} of {{EWMA Control Charts Based}} on {{Median Run Length}}},
  author = {Gan, F. F.},
  year = {1993},
  journal = {Journal of Statistical Computation and Simulation},
  volume = {45},
  number = {3-4},
  pages = {169--184},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10.1080/00949659308811479},
  urldate = {2023-09-13},
  abstract = {The in-control run length distribution of an exponentially weighted moving average (EWMA) control chart is highly skewed, hence interpretation based on the average run length (ARL) can be misleading. In addition, the skewness of the run length distribution changes according to the shift in the mean and this presents more difficulty in any interpretation based on ARL. With respect to a run length distribution, the median run length (MRL) is a more meaningful quantity to depend on since interpretation based on MRL is more readily understood. Two methods of computing the MRL of an EWMA chart are examined with emphasis given to the numerical accuracy. An optimal design of EWMA charts based on MRL is proposed and graphs are provided for determining the chart parameters of an optimal EWMA chart. A normal process is assumed.},
  keywords = {Average run length (ARL),Exponentially weighted moving average,Normal distribution,Statistical quality control,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gan_1993_An_optimal_design_of_ewma_control_charts_based_on_median_run_length.pdf}
}

@article{gan1994,
  title = {An {{Optimal Design}} of {{Cumulative Sum Control Chart Based}} on {{Median Run Length}}},
  author = {Gan, F. F.},
  year = {1994},
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {23},
  number = {2},
  pages = {485--503},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610919408813183},
  urldate = {2023-09-13},
  abstract = {Designs of cumulative sum (CUSUM) control charts have traditionally been based on the average run length (ARL). However,interpretatios based on the ARL can be misleading as the in-control run length distribution of a CUSUM chart is highly skewed. Any meaningful interpretation based on the ARL is complicated by the fact that the form of the ren length distribution changes according to the shift in the process mean, and for certain shifts, the run length distributions are almost symmetric. For a run length distribution which can vary from a bighly skewed distribution to an almost symmetric distribution with respect to the sift, the median run length (MRL) is a more meaningful quantity to depend on since interpretation based on the MRL is more readily understood. An important reason for the wide sqread and easily. On the other hand, the MRL and in general, percentage points of run length distribution of a CUSUM chart can be computed accurately and easily. On the other hand, the MRL and in general, percentage points of run length distribution of a CUSUM chart are much harder to compute. Two methods of computing the MRL of a CUSUM chart are examined with emphasis given to the numerical accuracy. An optional design of CUSUM chart based on the MRL is proposed and graphs are provided such that the chart parameters of an optimal CUSUM chart can be determined easily, A normal process is assumed.},
  keywords = {average run length (ARL): normal distribution; run length distribution,done,statistical quality control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gan_1994_An_optimal_design_of_cumulative_sum_control_chart_based_on_median_run_length.pdf}
}

@article{gan1995,
  title = {Joint {{Monitoring}} of {{Process Mean}} and {{Variance Using Exponentially Weighted Moving Average Control Charts}}},
  author = {Gan, F. F.},
  year = {1995},
  journal = {Technometrics},
  volume = {37},
  number = {4},
  eprint = {1269736},
  eprinttype = {jstor},
  pages = {446--453},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1269736},
  urldate = {2023-09-13},
  abstract = {The performances of control-charting schemes for the joint monitoring of a process mean and a variance are evaluated and compared. Certain schemes are shown to be inadequate for this purpose, and the potential risk of using these schemes alone is discussed. A combined scheme consisting of a two-sided exponentially weighted moving average (EWMA) mean chart and a two-sided EWMA variance chart is found to perform well under several types of out-of-control situations. A simple method is provided for approximating the average run length and the percentage points of a run-length distribution of a combined EWMA scheme. A simple procedure is also proposed for the design of a combined EWMA scheme.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gan_1995_Joint_Monitoring_of_Process_Mean_and_Variance_Using_Exponentially_Weighted.pdf}
}

@article{gandy2013a,
  title = {Guaranteed {{Conditional Performance}} of {{Control Charts}} via {{Bootstrap Methods}}},
  author = {Gandy, Axel and Kval{\o}y, Jan Terje},
  year = {2013},
  journal = {Scandinavian Journal of Statistics},
  volume = {40},
  number = {4},
  pages = {647--668},
  issn = {1467-9469},
  doi = {10.1002/sjos.12006},
  urldate = {2023-10-21},
  abstract = {AbstractTo use control charts in practice, the in-control state usually has to be estimated. This estimation has a detrimental effect on the performance of control charts, which is often measured by the false alarm probability or the average run length. We suggest an adjustment of the monitoring schemes to overcome these problems. It guarantees, with a certain probability, a conditional performance given the estimated in-control state. The suggested method is based on bootstrapping the data used to estimate the in-control state. The method applies to different types of control charts, and also works with charts based on regression models. If a non-parametric bootstrap is used, the method is robust to model errors. We show large sample properties of the adjustment. The usefulness of our approach is demonstrated through simulation studies.},
  copyright = {\textcopyright{} 2013 Board of the Foundation of the Scandinavian Journal of Statistics.},
  langid = {english},
  keywords = {bootstrap,confidence interval,control chart,CUSUM,estimation error,guaranteed performance,monitoring},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gandy_Kvaloy_2013_Guaranteed_Conditional_Performance_of_Control_Charts_via_Bootstrap_Methods.pdf;/home/dede/Zotero/storage/WLURNM4P/sjos.html}
}

@article{gandy2017,
  title = {\pkg{spcadjust}: {{Functions}} for {{Calibrating Control Charts}}},
  shorttitle = {Spcadjust},
  author = {Gandy, Axel and Kval{\o}y, Jan Terje},
  year = {2017},
  journal = {{The \proglang{R} Journal}},
  doi = {10.32614/RJ-2017-014},
  pages = {458--476},
  volume = {9},
  number = {1}
}

@article{garg2014,
  title = {A Hybrid {{M5}}'-Genetic Programming Approach for Ensuring Greater Trustworthiness of Prediction Ability in Modelling of {{FDM}} Process},
  author = {Garg, A. and Tai, K. and Lee, C. H. and Savalani, M. M.},
  year = {2014},
  journal = {Journal of Intelligent Manufacturing},
  volume = {25},
  number = {6},
  pages = {1349--1365},
  issn = {1572-8145},
  doi = {10.1007/s10845-013-0734-1},
  urldate = {2023-10-13},
  abstract = {Recent years have seen various rapid prototyping (RP) processes such as fused deposition modelling (FDM) and three-dimensional printing being used for fabricating prototypes, leading to shorter product development times and less human intervention. The literature reveals that the properties of RP built parts such as surface roughness, strength, dimensional accuracy, build cost, etc are related to and can be improved by the appropriate settings of the input process parameters. Researchers have formulated physics-based models and applied empirical modelling techniques such as regression analysis and artificial neural network for the modelling of RP processes. Physics-based models require in-depth understanding of the processes which is a formidable task due to their complexity. The issue of improving trustworthiness of the prediction ability of empirical models on test (unseen) samples is paid little attention. In the present work, a hybrid M5\$\$\^\{\textbackslash prime \}\$\$-genetic programming (M5\$\$\^\{\textbackslash prime \}\$\$-GP) approach is proposed for empirical modelling of the FDM process with an attempt to resolve this issue of ensuring trustworthiness. This methodology is based on the error compensation achieved using a GP model in parallel with a M5\$\$\^\{\textbackslash prime \}\$\$model. The performance of the proposed hybrid model is compared to those of support vector regression (SVR) and adaptive neuro fuzzy inference system (ANFIS) model and it is found that the M5\$\$\^\{\textbackslash prime \}\$\$-GP model has the goodness of fit better than those of the SVR and ANFIS models.},
  langid = {english},
  keywords = {Artificial neural network,Fused deposition modelling,Genetic programming,M
5
{${'}$}
~M5{${'}$},Rapid prototyping,Support vector regression,Trustworthiness},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Garg_et_al_2014_A_hybrid_$$-text _M 5^-prime_$$-genetic_programming_approach_for_ensuring.pdf}
}

@book{gibson2015,
  title = {Additive {{Manufacturing Technologies}}: {{3D Printing}}, {{Rapid Prototyping}}, and {{Direct Digital Manufacturing}}},
  shorttitle = {Additive {{Manufacturing Technologies}}},
  author = {Gibson, Ian and Rosen, David and Stucker, Brent},
  year = {2015},
  publisher = {{Springer-Verlag}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4939-2113-3},
  urldate = {2023-10-13},
  isbn = {978-1-4939-2112-6 978-1-4939-2113-3},
  langid = {english},
  keywords = {3D CAD data,Coating technologies,Layer-based fabrication,Layer-based manufacturing,Prototyping applications,Rapid prototyping,RP techniques},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gibson_et_al_2015_Additive_Manufacturing_Technologies.pdf}
}

@article{graham2017,
  title = {Design and {{Implementation Issues}} for a {{Class}} of {{Distribution-Free Phase II EWMA Exceedance Control Charts}}},
  author = {Graham, Marien A. and Mukherjee, Amitava and Chakraborti, Subhabrata},
  year = {2017},
  journal = {International Journal of Production Research},
  volume = {55},
  number = {8},
  pages = {2397--2430},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7543},
  doi = {10.1080/00207543.2016.1249428},
  urldate = {2023-09-13},
  abstract = {Distribution-free (nonparametric) control charts can play an essential role in process monitoring when there is dearth of information about the underlying distribution. In this paper, we study various aspects related to an efficient design and execution of a class of nonparametric Phase II exponentially weighted moving average (denoted by NPEWMA) charts based on exceedance statistics. The choice of the Phase I (reference) sample order statistic used in the design of the control chart is investigated. We use the exact time-varying control limits and the median run-length as the metric in an in-depth performance study. Based on the performance of the chart, we outline implementation strategies and make recommendations for selecting this order statistic from a practical point of view and provide illustrations with a data-set. We conclude with a summary and some remarks.},
  keywords = {average run-length (ARL),exponentially weighted moving average (EWMA),median run-length (MRL),nonparametric,order statistic,precedence,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Graham_et_al_2017_Design_and_implementation_issues_for_a_class_of_distribution-free_Phase_II_EWMA.pdf}
}

@article{grasso2016,
  title = {In-{{Process Monitoring}} of {{Selective Laser Melting}}: {{Spatial Detection}} of {{Defects Via Image Data Analysis}}},
  shorttitle = {In-{{Process Monitoring}} of {{Selective Laser Melting}}},
  author = {Grasso, Marco and Laguzza, Vittorio and Semeraro, Quirico and Colosimo, Bianca Maria},
  year = {2016},
  journal = {Journal of Manufacturing Science and Engineering},
  volume = {139},
  number = {5},
  issn = {1087-1357},
  doi = {10.1115/1.4034715},
  urldate = {2023-07-02},
  abstract = {Selective laser melting (SLM) has been attracting a growing interest in different industrial sectors as it is one of the key technologies for metal additive manufacturing (AM). Despite the relevant improvements made by the SLM technology in the recent years, process capability is still a major issue for its industrial breakthrough. As a matter of fact, different kinds of defect may originate during the layerwise process. In some cases, they propagate from one layer to the following ones leading to a job failure. In other cases, they are hardly visible and detectable by inspecting the final part, as they can affect the internal structure or structural features that are difficult to measure. This implies the need for in-process monitoring methods able to rapidly detect and locate defect onsets during the process itself. Different authors have been investigating machine sensorization architectures, but the development of statistical monitoring techniques is still in a very preliminary phase. This paper proposes a method for the detection and spatial identification of defects during the layerwise process by using a machine vision system in the visible range. A statistical descriptor based on principal component analysis (PCA) applied to image data is presented, which is suitable to identify defective areas of a layer. The use of image k-means clustering analysis is then proposed for automated defect detection. A real case study in SLM including both simple and complicated geometries is discussed to demonstrate the performances of the method.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Grasso_et_al_2016_In-Process_Monitoring_of_Selective_Laser_Melting.pdf;/home/dede/Zotero/storage/7HXDSVB2/In-Process-Monitoring-of-Selective-Laser-Melting.html}
}

@article{grigg2004,
  title = {An {{Overview}} of {{Risk-Adjusted Charts}}},
  author = {Grigg, O. and Farewell, V.},
  year = {2004},
  journal = {Journal of the Royal Statistical Society A (Statistics in Society)},
  volume = {167},
  number = {3},
  eprint = {3559779},
  eprinttype = {jstor},
  pages = {523--539},
  publisher = {{[John Wiley \& Sons, Royal Statistical Society]}},
  issn = {0964-1998},
  urldate = {2023-10-31},
  abstract = {The paper provides an overview of risk-adjusted charts, with examples based on two data sets: the first consisting of outcomes following cardiac surgery and patient factors contributing to the Parsonnet score; the second being age-sex-adjusted death-rates per year under a single general practitioner. Charts presented include the cumulative sum (CUSUM), resetting sequential probability ratio test, the sets method and Shewhart chart. Comparisons between the charts are made. Estimation of the process parameter and two-sided charts are also discussed. The CUSUM is found to be the least efficient, under the average run length (ARL) criterion, of the resetting sequential probability ratio test class of charts, but the ARL criterion is thought not to be sensible for comparisons within that class. An empirical comparison of the sets method and CUSUM, for binary data, shows that the sets method is more efficient when the in-control ARL is small and more efficient for a slightly larger range of in-control ARLs when the change in parameter being tested for is larger. The Shewart p-chart is found to be less efficient than the CUSUM even when the change in parameter being tested for is large.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Grigg_Farewell_2004_An_Overview_of_Risk-Adjusted_Charts.pdf}
}

@article{han2007,
  title = {Detection and {{Diagnosis}} of {{Unknown Abrupt Changes Using CUSUM Multi-Chart Schemes}}},
  author = {Han, Dong and Tsung, Fugee},
  year = {2007},
  journal = {Sequential Analysis},
  volume = {26},
  number = {3},
  pages = {225--249},
  publisher = {{Taylor \& Francis}},
  issn = {0747-4946},
  doi = {10.1080/07474940701404765},
  urldate = {2023-09-13},
  abstract = {A cumulative sum (CUSUM) multi-chart scheme that consists of multiple CUSUM control charts is studied for detecting and diagnosing an unknown abrupt change in a stochastic system on the basis of sequential observations. We prove that the CUSUM multi-chart not only has a high diagnostic capability but also possesses a better detection performance than individual CUSUM charts when the in-control average run length is large. We also present an optimal design of the CUSUM multi-chart and two illustrative examples involving the normal and exponential distributions. Moreover, numerical comparisons of the average run lengths are made via Monte Carlo simulation among the CUSUM, generalized likelihood ratio, exponentially weighted moving average (EWMA), multi-chart, and CUSUM multi-chart. The numerical results indicate that the CUSUM multi-chart has the best performance on the whole among the five schemes in detecting the unknown mean shift.},
  keywords = {62L10,62N10,Asymptotic optimality,Kullback\textendash Leibler information distances,Online detection and diagnosis,Sequential analysis,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Han_Tsung_2007_Detection_and_Diagnosis_of_Unknown_Abrupt_Changes_Using_CUSUM_Multi-Chart.pdf}
}

@article{han2007a,
  title = {{{CUSUM}} and {{EWMA Multi-Charts}} for {{Detecting}} a {{Range}} of {{Mean Shifts}}},
  author = {Han, Dong and Tsung, Fugee and Hu, Xijian and Wang, Kaibo},
  year = {2007},
  journal = {Statistica Sinica},
  volume = {17},
  number = {3},
  eprint = {24307716},
  eprinttype = {jstor},
  pages = {1139--1164},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  urldate = {2023-09-13},
  abstract = {The multi-chart consists of several CUSUM or EWMA charts with different reference values that are used simultaneously to detect anticipated process changes. We not only prove that the chart can quickly achieve the asymptotic optimal bound, but also give an integral equation to determine the reference values to arrive at optimality. Simulation results are used to verify the theoretical optimal properties and to show that the CUSUM multi-chart is superior on the whole to single CUSUM, single EWMA, and EWMA multi-charts in terms of run length and robustness, and can compete with GLR control charts in detecting a range of various mean shifts. We investigate the design of both CUSUM and EWMA multi-charts. Some practical guidelines are provided for determining multi-chart parameters, such as the number of constituent charts and the allocation of their reference values.},
  keywords = {todo}
}

@book{hausdorff1914,
  title = {{Grundz\"uge der Mengenlehre}},
  author = {Hausdorff, Felix},
  year = {1914},
  publisher = {{Leipzig Viet}},
  urldate = {2023-10-10},
  abstract = {14},
  collaborator = {{Gerstein - University of Toronto}},
  langid = {german},
  lccn = {ABD-1847},
  keywords = {Set theory}
}

@article{hawkins1987,
  title = {Self-{{Starting Cusum Charts}} for {{Location}} and {{Scale}}},
  author = {Hawkins, Douglas M.},
  year = {1987},
  journal = {Journal of the Royal Statistical Society D (The Statistician)},
  volume = {36},
  number = {4},
  eprint = {2348827},
  eprinttype = {jstor},
  pages = {299--316},
  publisher = {{[Royal Statistical Society, John Wiley \& Sons]}},
  issn = {0039-0526},
  doi = {10.2307/2348827},
  urldate = {2021-11-03},
  abstract = {In some quality control problems, it is not known what the exact process mean and standard deviation are under control but it is desired to determine whether there have been drifts from the conditions obtained at the process start-up. This situation is not well-covered by standard cumulative sum procedures, which generally assume known process parameters. This paper uses the running mean and standard deviation of all observations made on the process since start-up as substitutes for the unknown true values of the process mean and standard deviation. Using some theoretical properties of independence of residuals, two pairs of cusums are set up: one testing for constancy of location of the process, and the other for constancy of the spread. While the process is under control, both these cusum pairs are of approximately normal \$N(0, 1)\$ quantities (and therefore are well understood), but if the location, the spread or both change, then non-centrality is introduced into one or both of the location and scale cusum pairs, and it drifts out-of-control. It is shown that the procedure performs well in detecting changes in the process, even in comparison with the often utopian situation in which the process mean and variance are known exactly prior to the start of the cusum.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hawkins_1987_Self-Starting_Cusum_Charts_for_Location_and_Scale.pdf}
}

@article{hawkins1991,
  title = {Multivariate {{Quality Control Based}} on {{Regression-Adjusted Variables}}},
  author = {Hawkins, Douglas M.},
  year = {1991},
  journal = {Technometrics},
  volume = {33},
  number = {1},
  eprint = {1269008},
  eprinttype = {jstor},
  pages = {61--75},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1269008},
  urldate = {2023-10-23},
  abstract = {When performing quality control in a situation in which measures are made of several possibly related variables, it is desirable to use methods that capitalize on the relationship between the variables to provide controls more sensitive than those that may be made on the variables individually. The most common methods of multivariate quality control that assess the vector of variables as a whole are those based on the Hotelling T\textsuperscript{2} between the variables and the specification vector. Although T\textsuperscript{2} is the optimal single-test statistic for a general multivariate shift in the mean vector, it is not optimal for more structured mean shifts-for example, shifts in only some of the variables. Measures based on quadratic forms (like T\textsuperscript{2}) also confound mean shifts with variance shifts and require quite extensive analysis following a signal to determine the nature of the shift. This article proposes Shewhart and cumulative sum (CUSUM) controls based on the vector Z of scaled residuals from the regression of each variable on all others. Each component of Z is the (Neyman-Pearson) optimal single-test statistic for testing whether that variable is shifted in mean. The Shewhart charts plot the components of Z, and the CUSUM charts are based on accumulation of components of Z, leading one to anticipate good performance by the charts. This is verified by some average run length calculations. The vector Z also has the valuable interpretive property that signals given are for shifts in the mean, or shifts in the variance, of particular variables rather than global signals indicating some unspecified departure from control.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hawkins_1991_Multivariate_Quality_Control_Based_on_Regression-Adjusted_Variables.pdf}
}

@article{hawkins2003,
  title = {The {{Changepoint Model}} for {{Statistical Process Control}}},
  author = {Hawkins, Douglas M. and Qiu, Peihua and Kang, Chang Wook},
  year = {2003},
  journal = {Journal of Quality Technology},
  volume = {35},
  number = {4},
  pages = {355--366},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2003.11980233},
  urldate = {2021-11-03},
  abstract = {Statistical process control (SPC) requires statistical methodologies that detect changes in the pattern of data over time. The common methodologies, such as Shewhart, cumulative sum (cusum), and exponentially weighted moving average (EWMA) charting, require the in-control values of the process parameters, but these are rarely known accurately. Using estimated parameters, the run length behavior changes randomly from one realization to another, making it impossible to control the run length behavior of any particular chart. A suitable methodology for detecting and diagnosing step changes based on imperfect process knowledge is the unknown-parameter changepoint formulation. Long recognized as a Phase I analysis tool, we argue that it is also highly effective in allowing the user to progress seamlessly from the start of Phase I data gathering through Phase II SPC monitoring. Despite not requiring specification of the post-change process parameter values, its performance is never far short of that of the optimal cusum chart which requires this knowledge, and it is far superior for shifts away from the cusum shift for which the cusum chart is optimal. As another benefit, while changepoint methods are designed for step changes that persist, they are also competitive with the Shewhart chart, the chart of choice for isolated non-sustained special causes.},
  keywords = {Cumulative Sum Control Charts,done,Exponentially Weighted Moving Average Control Charts,Shewhart Control Charts},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hawkins_et_al_2003_The_Changepoint_Model_for_Statistical_Process_Control.pdf;/home/dede/Zotero/storage/XN95JT6L/00224065.2003.html}
}

@article{hawkins2008,
  title = {Multivariate {{Exponentially Weighted Moving Covariance Matrix}}},
  author = {Hawkins, Douglas M. and {Maboudou-Tchao}, Edgard M.},
  year = {2008},
  journal = {Technometrics},
  volume = {50},
  number = {2},
  eprint = {25471456},
  eprinttype = {jstor},
  pages = {155--166},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  urldate = {2020-12-21},
  abstract = {Multivariate exponentially weighted moving average (MEWMA) charts are among the best control charts for detecting small changes in any direction. The well-known MEWMA is directed at changes in the mean vector. But changes can occur in either the location or the variability of the correlated multivariate quality characteristics, calling for parallel methodologies for detecting changes in the covariance matrix. This article discusses an exponentially weighted moving covariance matrix for monitoring the stability of the covariance matrix of a process. Used together with the location MEWMA, this chart provides a way to satisfy Shewhart's dictum that proper process control monitor both mean and variability. We show that the chart is competitive, generally outperforming current control charts for the covariance matrix.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hawkins_Maboudou-Tchao_2008_Multivariate_Exponentially_Weighted_Moving_Covariance_Matrix.pdf}
}

@article{hawkins2009,
  title = {Combined {{Charts}} for {{Mean}} and {{Variance Information}}},
  author = {Hawkins, Douglas M. and Deng, Qiqi},
  year = {2009},
  journal = {Journal of Quality Technology},
  volume = {41},
  number = {4},
  pages = {415--425},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2009.11917795},
  urldate = {2023-10-31},
  abstract = {Since the earliest days of statistical process control, it has been recognized that it is essential to monitor both mean and variability and to sound an alarm if either of these characteristics shows sign of a special cause. Traditionally, this has been done by using separate \=U and S charts, with the implication that if either chart exceeds its control limit action will be taken to diagnose and resolve any special causes. It is, however, possible to monitor both mean and variance in a single chart responsive to shifts in either, and several proposals along these lines have been published. Two new combination charts are proposed, and it is shown that, in addition to the simplicity of a single chart rather than two, a combination chart may have significant performance advantages over the \=U and S chart pair. Good practice augments the traditional Shewhart charts with cumulative sum or exponentially weighted moving average charts for location and for scale; these pairs may also be replaced by combination charts.},
  keywords = {Fisher Methods,Likelihood Ratio,Shewhart Control Charts,todo}
}

@article{he2017b,
  title = {Enhancing the Monitoring of {{3D}} Scanned Manufactured Parts through Projections and Spatiotemporal Control Charts},
  author = {He, Ketai and Zhang, Min and Zuo, Ling and Alhwiti, Theyab and Megahed, Fadel M.},
  year = {2017},
  journal = {Journal of Intelligent Manufacturing},
  volume = {28},
  number = {4},
  pages = {899--911},
  issn = {1572-8145},
  doi = {10.1007/s10845-014-1025-1},
  urldate = {2023-10-10},
  abstract = {As measurement technologies evolve, our ability to detect, isolate and diagnose process faults on the shop-floor is rapidly changing. Three dimensional scanners provide the opportunity to capture the entire surface geometry of a manufactured part and allow for the detection of a wide variety of fault patterns that may not be captured by traditional measurement devices. Despite their advantages, their use in practice is limited due to the complexities associated with the analysis of 3D laser scan data (point clouds). Therefore, the objective of our work is to allow practitioners to fully utilize the inherent advantages of point clouds by providing a framework that can facilitate their analysis and visualization. More specifically, we transform point clouds into 2D images (without the loss of any spatial information) to benefit from the image analysis and monitoring techniques that are currently being implemented on the shop-floor. We provide numerical and experimental examples to illustrate and validate the advantages of our proposed method. Finally, we offer advice to practitioners and recommendations for future research.},
  langid = {english},
  keywords = {High-density data,Image monitoring,Map projection,Noncontact scanning systems,Statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/He_et_al_2017_Enhancing_the_monitoring_of_3D_scanned_manufactured_parts_through_projections2.pdf}
}

@article{he2019a,
  title = {Profile Monitoring Based Quality Control Method for Fused Deposition Modeling Process},
  author = {He, Ketai and Zhang, Qian and Hong, Yili},
  year = {2019},
  journal = {Journal of Intelligent Manufacturing},
  volume = {30},
  number = {2},
  pages = {947--958},
  issn = {1572-8145},
  doi = {10.1007/s10845-018-1424-9},
  urldate = {2023-10-13},
  abstract = {In order to monitor the quality of parts in printing, the methodology to monitor the geometric quality of the printed parts in fused deposition modeling process is researched. A non-contact measurement method based on machine vision technology is adopted to obtain the precise complete geometric information. An image acquisition system is established to capture the image of each layer of the part in building and image processing technology is used to obtain the geometric profile information. With the above information, statistical process control method is applied to monitor the geometric quality of the parts during the printing process. Firstly, a border signature method is applied to transform complex geometry into a simple distance-angle function to get the profile deviation data. Secondly, monitoring of the profile deviation data based on profile monitoring method is studied and applied to achieve the goal of layer-to-layer monitoring. In the research, quantile-quantile plot method is used to transform the profile deviation point cloud data monitoring problem into a linear profile relationship monitoring problem and EWMA control charts are established to monitor the parameters of the linear relationship to detect shifts occurred in the Fused Deposition Modeling process. Finally, laboratory experiments are conducted to demonstrate the effectiveness of the proposed approach.},
  langid = {english},
  keywords = {Fused deposition modeling,Machine vision,Profile monitoring,Quality control,Statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/He_et_al_2019_Profile_monitoring_based_quality_control_method_for_fused_deposition_modeling.pdf}
}

@article{hron2016,
  title = {Simplicial Principal Component Analysis for Density Functions in {{Bayes}} Spaces},
  author = {Hron, K. and Menafoglio, A. and Templ, M. and Hr{\r{u}}zov{\'a}, K. and Filzmoser, P.},
  year = {2016},
  journal = {Computational Statistics \& Data Analysis},
  volume = {94},
  pages = {330--350},
  issn = {01679473},
  doi = {10.1016/j.csda.2015.07.007},
  urldate = {2023-10-10},
  abstract = {Semantic Scholar extracted view of "Simplicial principal component analysis for density functions in Bayes spaces" by K. Hron et al.},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hron_et_al_2016_Simplicial_principal_component_analysis_for_density_functions_in_Bayes_spaces.pdf}
}

@article{hu2021,
  title = {Guaranteed {{Conditional Performance}} of the {{Median Run Length Based EWMA X}}\textasciimacron{} {{Chart}} with {{Unknown Process Parameters}}},
  author = {Hu, XueLong and Castagliola, Philippe and Tang, Anan and Zhong, Jianlan},
  year = {2021},
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {50},
  number = {12},
  pages = {4280--4299},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610918.2019.1642485},
  urldate = {2023-09-13},
  abstract = {Exponentially weighted moving average (EWMA) type charts are very popular and efficient in monitoring various kind of statistics. Much researches have been done on the EWMA X\textasciimacron{} chart with known process parameters. But, in practice, the process parameters used to set the control chart limits are often unknown and they need to be estimated from different Phase I samples. Moreover, because the shape of the run length distribution for the EWMA X\textasciimacron{} chart changes with the mean shift, the median run length (MRL) can serve as a good alternative to evaluate the performance of the EWMA X\textasciimacron{} chart. In this article, we will investigate the conditional properties of the EWMA X\textasciimacron{} chart with unknown process parameters based on the MRL metric. In order to investigate the chart's properties, the average MRL (AMRL) and the standard deviation of MRL (SDMRL) will be used together when the process parameters are unknown. To prevent too many lower in-control MRL values, the adjusted control limits of the MRL based EWMA chart are obtained by using a bootstrap type approach and the results show that the adjusted control limits can give a good tradeoff between the in-control and out-of-control MRL performances.},
  keywords = {Average median run length,Estimated parameters,EWMA {$<$}img src="/na101/home/literatum/publisher/tandf/journals/content/lssp20/2021/lssp20.v050.i12/03610918.2019.1642485/20211123/images/lssp\_a\_1642485\_ilm0002.gif" alt="" /{$>$}X\textasciimacron{} chart,Median run length,Standard deviation of median run length,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hu_et_al_2021_Guaranteed_conditional_performance_of_the_median_run_length_based_EWMA_X¯_chart.pdf}
}

@article{huang2002,
  title = {Diagnosis of {{Multi-Operational Machining Processes Through Variation Propagation Analysis}}},
  author = {Huang, Qiang and Zhou, Shiyu and Shi, Jianjun},
  year = {2002},
  journal = {Robotics and Computer-Integrated Manufacturing},
  series = {11th {{International Conference}} on {{Flexible Automation}} and {{Intelligent Manufacturing}}},
  volume = {18},
  number = {3},
  pages = {233--239},
  issn = {0736-5845},
  doi = {10.1016/S0736-5845(02)00014-5},
  urldate = {2023-07-02},
  abstract = {It is a very challenging task to develop effective process control methodologies for multi-operational manufacturing processes. Although Statistical Process Control (SPC) has been widely used as the primary method in the control of quality, it mainly serves as a change detection tool rather than a method to identify root causes of process changes. This paper proposes a systematic approach to overcome the limitations faced by SPC. In this method, a state space variation propagation model is derived from the product and process design information. The virtual machining concept is applied to isolate faults between operations, and further used in the root cause determination. The detailed methodology is presented, and a case study is conducted to illustrate and verify the developed diagnosis method.},
  langid = {english},
  keywords = {Process diagnosis,Root cause identification,todo,Variation propagation},
  file = {/home/dede/Zotero/storage/45HWII8M/S0736584502000145.html}
}

@article{huang2018,
  title = {Detection and Monitoring of Defects on Three-Dimensional Curved Surfaces Based on High-Density Point Cloud Data},
  author = {Huang, Delin and Du, Shichang and Li, Guilong and Zhao, Chen and Deng, Yafei},
  year = {2018},
  journal = {Precision Engineering},
  volume = {53},
  pages = {79--95},
  issn = {0141-6359},
  doi = {10.1016/j.precisioneng.2018.03.001},
  urldate = {2023-10-13},
  abstract = {The surface quality of three-dimensional (3-D) curved surfaces is one of the most important factors that can directly influence the performance of the final product. This paper presents a systematic approach for detection and monitoring of defects on 3-D curved surfaces based on high-density point cloud data. Firstly, an algorithm to remove outliers and a boundary recognition algorithm are proposed to divide the entire 3-D curved surface including millions of measured points into multiple sub-regions. Secondly, two new evaluation indexes based on wavelet packet entropy and normal vector are explored to represent the features of the multiple sub-regions to determine whether the sub-regions are out-of-limit (OOL) of specifications. Thirdly, three quality parameters representing quality characteristics of a curved surface are presented and their values are calculated based on the clusters of OOL sub-regions. Finally, three individual control charts are presented to monitor the three quality parameters. As long as any quality parameter is out of the control range, the manufacturing process of the curved surface is determined to be out-of-control (OOC). The results of a case study show that the proposed approach can effectively identify the OOC manufacturing process and detect defects on 3-D curved surfaces.},
  keywords = {Defect detection and monitoring,High-density point cloud data,Three-dimensional curved surface},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Huang_et_al_2018_Detection_and_monitoring_of_defects_on_three-dimensional_curved_surfaces_based.pdf;/home/dede/Zotero/storage/QFYTT3YZ/S0141635917303215.html}
}

@book{huber1981,
  title = {Robust {{Statistics}}},
  author = {Huber, Peter J.},
  year = {1981},
  series = {Wiley {{Series}} in {{Probability}} and {{Statistics}}},
  edition = {},
  publisher = {{John Wiley \& Sons}},
  doi = {10.1002/0471725250},
  urldate = {2023-10-19},
  isbn = {978-0-471-41805-4 978-0-471-72525-1},
  langid = {english}
}

@article{huwang2007,
  title = {Monitoring {{Multivariate Process Variability}} for {{Individual Observations}}},
  author = {Huwang, Longcheen and Yeh, Arthur B. and Wu, Chien-Wei},
  year = {2007},
  journal = {Journal of Quality Technology},
  volume = {39},
  number = {3},
  pages = {258--278},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2007.11917692},
  urldate = {2023-10-23},
  abstract = {Most of the existing control charts for monitoring multivariate process variability are based on subgroup sizes greater than one. In many practical applications, however, only individual observations are available and the usual control charts are not applicable in these cases. In this paper, two new control charts are proposed to monitor multivariate process variability for individual observations. The proposed control charts are constructed based on the traces of the estimated covariance matrices derived from the individual observations. When there is only one quality characteristic, these two charts respectively reduce to the exponentially weighted mean squared deviation and exponentially weighted moving variance charts. It is shown based on the simulation studies that the proposed charts are superior to the existing ones in detecting increases in variance and changes in correlation. An example from the semiconductor industry is also presented to illustrate the applicability of the proposed charts.},
  keywords = {Covariance Matrix,Exponentially Weighted Moving Average,Individual Observations,todo,Trace},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Huwang_et_al_2007_Monitoring_Multivariate_Process_Variability_for_Individual_Observations.pdf}
}

@article{jiang2012a,
  title = {A {{Variable-Selection-Based Multivariate EWMA Chart}} for {{Process Monitoring}} and {{Diagnosis}}},
  author = {Jiang, Wei and Wang, Kaibo and Tsung, Fugee},
  year = {2012},
  journal = {Journal of Quality Technology},
  volume = {44},
  number = {3},
  pages = {209--230},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2012.11917896},
  urldate = {2023-05-16},
  abstract = {Fault detection and root cause identification are both important tasks in Multivariate Statistical Process Control (MSPC) for improving process and product quality. Most traditional control charts, including Hotelling's T2 chart and the Multivariate Exponential Weighted Moving Average (MEWMA) chart, separate the two tasks into independent and successive procedures by signaling the existence of process faults followed by auxiliary methods to locate root causes. This paper proposes an integrated procedure, a Variable-Selection-based MEWMA (VS-MEWMA) chart, for multivariate process monitoring and fault diagnosis by utilizing dimensionality reduction techniques. The VS-MEWMA chart first locates potentially out-of-control variables via variable selection and then deploys such information in the monitoring statistics with the reduction in dimensionality providing increased sensitivity to out-of-control conditions. When a signal is given, the algorithm also identifies the suspected variables for further root cause diagnosis. Both numerical simulations and real examples are presented to illustrate the performance of the proposed chart, as well as design guidelines.},
  keywords = {Average Run Length,doing,Fault Diagnosis,Forward Selection,MEWMA,Multivariate Statistical Process Control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jiang_et_al_2012_A_Variable-Selection-Based_Multivariate_EWMA_Chart_for_Process_Monitoring_and.pdf}
}

@article{jin1999,
  title = {State {{Space Modeling}} of {{Sheet Metal Assembly}} for {{Dimensional Control}}},
  author = {Jin, Jionghua and Shi, Jianjun},
  year = {1999},
  journal = {Journal of Manufacturing Science and Engineering},
  volume = {121},
  number = {4},
  pages = {756--762},
  issn = {1087-1357},
  doi = {10.1115/1.2833137},
  urldate = {2023-07-02},
  abstract = {In this paper, a state space modeling approach is developed for the dimensional control of sheet metal assembly processes. In this study, a 3-2-1 scheme is assumed for the sheet metal assembly. Several key concepts, such as tooling locating error, part accumulative error, and re-orientation error, are defined. The inherent relationships among these error components are developed. Those relationships finally lead to a state space model which describes the variation propagation throughout the assembly process. An observation equation is also developed to represent the relationship between the observation vector (the in-line OCMM measurement information) and the state vector (the part accumulative error). Potential usage of the developed model is discussed in the paper.},
  keywords = {todo},
  file = {/home/dede/Zotero/storage/C3LFXFUJ/State-Space-Modeling-of-Sheet-Metal-Assembly-for.html}
}

@misc{johnson2023,
  title = {The {{\pkg{NLopt}}} Nonlinear-Optimization Package},
  author = {Johnson, Steven G.},
  year = {2007},
  urldate = {2023-04-18},
  abstract = {library for nonlinear optimization, wrapping many algorithms for global and local, constrained or unconstrained, optimization},
  url = {https://github.com/stevengj/nlopt}
}

@article{jones2012,
  title = {Assessing the Effect of Estimation Error on Risk-Adjusted {{CUSUM}} Chart Performance},
  author = {Jones, Mark A. and Steiner, Stefan H.},
  year = {2012},
  journal = {International Journal for Quality in Health Care: Journal of the International Society for Quality in Health Care},
  volume = {24},
  number = {2},
  pages = {176--181},
  issn = {1464-3677},
  doi = {10.1093/intqhc/mzr082},
  abstract = {BACKGROUND: Risk-adjusted control charts have become popular for monitoring processes that involve the management and treatment of patients in hospitals or other healthcare institutions. However, to date, the effect of estimation error on risk-adjusted control charts has not been studied. METHODS: We studied the effect of estimation error on risk-adjusted binary cumulative sum (CUSUM) performance using actual and simulated data on patients undergoing coronary artery bypass surgery and assessed for mortality up to 30 days post-surgery. The effect of estimation error was indicated by the variability of the 'true' average run lengths (ARLs) obtained using repeated sampling of the observed data under various realistic scenarios. RESULTS: Results showed that estimation error can have a substantial effect on risk-adjusted CUSUM chart performance in terms of variation of true ARLs. Moreover, the performance was highly dependent on the number of events used to derive the control chart parameters and the specified ARL for an in-control process (ARL(0)). However, the results suggest that it is the uncertainty in the overall adverse event rate that is the main component of estimation error. CONCLUSIONS: When designing a control chart, the effect of estimation error could be taken into account by generating a number of bootstrap samples of the available Phase I data and then determining the control limit needed to obtain an ARL(0) of a pre-specified level 95\% of the time. If limited Phase I data are available, it may be advisable to continue to update model parameters even after prospective patient monitoring is implemented.},
  langid = {english},
  pmid = {22190589},
  keywords = {Bias,Confidence Intervals,Data Collection,Hospital Administration,{Quality Assurance, Health Care},Risk Adjustment,todo,United States},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jones_Steiner_2012_Assessing_the_effect_of_estimation_error_on_risk-adjusted_CUSUM_chart.pdf}
}

@incollection{juran1951,
  title = {The {{Economics}} of {{Quality}}},
  booktitle = {Quality {{Control Handbook}}},
  author = {Juran, J. M.},
  editor = {Juran, J. M.},
  year = {1951},
  pages = {1--41},
  publisher = {{McGraw-Hill}},
  address = {{New Tork}}
}

@book{kalender2011,
  title = {Computed {{Tomography}}: {{Fundamentals}}, {{System Technology}}, {{Image Quality}}, {{Applications}}},
  shorttitle = {Computed {{Tomography}}},
  author = {Kalender, Willi A.},
  year = {2011},
  edition = {3rd edition},
  publisher = {{Publicis}},
  address = {{Erlangen}},
  abstract = {The book offers a comprehensive and user-oriented description of the theoretical and technical system fundamentals of computed tomography (CT) for a wide readership, from conventional single-slice acquisitions to volume acquisition with multi-slice and cone-beam spiral CT. It covers in detail all characteristic parameters relevant for image quality and all performance features significant for clinical application. Readers will thus be informed how to use a CT system to an optimum depending on the different diagnostic requirements. This includes a detailed discussion about the dose required and about dose measurements as well as how to reduce dose in CT. All considerations pay special attention to spiral CT and to new developments towards advanced multi-slice and cone-beam CT.  For the third edition most of the contents have been updated and latest topics like dual source CT, dual energy CT, flat detector CT and interventional CT have been added.  The enclosed CD-ROM again offers copies of all figures in the book and attractive case studies, including many examples from the most recent 64-slice acquisitions, and interactive exercises for image viewing and manipulation.  This book is intended for all those who work daily, regularly or even only occasionally with CT: physicians, radiographers, engineers, technicians and physicists. A glossary describes all the important technical terms in alphabetical order.   The enclosed DVD again offers attractive case studies, including many examples from the most recent 64-slice acquisitions, and interactive exercises for image viewing and manipulation.  This book is intended for all those who work daily, regularly or even only occasionally with CT: physicians, radiographers, engineers, technicians and physicists. A glossary describes all the important technical terms in alphabetical order.},
  isbn = {978-3-89578-317-3},
  langid = {english}
}

@article{kang2000,
  title = {On-{{Line Monitoring When}} the {{Process Yields}} a {{Linear Profile}}},
  author = {Kang, Lan and Albin, Susan L.},
  year = {2000},
  journal = {Journal of Quality Technology},
  volume = {32},
  number = {4},
  pages = {418--426},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2000.11980027},
  urldate = {2023-07-02},
  abstract = {Control charts monitor processes where performance is measured by one or multiple quality characteristics. Some processes, however, are characterized by a profile or a function. Here we focus on monitoring a process in semiconductor manufacturing that is characterized by a linear function. While the linear function is the simplest, it occurs frequently, for example in calibration studies. Two monitoring approaches are proposed: (1) monitor parameters, slope and intercept, with multivariate T2 and (2) monitor average residuals between sample and reference lines with EWMA and R charts. Simulation studies indicate that both methods work well. Both methods are extendable to complex functions.},
  keywords = {Calibration,Exponentially Weighted Moving Average Control Charts,Multivariate Control Charts,Process Control,todo}
}

@article{karatzoglou2004,
  title = {Kernlab - an {{S4}} Package for Kernel Methods in {{R}}},
  author = {Karatzoglou, Alexandros and Smola, Alex and Hornik, Kurt and Zeileis, Achim},
  year = {2004},
  journal = {Journal of Statistical Software},
  volume = {11},
  number = {9},
  pages = {1--20},
  doi = {10.18637/jss.v011.i09}
}

@manual{karatzoglou2023,
  type = {Manual},
  title = {Kernlab: {{Kernel-based}} Machine Learning Lab},
  author = {Karatzoglou, Alexandros and Smola, Alex and Hornik, Kurt},
  year = {2023}
}

@article{khoo2002,
  title = {Computing the {{Percentage Points}} of the {{Run-Length Distributions}} of {{Multivariate CUSUM Control Charts}}},
  author = {Khoo, Michael B. C. and Quah, S. H.},
  year = {2002},
  journal = {Quality Engineering},
  volume = {15},
  number = {2},
  pages = {299--310},
  publisher = {{Taylor \& Francis}},
  issn = {0898-2112},
  doi = {10.1081/QEN-120015862},
  urldate = {2023-09-13},
  abstract = {Multivariate CUSUM control charts are often used instead of the standard Hotelling's control charts in many practical problems when detection of small shifts in the process mean is important. However, design of multivariate CUSUM control charts are usually based on the average run length (ARL). In this work, we will compute the percentage points of the run-length distributions of two multivariate CUSUM control charts. It will be shown that interpretations based on ARL can be misleading since the in-control run-length distribution of a multivariate CUSUM is highly skewed. On the other hand, the percentage points of the run-length distribution provide additional information such as the median run length, early false out-of-control signals, and the skewness of the run-length distribution for a particular scheme. These extra information might provide quality control engineers further knowledge of a particular multivariate CUSUM control chart scheme.},
  keywords = {Average run length,In-control,Median run length,Multivariate CUSUM control chart,Out-of-control,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Khoo_Quah_2002_Computing_the_Percentage_Points_of_the_Run-Length_Distributions_of_Multivariate.pdf}
}

@article{khoo2003,
  title = {Design of {{Runs Rules Schemes}}},
  author = {Khoo, Michael B. C.},
  year = {2003},
  journal = {Quality Engineering},
  volume = {16},
  number = {1},
  pages = {27--43},
  publisher = {{Taylor \& Francis}},
  issn = {0898-2112},
  doi = {10.1081/QEN-120020769},
  urldate = {2023-10-18},
  abstract = {Runs rules are often used to increase the sensitivity of a Shewhart control chart. In this work, plots of various runs rules schemes are given to simplify the determination of control limits based on a desired in-control average run length (ARL0).},
  keywords = {Average run length,False alarm,In-control,Markov chain,Out-of-control,Runs rules,Sensitivity analysis,Shewhart control chart,States,Transient,Transition probabilities,Type-I error},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Khoo_2003_Design_of_Runs_Rules_Schemes.pdf}
},

@Manual{killick2022,
  title = {\pkg{changepoint}: {{An \proglang{R} Package}} for {{Changepoint Analysis}}},
  author = {Rebecca Killick and Kaylea Haynes and Idris A. Eckley},
  year = {2022},
  url = {https://CRAN.R-project.org/package=changepoint},
  note = {R package version 2.2.4},
}

@article{killick2014a,
  title = {\pkg{changepoint}: {{An \proglang{R} Package}} for {{Changepoint Analysis}}},
  shorttitle = {Changepoint},
  author = {Killick, Rebecca and Eckley, Idris A.},
  year = {2014},
  journal = {Journal of Statistical Software},
  volume = {58},
  pages = {1--19},
  issn = {1548-7660},
  doi = {10.18637/jss.v058.i03},
  urldate = {2023-07-02},
  abstract = {One of the key challenges in changepoint analysis is the ability to detect multiple changes within a given time series or sequence. The changepoint package has been developed to provide users with a choice of multiple changepoint search methods to use in conjunction with a given changepoint method and in particular provides an implementation of the recently proposed PELT algorithm. This article describes the search methods which are implemented in the package as well as some of the available test statistics whilst highlighting their application with simulated and practical examples. Particular emphasis is placed on the PELT algorithm and how results differ from the binary segmentation approach.},
  copyright = {Copyright (c) 2013 Rebecca Killick, Idris A. Eckley},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Killick_Eckley_2014_changepoint.pdf}
}

@article{kim2003,
  title = {On the {{Monitoring}} of {{Linear Profiles}}},
  author = {Kim, Keunpyo and Mahmoud, Mahmoud A. and Woodall, William H.},
  year = {2003},
  journal = {Journal of Quality Technology},
  volume = {35},
  number = {3},
  pages = {317--328},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2003.11980225},
  urldate = {2022-08-30},
  abstract = {We propose control chart methods for process monitoring when the quality of a process or product is characterized by a linear function. In the historical analysis of Phase I data, we recommend methods including the use of a bivariate T2 chart to check for stability of the regression coefficients in conjunction with a univariate Shewhart chart to check for stability of the variation about the regression line. We recommend the use of three univariate control charts in Phase II. These three charts are used to monitor the {$\mathsfsl{Y}$}-intercept, the slope, and the variance of the deviations about the regression line, respectively. A simulation study shows that this type of Phase II method can detect sustained shifts in the parameters better than competing methods in terms of average run length performance. We also relate the monitoring of linear profiles to the control charting of regression-adjusted variables and other methods.},
  keywords = {Calibration,Exponentially Weighted Moving Average Control Chart,Multivariate T2 Control Charts,reference only,Statistical Process Control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kim_et_al_2003_On_the_Monitoring_of_Linear_Profiles.pdf}
}

@article{knoth2015b,
  title = {Run {{Length Quantiles}} of {{EWMA Control Charts Monitoring Normal Mean}} or/and {{Variance}}},
  author = {Knoth, Sven},
  year = {2015},
  journal = {International Journal of Production Research},
  volume = {53},
  number = {15},
  pages = {4629--4647},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7543},
  doi = {10.1080/00207543.2015.1005253},
  urldate = {2023-05-19},
  abstract = {Exponentially weighted moving average (EWMA) control charts are well-established devices for monitoring process stability. Typically, control charts are evaluated by considering their Average Run Length (ARL), that is the expected number of observations or samples until the chart signals. Because of the limitations of an average, various papers also dealt with the run length distribution and quantiles. Going beyond these papers, we develop algorithms for and evaluate the quantile performance of EWMA control charts with variance adjusted control limits and with fast initial response features, of EWMA charts based on the sample variance, and of EWMA charts simultaneously monitoring mean and variance. Additionally, for the mean charts we consider medium, late and very late process changes and their impact on appropriately conditioned run length quantiles. It is demonstrated that considering run length quantiles can protect from constructing distorted EWMA designs while optimising their zero-state ARL performance. The implementation of all the considered measures in the \proglang{R} package `spc' allows any control chart user to consider EWMA schemes from the run length quantile prospective in an easy way.},
  keywords = {average run length,doing,false alarm probability,fast initial response,median run length,numerical methods,quality control,time-varying control limits},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Knoth_2015_Run_length_quantiles_of_EWMA_control_charts_monitoring_normal_mean_or-and.pdf}
}

@article{knoth2017,
  title = {{{ARL Numerics}} for {{MEWMA Charts}}},
  author = {Knoth, Sven},
  year = {2017},
  journal = {Journal of Quality Technology},
  volume = {49},
  number = {1},
  pages = {78--89},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2017.11918186},
  urldate = {2023-05-17},
  abstract = {The FORTRAN code in Bodden and Rigdon (1999) for the in-control average run length (ARL) of multivariate exponentially weighted moving average charts (MEWMA) became quite popular and is widely used in statistical software systems such as MINITAB and STATISTICA. We find that the algorithms' accuracy is poor for low-dimensional processes. The Markov chain approximation described in Runger and Prabhu (1996) is not able to resolve the issue. The same holds for the calculation of the out-of-control ARL as proposed in Ridgon (1995b). We present two concepts that achieve higher accuracy for all dimensions. The competing numerical procedures are implemented in the \proglang{R} package spc.},
  keywords = {Collocation,Fredholm Integral Equation of the Second Kind,Markov Chain Approximation,Multivariate Statistical Process Control,Nystr\"om Method,Software implementation},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Knoth_2017_ARL_Numerics_for_MEWMA_Charts.pdf}
}

@manual{knoth2022a,
  type = {Manual},
  title = {\pkg{spc}: {{Statistical}} Process Control \textendash{} Calculation of {{ARL}} and Other Control Chart Performance Measures},
  author = {Knoth, Sven},
  year = {2022},
  keywords = {todo},
  note = {\proglang{R} package version 0.6.7},
  url = {https://CRAN.R-project.org/package=spc},
}

@article{kreiss2011,
  title = {Bootstrap Methods for Dependent Data: {{A}} Review},
  shorttitle = {Bootstrap Methods for Dependent Data},
  author = {Kreiss, Jens-Peter and Paparoditis, Efstathios},
  year = {2011},
  journal = {Journal of the Korean Statistical Society},
  volume = {40},
  number = {4},
  pages = {357--378},
  issn = {1226-3192},
  doi = {10.1016/j.jkss.2011.08.009},
  urldate = {2020-10-18},
  abstract = {This paper gives a review on a variety of bootstrap methods for dependent data. The main focus is not on an exhaustive listing and description of bootstrap procedures but on general principles which should be taken into account when selecting a particular bootstrap procedure in order to approximate the (properly standardized) distribution of a statistic of interest. Questions are considered related to which dependence properties of the underlying data generating process asymptotically influence the distribution of the statistic of interest and which dependence properties (or even which process) a particular bootstrap method really mimics. For answering these questions we introduce the concept of a companion stochastic process. As statistics we consider generalized means, and integrated periodogram statistics (including ratio statistics) as well as nonparametric estimators.},
  langid = {english},
  keywords = {Bootstrap methods,done,Stochastic processes,Time series},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kreiss_Paparoditis_2011_Bootstrap_methods_for_dependent_data.pdf;/home/dede/Zotero/storage/II8RR8DT/S1226319211000780.html}
}

@book{kreyszig1991,
  title = {Differential {{Geometry}}},
  author = {Kreyszig, Erwin},
  year = {1991},
  publisher = {{Dover Publications}},
  address = {{New York}},
  abstract = {This outstanding textbook by a distinguished mathematical scholar introduces the differential geometry of curves and surfaces in three-dimensional Euclidean space. The subject is presented in its simplest, most essential form, but with many explanatory details, figures and examples, and in a manner that conveys the geometric significance and theoretical and practical importance of the different concepts, methods and results involved.The first chapters of the book focus on the basic concepts and facts of analytic geometry, the theory of space curves, and the foundations of the theory of surfaces, including problems closely related to the first and second fundamental forms. The treatment of the theory of surfaces makes full use of the tensor calculus.The later chapters address geodesics, mappings of surfaces, special surfaces, and the absolute differential calculus and the displacement of Levi-Civit\`a. Problems at the end of each section (with solutions at the end of the book) will help students meaningfully review the material presented, and familiarize themselves with the manner of reasoning in differential geometry.},
  isbn = {978-0-486-66721-8},
  langid = {english}
}

@book{kreyszig2013,
  title = {Differential {{Geometry}}},
  author = {Kreyszig, Erwin},
  year = {2013},
  publisher = {{Courier Corporation}},
  abstract = {This outstanding textbook by a distinguished mathematical scholar introduces the differential geometry of curves and surfaces in three-dimensional Euclidean space. The subject is presented in its simplest, most essential form, but with many explanatory details, figures and examples, and in a manner that conveys the geometric significance and theoretical and practical importance of the different concepts, methods and results involved.The first chapters of the book focus on the basic concepts and facts of analytic geometry, the theory of space curves, and the foundations of the theory of surfaces, including problems closely related to the first and second fundamental forms. The treatment of the theory of surfaces makes full use of the tensor calculus.The later chapters address geodesics, mappings of surfaces, special surfaces, and the absolute differential calculus and the displacement of Levi-Civit\`a. Problems at the end of each section (with solutions at the end of the book) will help students meaningfully review the material presented, and familiarize themselves with the manner of reasoning in differential geometry.},
  googlebooks = {Vw3CAgAAQBAJ},
  isbn = {978-0-486-31862-2},
  langid = {english},
  keywords = {Mathematics / Geometry / Differential}
}

@article{kullback1951,
  title = {On {{Information}} and {{Sufficiency}}},
  author = {Kullback, S. and Leibler, R. A.},
  year = {1951},
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  pages = {79--86},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729694},
  urldate = {2023-10-10},
  abstract = {The Annals of Mathematical Statistics},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kullback_Leibler_1951_On_Information_and_Sufficiency.pdf}
}

@book{kushner2003,
  title = {Stochastic {{Approximation}} and {{Recursive Algorithms}} and {{Applications}}},
  author = {Kushner, Harold and Yin, G. George},
  year = {2003},
  edition = {2nd},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  isbn = {978-1-4419-1847-5},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kushner_Yin_2010_Stochastic_Approximation_and_Recursive_Algorithms_and_Applications.pdf}
}

@article{leblanc2010,
  title = {A Bias-Reduced Approach to Density Estimation Using {{Bernstein}} Polynomials},
  author = {Leblanc, Alexandre},
  year = {2010},
  journal = {Journal of Nonparametric Statistics},
  volume = {22},
  number = {4},
  pages = {459--475},
  publisher = {{Taylor \& Francis}},
  issn = {1048-5252},
  doi = {10.1080/10485250903318107},
  urldate = {2023-10-10},
  abstract = {Mixtures of Beta densities have led to different methods of density estimation for univariate data assumed to have compact support. One such method relies on Bernstein polynomials and leads to good approximation properties for the resulting estimator of the underlying density f. In particular, if f is twice continuously differentiable, this estimator can be shown to attain the optimal nonparametric convergence rate of n -4/5 in terms of mean integrated squared error (MISE). However, this rate cannot be improved upon directly when relying on the usual Bernstein polynomials, no matter what other assumptions are made on the smoothness of f. In this note, we show how a simple method of bias reduction can lead to a Bernstein-based estimator that does achieve a higher rate of convergence. Precisely, we exhibit a bias-corrected estimator that achieves the optimal nonparametric MISE rate of n -8/9 when the underlying density f is four times continuously differentiable on its support.},
  keywords = {62G30,asymptotic properties,Bernstein polynomials,bias correction,boundary bias,density estimation,mean integrated squared error,nonparametric statistics,Primary: 62G07,Secondary: 62G20}
}

@article{lee2011,
  title = {Improved Design of Robust Exponentially Weighted Moving Average Control Charts for Autocorrelated Processes},
  author = {Lee, Hyun Cheol and Apley, Daniel W.},
  year = {2011},
  journal = {Quality and Reliability Engineering International},
  volume = {27},
  number = {3},
  pages = {337--352},
  issn = {1099-1638},
  doi = {10.1002/qre.1126},
  urldate = {2023-09-29},
  abstract = {Residual-based control charts for autocorrelated processes are known to be sensitive to time series modeling errors, which can seriously inflate the false alarm rate. This paper presents a design approach for a residual-based exponentially weighted moving average (EWMA) chart that mitigates this problem by modifying the control limits based on the level of model uncertainty. Using a Bayesian analysis, we derive the approximate expected variance of the EWMA statistic, where the expectation is with respect to the posterior distribution of the unknown model parameters. The result is a relatively clean expression for the expected variance as a function of the estimated parameters and their covariance matrix. We use control limits proportional to the square root of the expected variance. We compare our approach to two other approaches for designing robust residual-based EWMA charts and argue that our approach generally results in a more appropriate widening of the control limits. Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {autoregressive moving average models,exponentially weighted moving average,model uncertainty,residual-based control charts,robust design,time series,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lee_Apley_2011_Improved_design_of_robust_exponentially_weighted_moving_average_control_charts.pdf;/home/dede/Zotero/storage/BVFPP4F3/qre.html}
}

@article{li2012,
  title = {Directional {{Control Schemes}} for {{Multivariate Categorical Processes}}},
  author = {Li, Jian and Tsung, Fugee and Zou, Changliang},
  year = {2012},
  journal = {Journal of Quality Technology},
  volume = {44},
  number = {2},
  pages = {136--154},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2012.11917889},
  urldate = {2022-09-12},
  abstract = {We consider statistical process control of multivariate categorical processes and propose a Phase II log-linear directional control chart that exploits directional shift information and integrates the monitoring of multivariate categorical processes into the unified framework of multivariate binomial and multivariate multinomial distributions. We also suggest a diagnostic scheme for identifying the shift direction. Both the control chart and the diagnostic approach are simple and easily computed. Numerical simulations and real applications are presented to demonstrate their effectiveness.},
  keywords = {Contingency Table,done,EWMA,Generalized Likelihood-Ratio Test,Multivariate Multinomial Distribution,Statistical Process Control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_et_al_2012_Directional_Control_Schemes_for_Multivariate_Categorical_Processes2.pdf;/home/dede/Documents/MEGA/universita/zotero-pdf/Li_et_al_2012_Directional_Control_Schemes_for_Multivariate_Categorical_Processes3.pdf}
}

@article{li2014d,
  title = {The {{Computation}} of {{Average Run Length}} and {{Average Time}} to {{Signal}}: {{An Overview}}},
  shorttitle = {The {{Computation}} of {{Average Run Length}} and {{Average Time}} to {{Signal}}},
  author = {Li, Zhonghua and Zou, Changliang and Gong, Zhen and Wang, Zhaojun},
  year = {2014},
  journal = {Journal of Statistical Computation and Simulation},
  volume = {84},
  number = {8},
  pages = {1779--1802},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10.1080/00949655.2013.766737},
  urldate = {2023-09-13},
  abstract = {Control charts are widely used in industries to monitor a process for quality improvement. Evaluation of the average run length (ARL) or average time to signal (ATS) plays an important role in the design of control charts and performance comparison. In this paper, we review several basic and popular procedures, including the Markov chain and integral equation methods for computing ARL, ATS and associated run length distributions for cumulative sum charts, exponentially weighted moving average charts and combined control charts, respectively. Some important references and key formulations are provided for practitioners.},
  keywords = {62P30,65C20,65C60,68U20,average run length,average time to signal,integral equation,Markov chain,statistical process control,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_et_al_2014_The_computation_of_average_run_length_and_average_time_to_signal.pdf}
}

@article{li2015,
  title = {Localized Discrete {{Laplace}}\textendash{{Beltrami}} Operator over Triangular Mesh},
  author = {Li, Xinge and Xu, Guoliang and Zhang, Yongjie Jessica},
  year = {2015},
  journal = {Computer Aided Geometric Design},
  volume = {39},
  pages = {67--82},
  issn = {0167-8396},
  doi = {10.1016/j.cagd.2015.09.001},
  urldate = {2023-10-10},
  abstract = {The Laplace\textendash Beltrami operator is the foundation of describing geometric partial differential equations, and it also plays an important role in the fields of computational geometry, computer graphics and image processing, such as surface parameterization, shape analysis, matching and interpolation. However, constructing the discretized Laplace\textendash Beltrami operator with convergent property has been an open problem. In this paper we propose a new discretization scheme of the Laplace\textendash Beltrami operator over triangulated surfaces. We prove that our discretization of the Laplace\textendash Beltrami operator converges to the Laplace\textendash Beltrami operator at every point of an arbitrary smooth surface as the size of the triangular mesh over the surface tends to zero. Numerical experiments are conducted, which support the theoretical analysis.},
  keywords = {Convergence,Discretization,Laplace\textendash Beltrami operator,Surface triangulation},
  file = {/home/dede/Zotero/storage/43Y5PBNM/S0167839615001004.html}
}

@article{li2017a,
  title = {A Robust Self-Starting Spatial Rank Multivariate {{EWMA}} Chart Based on Forward Variable Selection},
  author = {Li, Wendong and Pu, Xiaolong and Tsung, Fugee and Xiang, Dongdong},
  year = {2017},
  journal = {Computers and Industrial Engineering},
  volume = {103},
  number = {C},
  pages = {116--130},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2016.11.024},
  urldate = {2022-03-25},
  abstract = {A robust self-starting control chart based on forward variable selection is proposed.The proposed chart does not need prior knowledge of the IC distribution and is robust to non-normally distributed data.The need to gather extensive data before monitoring is overcome.The sensitivity to small and moderate sparse shifts in mean vectors is remarkable. Shifts in one or a few components of process mean vectors, called sparse shifts, are monitored in many applications. To monitor sparse shifts, several control charts have recently been proposed based on the variable selection technique. These charts assume either that the in-control (IC) distribution is completely known or that a sufficiently large reference dataset is available. However, this assumption is not always valid in practice. This paper develops a self-starting control chart that integrates a multivariate spatial rank test with the EWMA charting scheme based on forward variable selection for monitoring sparse mean shifts. Both the theoretical and numerical results show that the proposed chart is robust to non-normally distributed data, fast to compute, easy to construct, and can efficiently detect sparse shifts, especially when the process distribution is heavy-tailed or skewed. The proposed control chart does not need prior knowledge of the IC distribution and can start monitoring even before considerable reference data have been collected. A real-data example from a white wine production process illustrates the effectiveness of the proposed control chart.},
  keywords = {Forward variable selection,Robust,Self-starting,Statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_et_al_2017_A_robust_self-starting_spatial_rank_multivariate_EWMA_chart_based_on_forward.pdf}
}

@article{liu2008,
  title = {A {{Local}}/{{Global Approach}} to {{Mesh Parameterization}}},
  author = {Liu, Ligang and Zhang, Lei and Xu, Yin and Gotsman, Craig and Gortler, Steven J.},
  year = {2008},
  journal = {Computer Graphics Forum},
  volume = {27},
  number = {5},
  pages = {1495--1504},
  issn = {1467-8659},
  doi = {10.1111/j.1467-8659.2008.01290.x},
  urldate = {2023-10-10},
  abstract = {We present a novel approach to parameterize a mesh with disk topology to the plane in a shape-preserving manner. Our key contribution is a local/global algorithm, which combines a local mapping of each 3D triangle to the plane, using transformations taken from a restricted set, with a global ``stitch'' operation of all triangles, involving a sparse linear system. The local transformations can be taken from a variety of families, e.g. similarities or rotations, generating different types of parameterizations. In the first case, the parameterization tries to force each 2D triangle to be an as-similar-as-possible version of its 3D counterpart. This is shown to yield results identical to those of the LSCM algorithm. In the second case, the parameterization tries to force each 2D triangle to be an as-rigid-as-possible version of its 3D counterpart. This approach preserves shape as much as possible. It is simple, effective, and fast, due to pre-factoring of the linear system involved in the global phase. Experimental results show that our approach provides almost isometric parameterizations and obtains more shape-preserving results than other state-of-the-art approaches. We present also a more general ``hybrid'' parameterization model which provides a continuous spectrum of possibilities, controlled by a single parameter. The two cases described above lie at the two ends of the spectrum. We generalize our local/global algorithm to compute these parameterizations. The local phase may also be accelerated by parallelizing the independent computations per triangle.},
  copyright = {\textcopyright{} 2008 The Author(s) Journal compilation \textcopyright{} 2008 The Eurographics Association and Blackwell Publishing Ltd.},
  langid = {english},
  keywords = {and,Application,Computational,Computer,Geometry,Graphics:,I.3.5,I.3.8,Modeling,Object},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liu_et_al_2008_A_Local-Global_Approach_to_Mesh_Parameterization.pdf;/home/dede/Zotero/storage/5NTTM9GL/j.1467-8659.2008.01290.html}
}

@article{liu2015,
  title = {An {{Adaptive Sampling Strategy}} for {{Online High-Dimensional Process Monitoring}}},
  author = {Liu, Kaibo and Mei, Yajun and Shi, Jianjun},
  year = {2015},
  journal = {Technometrics},
  volume = {57},
  number = {3},
  pages = {305--319},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2014.947005},
  urldate = {2021-10-20},
  abstract = {Temporally and spatially dense data-rich environments provide unprecedented opportunities and challenges for effective process control. In this article, we propose a systematic and scalable adaptive sampling strategy for online high-dimensional process monitoring in the context of limited resources with only partial information available at each acquisition time. The proposed adaptive sampling strategy includes a broad range of applications: (1) when only a limited number of sensors is available; (2) when only a limited number of sensors can be in ``ON'' state in a fully deployed sensor network; and (3) when only partial data streams can be analyzed at the fusion center due to limited transmission and processing capabilities even though the full data streams have been acquired remotely. A monitoring scheme of using the sum of top-r local CUSUM statistics is developed and named as ``TRAS'' (top-r based adaptive sampling), which is scalable and robust in detecting a wide range of possible mean shifts in all directions, when each data stream follows a univariate normal distribution. Two properties of this proposed method are also investigated. Case studies are performed on a hot-forming process and a real solar flare process to illustrate and evaluate the performance of the proposed method.},
  keywords = {done,Multiple data streams,Partial information over the spatial domain,Sensor redeployment,Shift detection,Sum of the top-r local statistics},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liu_et_al_2015_An_Adaptive_Sampling_Strategy_for_Online_High-Dimensional_Process_Monitoring.pdf;/home/dede/Zotero/storage/YM8TVDT6/00401706.2014.html}
}

@article{liu2018a,
  title = {Remarks on Multi-Output {{Gaussian}} Process Regression},
  author = {Liu, Haitao and Cai, Jianfei and Ong, Yew-Soon},
  year = {2018},
  journal = {Knowledge-Based Systems},
  volume = {144},
  pages = {102--121},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2017.12.034},
  urldate = {2023-10-13},
  abstract = {Multi-output regression problems have extensively arisen in modern engineering community. This article investigates the state-of-the-art multi-output Gaussian processes (MOGPs) that can transfer the knowledge across related outputs in order to improve prediction quality. We classify existing MOGPs into two main categories as (1) symmetric MOGPs that improve the predictions for all the outputs, and (2) asymmetric MOGPs, particularly the multi-fidelity MOGPs, that focus on the improvement of high fidelity output via the useful information transferred from related low fidelity outputs. We review existing symmetric/asymmetric MOGPs and analyze their characteristics, e.g., the covariance functions (separable or non-separable), the modeling process (integrated or decomposed), the information transfer (bidirectional or unidirectional), and the hyperparameter inference (joint or separate). Besides, we assess the performance of ten representative MOGPs thoroughly on eight examples in symmetric/asymmetric scenarios by considering, e.g., different training data (heterotopic or isotopic), different training sizes (small, moderate and large), different output correlations (low or high), and different output sizes (up to four outputs). Based on the qualitative and quantitative analysis, we give some recommendations regarding the usage of MOGPs and highlight potential research directions.},
  keywords = {Knowledge transfer,Multi-fidelity,Multi-output Gaussian process,Output correlation,Symmetric/asymmetric MOGP},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liu_et_al_2018_Remarks_on_multi-output_Gaussian_process_regression.pdf;/home/dede/Zotero/storage/G8NALUL5/S0950705117306123.html}
}

@article{liu2022,
  title = {In-Profile Monitoring for Cluster-Correlated Data in Advanced Manufacturing System},
  author = {Liu, Peiyao and Du, Juan and Zang, Yangyang and Zhang, Chen and Wang, Kaibo},
  year = {2023},
  journal = {Journal of Quality Technology},
  volume = {55},
  number = {2},
  pages = {195--219},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2022.2106912},
  urldate = {2022-11-07},
  abstract = {Nowadays advanced sensing technology enables real-time data collection of key variables during manufacturing, known as multi-channel profiles. These data facilitate in-process monitoring and anomaly detection, which have been extensively studied in recent years. However, most studies treat each profile as a whole, e.g., a high-dimensional vector or function, and construct monitoring schemes accordingly. As a result, these methods cannot be implemented until the entire profile has been obtained, leading to long detection delay especially if anomalies occur in early sensing points of the profile. In addition, they require that profiles of different samples have the same time length and feature location, yet additional time-warping operation for real misaligned samples may weaken the anomaly patterns. To address these problems, this article proposes an in-profile monitoring (INPOM) control chart, which not only gives the feasibility of detecting anomalies inside the profile, but also can handle the misalignment problem of different samples. In particular, our INPOM scheme is built upon state space model (SSM). To better describe the clustered between-profile correlation and avoid overfitting, SSM is extended to a regularized SSM (RSSM), where regularizations are imposed as prior information and expectation maximization algorithm is integrated for posterior maximization to efficiently learn the model parameters. Furthermore, a monitoring statistic based on one-step-ahead prediction error of RSSM is constructed for INPOM control chart. Thorough numerical studies and real case studies demonstrate the effectiveness and applicability of our proposed RSSM-INPOM framework.},
  keywords = {between-profile correlation,cluster-correlated data,doing,in-profile monitoring,multi-channel profiles,regularized state space model},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liu_et_al_2022_In-profile_monitoring_for_cluster-correlated_data_in_advanced_manufacturing.pdf}
}

@article{lorden1973,
  title = {Detection of {{Failure Rate Increases}}},
  author = {Lorden, G. and Eisenberger, I.},
  year = {1973},
  journal = {Technometrics},
  volume = {15},
  number = {1},
  eprint = {1266833},
  eprinttype = {jstor},
  pages = {167--175},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1266833},
  urldate = {2023-10-20},
  abstract = {The problem of devising systematic policies for replacement of equipment subject to wear-out involves the detection of increases in failure rates. Detection procedures are defined as stopping times N with respect to the observed sequence of random failures. The concepts of "quickness of detection" and "frequency of false reactions" are made precise and a class of procedures is studied which optimizes the former asymptotically as the latter is reduced to zero. Results of Monte Carlo experiments are given which show that efficient quickness of detection is attainable simultaneously for various levels of increase in failure rates.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lorden_Eisenberger_1973_Detection_of_Failure_Rate_Increases.pdf}
}

@article{lorenzen1986,
  title = {The {{Economic Design}} of {{Control Charts}}: {{A Unified Approach}}},
  shorttitle = {The {{Economic Design}} of {{Control Charts}}},
  author = {Lorenzen, Thomas J. and Vance, Lonnie C.},
  year = {1986},
  journal = {Technometrics},
  volume = {28},
  number = {1},
  eprint = {1269598},
  eprinttype = {jstor},
  pages = {3--10},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1269598},
  urldate = {2023-10-18},
  abstract = {The choice of control chart parameters-sample size, sampling interval, and control limits-is considered from an economic point of view. A general process model is considered, and the hourly cost function is derived. This cost function simplifies when the recorded statistics are independent. Numerical techniques to minimize this cost function are discussed, and sensitivity analyses are performed. An example illustrates the potential savings of this technique of designing control charts.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lorenzen_Vance_1986_The_Economic_Design_of_Control_Charts.pdf}
}

@article{lowry1992,
  title = {A {{Multivariate Exponentially Weighted Moving Average Control Chart}}},
  author = {Lowry, Cynthia A. and Woodall, William H. and Champ, Charles W. and Rigdon, Steven E.},
  year = {1992},
  journal = {Technometrics},
  volume = {34},
  number = {1},
  pages = {46--53},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1992.10485232},
  urldate = {2020-11-26},
  abstract = {A multivariate extension of the exponentially weighted moving average (EWMA) control chart is presented, and guidelines given for designing this easy-to-implement multivariate procedure. A comparison shows that the average run length (ARL) performance of this chart is similar to that of multivariate cumulative sum (CUSUM) control charts in detecting a shift in the mean vector of a multivariate normal distribution. As with the Hotelling's {$\chi$}2 and multivariate CUSUM charts, the ARL performance of the multivariate EWMA chart depends on the underlying mean vector and covariance matrix only through the value of the noncentrality parameter. Worst-case scenarios show that Hotelling's {$\chi$}2 charts should always be used in conjunction with multivariate CUSUM and EWMA charts to avoid potential inertia problems. Examples are given to illustrate the use of the proposed procedure.},
  keywords = {Average run length,done,Hotelling's T 2 chart,Multivariate CUSUM,Statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lowry_et_al_1992_A_Multivariate_Exponentially_Weighted_Moving_Average_Control_Chart.pdf;/home/dede/Zotero/storage/KR2GWAU5/00401706.1992.html}
}

@article{lu1999,
  title = {{{EWMA Control Charts}} for {{Monitoring}} the {{Mean}} of {{Autocorrelated Processes}}},
  author = {Lu, Chao-Wen and Reynolds, Marion R.},
  year = {1999},
  journal = {Journal of Quality Technology},
  volume = {31},
  number = {2},
  pages = {166--188},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.1999.11979913},
  urldate = {2023-10-21},
  abstract = {A standard assumption when using a control chart to monitor a process is that the observations from the process output are independent. However, for many processes the observations are autocorrelated, and this autocorrelation can have a significant effect on the performance of the control chart. This paper considers the problem of monitoring the mean of a process in which the observations can be modeled as an AR(1) process plus a random error. An exponentially weighted moving average (EWMA) control chart based on the residuals from the forecast values of the model is evaluated using an integral equation method. This control chart's performance is compared to the performance of an EWMA control chart based on the original observations, and the effect of process parameter estimation on the control charts is investigated. When the level of autocorrelation is low or moderate, the two EWMA charts require about the same amount of time to detect various shifts; but for high levels of autocorrelation and large shifts, the EWMA chart of the residuals is a little faster.},
  keywords = {Autocorrelated Observations,Autoregressive Moving Average Model,Average Run Length,Exponentially Weighted Moving Average Control Charts,First Order Autoregressive Models,Integral Equations,Residuals,Shewhart Control Charts,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lu_Reynolds_1999_EWMA_Control_Charts_for_Monitoring_the_Mean_of_Autocorrelated_Processes.pdf}
}

@article{lucas1982,
  title = {Combined {{Shewhart-CUSUM Quality Control Schemes}}},
  author = {Lucas, James M.},
  year = {1982},
  journal = {Journal of Quality Technology},
  volume = {14},
  number = {2},
  pages = {51--59},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.1982.11978790},
  urldate = {2023-10-20},
  abstract = {The Shewhart-CUSUM quality control scheme which combines the key features of the Shewhart and CUSUM control procedures is described and evaluated. In this scheme the CUSUM feature will quickly detect small shifts from the goal while the addition of Shewhart limits increases the speed of detecting large shifts. The combined Shewhart-CUSUM scheme has performance comparable to that of the more involved parabolic CUSUM. The Shewhart-CUSUM scheme works with only the current observations so it requires less computer storage space than parbolic CUSUM, which requires the storage of several previous observations. This becomes particularly important when many control schemes are simultaneously implemented.},
  keywords = {Average Run Length,CUSUM Scheme,Fast Initial Response,Parabolic CUSUM,Shewhart Control Limits,V Mask}
}

@article{lucas1990,
  title = {Exponentially {{Weighted Moving Average Control Schemes}}: {{Properties}} and {{Enhancements}}},
  shorttitle = {Exponentially {{Weighted Moving Average Control Schemes}}},
  author = {Lucas, James M. and Saccucci, Michael S.},
  year = {1990},
  journal = {Technometrics},
  volume = {32},
  number = {1},
  eprint = {1269835},
  eprinttype = {jstor},
  pages = {1--12},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1269835},
  urldate = {2023-02-27},
  abstract = {Roberts (1959) first introduced the exponentially weighted moving average (EWMA) control scheme. Using simulation to evaluate its properties, he showed that the EWMA is useful for detecting small shifts in the mean of a process. The recognition that an EWMA control scheme can be represented as a Markov chain allows its properties to be evaluated more easily and completely than has previously been done. In this article, we evaluate the properties of an EWMA control scheme used to monitor the mean of a normally distributed process that may experience shifts away from the target value. A design procedure for EWMA control schemes is given. Parameter values not commonly used in the literature are shown to be useful for detecting small shifts in a process. In addition, several enhancements to EWMA control schemes are considered. These include a fast initial response feature that makes the EWMA control scheme more sensitive to start-up problems, a combined Shewhart EWMA that provides protection against both large and small shifts in a process, and a robust EWMA that provides protection against occasional outliers in the data that might otherwise cause an out-of-control signal. An extensive comparison reveals that EWMA control schemes have average run length properties similar to those for cumulative sum control schemes.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lucas_Saccucci_1990_Exponentially_Weighted_Moving_Average_Control_Schemes.pdf}
}

@article{luceno2000,
  title = {Evaluation of the {{Run-Length Probability Distribution}} for {{CUSUM Charts}}: {{Assessing Chart Performance}}},
  shorttitle = {Evaluation of the {{Run-Length Probability Distribution}} for {{CUSUM Charts}}},
  author = {Luceno, Alberto and {Puig-pey}, Jaime},
  year = {2000},
  journal = {Technometrics},
  volume = {42},
  number = {4},
  eprint = {1270951},
  eprinttype = {jstor},
  pages = {411--416},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1270951},
  urldate = {2023-09-13},
  abstract = {This article provides a fast and accurate algorithm to compute the run-length probability distribution for cumulative sum charts to control process mean. This algorithm uses a fast and numerically stable recursive formula based on accurate Gaussian quadrature rules throughout the whole range of the computed run-length distribution and, therefore, improves the numerical efficiency and accuracy of existing methods. The algorithm may detect whether or not the geometric approximation is adequate and, when it is possible, it allows switching to the geometric recursion. The procedure may be applied not only to the normal distribution but also to nonsymmetric and long-tailed continuous distributions, some examples of which are provided. Methods to assess chart performance according to the run-length distribution, as well as some multivariate issues in statistical process control, are considered.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Luceno_Puig-pey_2000_Evaluation_of_the_Run-Length_Probability_Distribution_for_CUSUM_Charts.pdf}
}

@article{luo2016,
  title = {A Kernel Machine-Based Secure Data Sensing and Fusion Scheme in Wireless Sensor Networks for the Cyber-Physical Systems},
  author = {Luo, Xiong and Zhang, Dandan and Yang, Laurence T. and Liu, Ji and Chang, Xiaohui and Ning, Huansheng},
  year = {2016},
  journal = {Future Generation Computer Systems},
  volume = {61},
  pages = {85--96},
  issn = {0167-739X},
  doi = {10.1016/j.future.2015.10.022},
  urldate = {2023-10-13},
  abstract = {Wireless sensor networks (WSNs) as one of the key technologies for delivering sensor-related data drive the progress of cyber-physical systems (CPSs) in bridging the gap between the cyber world and the physical world. It is thus desirable to explore how to utilize intelligence properly by developing the effective scheme in WSN to support data sensing and fusion of CPS. This paper intends to serve this purpose by proposing a prediction-based data sensing and fusion scheme to reduce the data transmission and maintain the required coverage level of sensors in WSN while guaranteeing the data confidentiality. The proposed scheme is called GM\textendash KRLS, which is featured through the use of grey model (GM), kernel recursive least squares (KRLS), and Blowfish algorithm (BA). During the data sensing and fusion process, GM is responsible for initially predicting the data of next period with a small number of data items, while KRLS is used to make the initial predicted value approximate its true value with high accuracy. The KRLS as an improved kernel machine learning algorithm can adaptively adjust the coefficients with every input, while making the predicted value more close to actual value. And BA is used for data encoding and decoding during the transmission process due to its successful applications across a wide range of domains. Then, the proposed secure data sensing and fusion scheme GM\textendash KRLS can provide high prediction accuracy, low communication, good scalability, and confidentiality. In order to verify the effectiveness and reasonableness of our proposed approach, we conduct simulations on actual data sets that are collected from sensors in the Intel Berkeley research lab. The simulation results have shown that the proposed scheme can significantly reduce redundant transmissions with high prediction accuracy.},
  keywords = {Cyber-physical systems,Kernel recursive least squares,Secure data sensing and fusion,Wireless sensor networks},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Luo_et_al_2016_A_kernel_machine-based_secure_data_sensing_and_fusion_scheme_in_wireless_sensor.pdf;/home/dede/Zotero/storage/TD3FNXIR/S0167739X15003350.html}
}

@article{maaten2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {Nov},
  pages = {2579--2605},
  issn = {ISSN 1533-7928},
  urldate = {2020-02-05},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Maaten_Hinton_2008_Visualizing_Data_using_t-SNE.pdf;/home/dede/Zotero/storage/ZTLT2UFS/vandermaaten08a.html}
}

@article{mahmoud2010,
  title = {A {{Multivariate Adaptive Exponentially Weighted Moving Average Control Chart}}},
  author = {Mahmoud, Mahmoud A. and Zahran, Alyaa R.},
  year = {2010},
  journal = {Communications in Statistics - Theory and Methods},
  volume = {39},
  number = {4},
  pages = {606--625},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0926},
  doi = {10.1080/03610920902755813},
  urldate = {2022-11-08},
  abstract = {A multivariate extension of the adaptive exponentially weighted moving average (AEWMA) control chart is proposed. The new multivariate scheme can detect small and large shifts in the process mean vector effectively. The proposed scheme can be viewed as a smooth combination of a multivariate exponentially weighted moving average (MEWMA) chart and a Shewhart {$\chi$}2-chart. The optimal design of the proposed chart is given according to a pre-specified in-control average run length and two shift sizes; a small and large shift each measured in terms of the non centrality parameter. The signal resistance of the newly proposed multivariate chart is also given. Comparisons among the new chart, the MEWMA chart, and the combined Shewhart-MEWMA (S-MEWMA) chart in terms of the standard and worst-case average run length profiles are presented. In addition, the three charts are compared with respect to their worst-case signal resistance values. The proposed chart gives somewhat better worst-case ARL and signal resistance values than the competing charts. It also gives better standard ARL performance especially for moderate and large shifts. The effectiveness of our proposed chart is illustrated through an example with simulated data set.},
  keywords = {Adaptive weighting,Average run length,Exponentially weighted moving average,Inertia,Multivariate control chart,Primary 62P30,Secondary 62H99,Signal resistance,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mahmoud_Zahran_2010_A_Multivariate_Adaptive_Exponentially_Weighted_Moving_Average_Control_Chart.pdf}
}

@article{maleki2018,
  title = {An Overview on Recent Profile Monitoring Papers (2008\textendash 2018) Based on Conceptual Classification Scheme},
  author = {Maleki, Mohammad Reza and Amiri, Amirhossein and Castagliola, Philippe},
  year = {2018},
  journal = {Computers \& Industrial Engineering},
  volume = {126},
  pages = {705--728},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2018.10.008},
  urldate = {2023-10-10},
  abstract = {Sometimes the quality of a process is best expressed by a relationship between response variables and explanatory variables. Checking over the time the stability of such functional relationships using statistical methods is called ``profile monitoring''. Since 2007, when a detailed review paper in the field of profile monitoring was presented, an increasing number of papers have been published in this area. In this paper, we present a conceptual classification scheme and classify the papers in this area since 2008 up to 2018. The relevant papers are categorized and analyzed under different metrics and directions for future studies are recommended.},
  keywords = {Change point estimation,Control chart,Economic design,Parameter estimation,Phase I and Phase II,Statistical design,Survey},
  file = {/home/dede/Zotero/storage/H4UTGU86/S0360835218304789.html}
}

@book{mardia1979,
  title = {Multivariate {{Analysis}}},
  author = {Mardia, K. V. and Kent, John T. and Kent, J. T. and Bibby, John M.},
  year = {1979},
  publisher = {{Academic Press}},
  abstract = {Multivariate Analysis deals with observations on more than one variable where there is some inherent interdependence between the variables. With several texts already available in this area, one may very well enquire of the authors as to the need for yet another book. Most of the available books fall into two categories, either theoretical or data analytic. The present book not only combines the two approaches but it also has been guided by the need to give suitable matter for the beginner as well as illustrating some deeper aspects of the subject for the research worker. Practical examples are kept to the forefront and, wherever feasible, each technique is motivated by such an example.},
  googlebooks = {bxjvAAAAMAAJ},
  isbn = {978-0-12-471250-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mardia_et_al_1979_Multivariate_Analysis.pdf}
}

@incollection{marguet2003,
  title = {Method for {{Geometric Variation Management}} from {{Key Characteristics}} to {{Specification}}},
  booktitle = {Geometric {{Product Specification}} and {{Verification}}: {{Integration}} of {{Functionality}}: {{Selected Conference Papers}} of the 7th {{CIRP International Seminar}} on {{Computer-Aided Tolerancing}}, Held at the {{\'Ecole Normale Sup\'erieure}} de {{Cachan}}, {{France}}, 24\textendash 25 {{April}} 2001},
  author = {Marguet, Benoit and Mathieu, Luc},
  editor = {Bourdet, Pierre and Mathieu, Luc},
  year = {2003},
  pages = {217--226},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-017-1691-8_22},
  urldate = {2023-10-13},
  abstract = {For complex product like aircraft or car body, producibility improvement has to go through a better product assemblability. In order to reach this goal, effects of manufacturing variations have to be reduced. We propose to describe in this paper a pragmatic method analysing the impact of geometrical variations on product Key Characteristics (KC). The main idea of this method is that the flow of geometrical variations belongs to the assembly process. To identify the most promising assembly sequence allows then to minimise impact of geometrical variations. Based on this principle, the first step of the method is to identify where are the Key Characteristics for the product. This identification uses a top-down process going from functional product requirements to geometrical characteristics. Relative to this KC identification, our method performs a qualitative product analysis in order to eliminate the worst assembly sequences. This analysis is based on oriented graphs. At last a quantitative analysis allows to select the most promising assembly sequence. This integrated method has been performed with success on aircraft assemblies and has improved aircraft producibility.},
  isbn = {978-94-017-1691-8},
  langid = {english},
  keywords = {Key Characteristics,Management of tolerances,Product Function,Specifications.}
}

@book{mason2002,
  title = {Multivariate {{Statistical Process Control}} with {{Industrial Applications}}},
  author = {Mason, Robert L. and Young, John C.},
  year = {2002},
  publisher = {{SIAM}},
  abstract = {This applied, self-contained text provides detailed coverage of the practical aspects of multivariate statistical process control (MVSPC)based on the application of Hotelling\&\#39;s T2 statistic. MVSPC is the application of multivariate statistical techniques to improve the quality and productivity of an industrial process. The authors, leading researchers in this area who have developed major software for this type of charting procedure, provide valuable insight into the T2 statistic. Intentionally including only a minimal amount of theory, they lead readers through the construction and monitoring phases of the T2 control statistic using numerous industrial examples taken primarily from the chemical and power industries. These examples are applied to the construction of historical data sets to serve as a point of reference for the control procedure and are also applied to the monitoring phase, where emphasis is placed on signal location and interpretation in terms of the process variables.},
  googlebooks = {0ST3luLyUMsC},
  isbn = {978-0-89871-846-1},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Technology \& Engineering / Industrial Technology}
}

@book{matern1986,
  title = {Spatial {{Variation}}},
  author = {Mat{\'e}rn, Bertil},
  editor = {Brillinger, D. and Fienberg, S. and Gani, J. and Hartigan, J. and Krickeberg, K.},
  year = {1986},
  series = {Lecture {{Notes}} in {{Statistics}}},
  volume = {36},
  publisher = {{Springer-Verlag}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4615-7892-5},
  urldate = {2023-10-10},
  isbn = {978-0-387-96365-5 978-1-4615-7892-5},
  keywords = {boundary element method,development,Mathematica,mathematical statistics,PostScript,statistics,time,university,Volume},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Matern_1986_Spatial_Variation.pdf}
}

@article{mccracken2013,
  title = {Control {{Charts}} for {{Joint Monitoring}} of {{Mean}} and {{Variance}}: {{An Overview}}},
  shorttitle = {Control {{Charts}} for {{Joint Monitoring}} of {{Mean}} and {{Variance}}},
  author = {McCracken, A.K. and Chakraborti, S.},
  year = {2013},
  journal = {Quality Technology \& Quantitative Management},
  volume = {10},
  number = {1},
  pages = {17--36},
  publisher = {{Taylor \& Francis}},
  issn = {null},
  doi = {10.1080/16843703.2013.11673306},
  urldate = {2023-10-31},
  abstract = {In the control chart literature, a number of one-and two-chart schemes has been developed to simultaneously monitor the mean and variance parameters of normally distributed processes. These ``joint'' monitoring schemes are useful for situations in which special causes can result in a change in both the mean and the variance, and they allow practitioners to avoid the inflated false alarm rate which results from simply using two independent control charts (one each for mean and variance) without adjusting for multiple testing. We present an overview of this literature covering some of the one-and two-chart schemes, including those that are appropriate in parameters known (standards known) and unknown (standards unknown) situations. We also discuss some of the joint monitoring schemes for multivariate processes, autocorrelated data, and individual observations. In addition, noting that normality is often an elusive assumption, we discuss some available nonparametric schemes for jointly monitoring location and scale. We end with a conclusion and some recommendations for areas of further research.},
  keywords = {done,Nonparametric control charts,normal distribution,one-and two-chart schemes,parameter estimation,parametric control charts.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/McCracken_Chakraborti_2013_Control_Charts_for_Joint_Monitoring_of_Mean_and_Variance.pdf}
}

@techreport{megahed2010,
  type = {{{SAE Technical Paper}}},
  title = {The {{Use}} of {{3D Laser Scanners}} in {{Statistical Process Control}}},
  author = {Megahed, Fadel and Wells, Lee and Camelio, Jaime},
  year = {2010},
  number = {2010-01-1864},
  address = {{Warrendale, PA}},
  institution = {{SAE International}},
  issn = {0148-7191, 2688-3627},
  doi = {10.4271/2010-01-1864},
  urldate = {2023-10-13},
  abstract = {The emphasis placed on statistical process control over the past few decades has significantly aided manufacturers in measuring and monitoring dimensional parameters of production parts, and inferring process behavior from control charting. However, current manufacturing quality control methods fail},
  langid = {english}
}

@article{menafoglio2018,
  title = {Profile {{Monitoring}} of {{Probability Density Functions}} via {{Simplicial Functional PCA With Application}} to {{Image Data}}},
  author = {Menafoglio, Alessandra and Grasso, Marco and Secchi, Piercesare and Colosimo, Bianca Maria},
  year = {2018},
  journal = {Technometrics},
  volume = {60},
  number = {4},
  pages = {497--510},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2018.1437473},
  urldate = {2022-11-07},
  abstract = {The advance of sensor and information technologies is leading to data-rich industrial environments, where large amounts of data are potentially available. This study focuses on industrial applications where image data are used more and more for quality inspection and statistical process monitoring. In many cases of interest, acquired images consist of several and similar features that are randomly distributed within a given region. Examples are pores in parts obtained via casting or additive manufacturing, voids in metal foams and light-weight components, grains in metallographic analysis, etc. The proposed approach summarizes the random occurrences of the observed features via their (empirical) probability density functions (PDFs). In particular, a novel approach for PDF monitoring is proposed. It is based on simplicial functional principal component analysis (SFPCA), which is performed within the space of density functions, that is, the Bayes space B2. A simulation study shows the enhanced monitoring performances provided by SFPCA-based profile monitoring against other competitors proposed in the literature. Finally, a real case study dealing with the quality control of foamed material production is discussed, to highlight a practical use of the proposed methodology. Supplementary materials for the article are available online.},
  keywords = {Bayes space,Constrained curves,Functional data analysis,Image-based process monitoring,Statistical process control,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Menafoglio_et_al_2018_Profile_Monitoring_of_Probability_Density_Functions_via_Simplicial_Functional2.pdf}
}

@article{menafoglio2018a,
  title = {Profile {{Monitoring}} of {{Probability Density Functions}} via {{Simplicial Functional PCA With Application}} to {{Image Data}}},
  author = {Menafoglio, Alessandra and Grasso, Marco and Secchi, Piercesare and Colosimo, Bianca Maria},
  year = {2018},
  journal = {Technometrics},
  volume = {60},
  number = {4},
  pages = {497--510},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2018.1437473},
  urldate = {2023-10-10},
  abstract = {The advance of sensor and information technologies is leading to data-rich industrial environments, where large amounts of data are potentially available. This study focuses on industrial applications where image data are used more and more for quality inspection and statistical process monitoring. In many cases of interest, acquired images consist of several and similar features that are randomly distributed within a given region. Examples are pores in parts obtained via casting or additive manufacturing, voids in metal foams and light-weight components, grains in metallographic analysis, etc. The proposed approach summarizes the random occurrences of the observed features via their (empirical) probability density functions (PDFs). In particular, a novel approach for PDF monitoring is proposed. It is based on simplicial functional principal component analysis (SFPCA), which is performed within the space of density functions, that is, the Bayes space B2. A simulation study shows the enhanced monitoring performances provided by SFPCA-based profile monitoring against other competitors proposed in the literature. Finally, a real case study dealing with the quality control of foamed material production is discussed, to highlight a practical use of the proposed methodology. Supplementary materials for the article are available online.},
  keywords = {Bayes space,Constrained curves,Functional data analysis,Image-based process monitoring,Statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Menafoglio_et_al_2018_Profile_Monitoring_of_Probability_Density_Functions_via_Simplicial_Functional3.pdf}
}

@article{mohamed2016,
  title = {Mathematical Modeling and {{FDM}} Process Parameters Optimization Using Response Surface Methodology Based on {{Q-optimal}} Design},
  author = {Mohamed, Omar Ahmed and Masood, Syed Hasan and Bhowmik, Jahar Lal},
  year = {2016},
  journal = {Applied Mathematical Modelling},
  volume = {40},
  number = {23},
  pages = {10052--10073},
  issn = {0307-904X},
  doi = {10.1016/j.apm.2016.06.055},
  urldate = {2023-10-13},
  abstract = {Fused deposition modeling (FDM) is a growing 3D printing technique widely practiced around the world in various industrial applications because of its ability to create complex 3D objects and geometries. Reduction of build time and feedstock material consumption without compromising the mechanical performance is the major concern in most industrial applications affecting the cost and the functionality of the manufactured part. One of the key issues of FDM process is how to select the correct parameters to reduce the build time and to reduce feedstock material consumption while maintaining high dynamic mechanical properties. In this study, influence of critical FDM parameters\textemdash layer thickness, air gap, raster angle, build orientation, road width, and number of contours\textemdash are studied using Q-optimal response surface methodology. Their effects on build time, feedstock material consumption and dynamic flexural modulus are critically examined. Mathematical models have been formulated to develop a functional relationship between the processing conditions and the process quality characteristics. Analysis of variance (ANOVA) technique was employed to check the adequacy and significance of mathematical models. Moreover, the optimal setting of process parameters was determined. A confirmation test was also conducted in order to verify the developed models and the optimal settings. The results show that Q-optimal design is a very promising method in FDM process parameter optimization. The results also confirm the adequacy of the developed models.},
  keywords = {Build time,Dynamic flexural modulus,Feedstock material consumption,Fused deposition modeling (FDM),Optimization,Process parameters,Q-optimal design},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mohamed_et_al_2016_Mathematical_modeling_and_FDM_process_parameters_optimization_using_response.pdf;/home/dede/Zotero/storage/PU8JVUZW/S0307904X16303675.html}
}

@book{moller1994,
  title = {Lectures on {{Random Voronoi Tessellations}}},
  author = {M{\o}ller, Jesper},
  editor = {Fienberg, S. and Gani, J. and Krickeberg, K. and Olkin, I. and Wermuth, N.},
  year = {1994},
  series = {Lecture {{Notes}} in {{Statistics}}},
  volume = {87},
  publisher = {{Springer-Verlag}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-2652-9},
  urldate = {2023-10-10},
  isbn = {978-0-387-94264-3 978-1-4612-2652-9},
  keywords = {computation,computational geometry,distribution,Division,geometry,integral,knowledge,Mathematica,Natural,point process,Poisson process,Simula,statistics,story,Tessellation},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Moller_1994_Lectures_on_Random_Voronoi_Tessellations.pdf}
}

@article{montgomery1991,
  title = {Some {{Statistical Process Control Methods}} for {{Autocorrelated Data}}},
  author = {Montgomery, Douglas C. and Mastrangelo, Christina M.},
  year = {1991},
  journal = {Journal of Quality Technology},
  volume = {23},
  number = {3},
  pages = {179--193},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.1991.11979321},
  urldate = {2023-09-13},
  abstract = {Traditionally, control charts are developed assuming that the sequence of process observations to which they are applied are uncorrelated. Unfortunately, this assumption is frequently violated in practice. The presence of autocorrelation has a serious impact on the performance of control charts, causing a dramatic increase in the frequency of false alarms. This paper presents methods for applying statistical control charts to autocorrelated data. The primary method is based on modeling the autocorrelative structure in the original data and applying control charts to the residuals. We show that the exponentially weighted moving average (EWMA) statistic provides the basis of an approximate procedure that can be useful for autocorrelated data. Illustrations are provided using real process data.},
  keywords = {Autocorrelation,Control Charts,EWMA,Time Series Models,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Montgomery_Mastrangelo_1991_Some_Statistical_Process_Control_Methods_for_Autocorrelated_Data.pdf}
}

@book{montgomery2020,
  title = {Introduction to {{Statistical Quality Control}}},
  author = {Montgomery, Douglas C.},
  year = {2020},
  edition = {8th},
  publisher = {{John Wiley \& Sons}},
  abstract = {Once solely the domain of engineers, quality control has become a vital business operation used to increase productivity and secure competitive advantage. Introduction to Statistical Quality Control offers a detailed presentation of the modern statistical methods for quality control and improvement. Thorough coverage of statistical process control (SPC) demonstrates the efficacy of statistically-oriented experiments in the context of process characterization, optimization, and acceptance sampling, while examination of the implementation process provides context to real-world applications. Emphasis on Six Sigma DMAIC (Define, Measure, Analyze, Improve and Control) provides a strategic problem-solving framework that can be applied across a variety of disciplines.Adopting a balanced approach to traditional and modern methods, this text includes coverage of SQC techniques in both industrial and non-manufacturing settings, providing fundamental knowledge to students of engineering, statistics, business, and management sciences. A strong pedagogical toolset, including multiple practice problems, real-world data sets and examples, and incorporation of Minitab statistics software, provides students with a solid base of conceptual and practical knowledge.},
  isbn = {978-1-119-72309-7},
  langid = {english}
}

@book{noorossana2011,
  title = {Statistical {{Analysis}} of {{Profile Monitoring}}},
  author = {Noorossana, Rassoul and Saghaei, Abbas and Amiri, Amirhossein},
  year = {2011},
  edition = {1st edition},
  publisher = {{John Wiley \& Sons}},
  address = {{Hoboken, N.J}},
  abstract = {A one-of-a-kind presentation of the major achievements in statistical profile monitoring methods  Statistical profile monitoring is an area of statistical quality control that is growing in significance for researchers and practitioners, specifically because of its range of applicability across various service and manufacturing settings. Comprised of contributions from renowned academicians and practitioners in the field, Statistical Analysis of Profile Monitoring presents the latest state-of-the-art research on the use of control charts to monitor process and product quality profiles. The book presents comprehensive coverage of profile monitoring definitions, techniques, models, and application examples, particularly in various areas of engineering and statistics.The book begins with an introduction to the concept of profile monitoring and its applications in practice. Subsequent chapters explore the fundamental concepts, methods, and issues related to statistical profile monitoring, with topics of coverage including:Simple and multiple linear profilesBinary response profilesParametric and nonparametric nonlinear profilesMultivariate linear profiles monitoringStatistical process control for geometric specificationsCorrelation and autocorrelation in profilesNonparametric profile monitoringThroughout the book, more than two dozen real-world case studies highlight the discussed topics along with innovative examples and applications of profile monitoring. Statistical Analysis of Profile Monitoring is an excellent book for courses on statistical quality control at the graduate level. It also serves as a valuable reference for quality engineers, researchers and anyone who works in monitoring and improving statistical processes.},
  isbn = {978-0-470-90322-3},
  langid = {english}
}

@book{oja2010,
  title = {Multivariate {{Nonparametric Methods}} with {{R}}: {{An}} Approach Based on Spatial Signs and Ranks},
  shorttitle = {Multivariate {{Nonparametric Methods}} with {{R}}},
  author = {Oja, Hannu},
  year = {2010},
  series = {Lecture {{Notes}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-1-4419-0468-3},
  urldate = {2020-11-12},
  abstract = {This book offers a new, fairly efficient, and robust alternative to analyzing multivariate data. The analysis of data based on multivariate spatial signs and ranks proceeds very much as does a traditional multivariate analysis relying on the assumption of multivariate normality; the regular L2 norm is just replaced by different L1 norms, observation vectors are replaced by spatial signs and ranks, and so on. A unified methodology starting with the simple one-sample multivariate location problem and proceeding to the general multivariate multiple linear regression case is presented. Companion estimates and tests for scatter matrices are considered as well. The \proglang{R} package MNM is available for computation of the procedures. This monograph provides an up-to-date overview of the theory of multivariate nonparametric methods based on spatial signs and ranks. The classical book by Puri and Sen (1971) uses marginal signs and ranks and different type of L1 norm. The book may serve as a textbook and a general reference for the latest developments in the area. Readers are assumed to have a good knowledge of basic statistical theory as well as matrix theory. Hannu Oja is an academy professor and a professor in biometry in the University of Tampere. He has authored and coauthored numerous research articles in multivariate nonparametrical and robust methods as well as in biostatistics.},
  isbn = {978-1-4419-0467-6},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Oja_2010_Multivariate_Nonparametric_Methods_with_R.pdf;/home/dede/Zotero/storage/AEGXDWJT/9781441904676.html}
}

@article{page1954,
  title = {Continuous {{Inspection Schemes}}},
  author = {Page, E. S.},
  year = {1954},
  journal = {Biometrika},
  volume = {41},
  number = {1/2},
  eprint = {2333009},
  eprinttype = {jstor},
  pages = {100},
  issn = {00063444},
  doi = {10.2307/2333009},
  urldate = {2020-10-30},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Page_1954_Continuous_Inspection_Schemes.pdf}
}

@book{patane2017,
  title = {An {{Introduction}} to {{Laplacian Spectral Distances}} and {{Kernels}}: {{Theory}}, {{Computation}}, and {{Applications}}},
  shorttitle = {An {{Introduction}} to {{Laplacian Spectral Distances}} and {{Kernels}}},
  author = {Patan{\`e}, Giuseppe},
  year = {2017},
  series = {Synthesis {{Lectures}} on {{Visual Computing}}: {{Computer Graphics}}, {{Animation}}, {{Computational Photography}} and {{Imaging}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-02593-8},
  urldate = {2023-10-10},
  isbn = {978-3-031-01465-9 978-3-031-02593-8},
  langid = {english}
}

@book{pesarin2001,
  title = {Multivariate {{Permutation Tests}} : {{With Applications}} in {{Biostatistics}}},
  shorttitle = {Multivariate {{Permutation Tests}}},
  author = {Pesarin, Fortunato},
  year = {2001},
  edition = {1st edition},
  publisher = {{John Wiley \& Sons}},
  address = {{Chichester ; New York}},
  abstract = {Complex multivariate problems are frequently encountered in many scientific disciplines and it can be very difficult to obtain meaningful results. Permutation and nonparametric combination methods provide flexible solutions to complex problems by reducing the problem down to a set of simpler sub-problems. The author presents a novel but well tested approach using real examples taken from biomedical research. Statistical analyses are performed in a nonparametric setting, so that no assumptions need be made about the underlying distribution and the dependence relations between variables.  * Provides a clear exposition of the use of multivariate permutation testing, with emphasis on the use of nonparametric combination methodology. * Growing area of research with many practical applications, notably in biostatistics. * Numerous case studies and examples help to illustrate the theory. * Provides solutions to multi-aspect problems, to problems with missing data, analysis of factorial designs and repeated measures. * Explains the analysis of categorical, ordered categorical, binary, continuous, and mixed variables in both an experimental and an observational context. * NPC-Test(c) software (demo copy), SAS macros, S-Plus code and datasets are available on the Web at http://www.stat.unipd.it/\textasciitilde pesarin/ For researchers and practitioners in a number of scientific disciplines, particularly biostatistics, the vast collection of techniques, examples and case studies will be an invaluable resource. Graduate students of applied statistics and nonparametric methods will find the book provides an accessible introduction to multivariate permutation testing.},
  isbn = {978-0-471-49670-0},
  langid = {english}
}

@incollection{politis1992,
  title = {A {{Circular Block-resampling Procedure}} for {{Stationary Data}}},
  booktitle = {Exploring the {{Limits}} of {{Bootstrap}}},
  author = {Politis, D. N. and Romano, J. P.},
  editor = {LePage, Raoul and Billard, Lynne},
  year = {1992},
  pages = {263--270},
  publisher = {{John Wiley \& Sons}},
  address = {{New York}},
  googlebooks = {bR5UHwAACAAJ},
  isbn = {978-0-471-53631-4},
  langid = {english}
}

@article{politis1994,
  title = {The {{Stationary Bootstrap}}},
  author = {Politis, Dimitris N. and Romano, Joseph P.},
  year = {1994},
  journal = {Journal of the American Statistical Association},
  volume = {89},
  number = {428},
  eprint = {2290993},
  eprinttype = {jstor},
  pages = {1303--1313},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2290993},
  urldate = {2022-05-13},
  abstract = {This article introduces a resampling procedure called the stationary bootstrap as a means of calculating standard errors of estimators and constructing confidence regions for parameters based on weakly dependent stationary observations. Previously, a technique based on resampling blocks of consecutive observations was introduced to construct confidence intervals for a parameter of the m-dimensional joint distribution of m consecutive observations, where m is fixed. This procedure has been generalized by constructing a "blocks of blocks" resampling scheme that yields asymptotically valid procedures even for a multivariate parameter of the whole (i.e., infinite-dimensional) joint distribution of the stationary sequence of observations. These methods share the construction of resampling blocks of observations to form a pseudo-time series, so that the statistic of interest may be recalculated based on the resampled data set. But in the context of applying this method to stationary data, it is natural to require the resampled pseudo-time series to be stationary (conditional on the original data) as well. Although the aforementioned procedures lack this property, the stationary procedure developed here is indeed stationary and possesses other desirable properties. The stationary procedure is based on resampling blocks of random length, where the length of each block has a geometric distribution. In this article, fundamental consistency and weak convergence properties of the stationary resampling scheme are developed.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Politis_Romano_1994_The_Stationary_Bootstrap.pdf}
}

@article{polyak1992,
  title = {Acceleration of Stochastic Approximation by Averaging},
  author = {Polyak, B. and Juditsky, A.},
  year = {1992},
  volume = {20},
  pages = {838--855},
  number = {4},
  journal = {Siam Journal on Control and Optimization},
  doi = {10.1137/0330046},
  abstract = {A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Polyak_Juditsky_1992_Acceleration_of_stochastic_approximation_by_averaging.pdf}
}

@article{powell2009a,
  title = {The {{BOBYQA Algorithm}} for {{Bound Constrained Optimization}} without {{Derivatives}}},
  author = {Powell, M.},
  year = {2009},
  journal = {Technical Report, Department of Applied Mathematics and Theoretical Physics},
  abstract = {BOBYQA is an iterative algorithm for finding a minimum of a function F(x), x2Rn, subject to bounds axb on the variables, F being specified by a "black box" that returns the value F(x) for any feasible x. Each iteration employs a quadratic approximation Q to F that satisfies Q(yj )= F(yj), j =1 ,2,...,m, the interpolation points yj being chosen and adjusted automatically, but m is a prescribed constant, the value m =2 n+1 being typical. These conditions leave much freedom in Q, taken up when the model is updated by the highly successful technique of minimizing the Frobenius norm of the change to the second derivative matrix of Q. Thus no first derivatives of F are required explicitly. Most changes to the variables are an approximate solution to a trust region subproblem, using the current quadratic model, with a lower bound on the trust region radius that is reduced cautiously, in order to keep the interpolation points well separated until late in the calculation, which lessens damage from computer rounding errors. Some other changes to the variables are designed to improve the model without reducing F. These techniques are described. Other topics include the starting procedure that is given an initial vector of variables, the value of m and the initial trust region radius. There is also a new device called RESCUE that tries to restore normality if severe loss of accuracy occurs in the matrix calculations of the updating of the model. Numerical results are reported and discussed for two test problems, the numbers of variables being between 10 and 320.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Powell_2009_The_BOBYQA_Algorithm_for_Bound_Constrained_Optimization_without_Derivatives.pdf}
}

@article{prajapati2012,
  title = {Control Charts for Monitoring the Autocorrelated Process Parameters: A Literature Review},
  shorttitle = {Control Charts for Monitoring the Autocorrelated Process Parameters},
  author = {Prajapati, D.R. and Singh, Sukhraj},
  year = {2012},
  journal = {International Journal of Productivity and Quality Management},
  volume = {10},
  number = {2},
  pages = {207},
  issn = {1746-6474, 1746-6482},
  doi = {10.1504/IJPQM.2012.048298},
  urldate = {2023-10-09},
  abstract = {In most of the process monitoring, it is assumed that the observations from the process output are independent and identically distributed. But for many processes, the observations are correlated, and when this correlation build-up automatically in the entire process, it is known as autocorrelation. Autocorrelation among the observations can have significant effect on the performance of a control chart. The detection of special cause/s in the process may become very difficult in such situations. Several types of control charts and their combinations are evaluated for their ability to detect changes in the process mean and variance, since two decades. To counter the effect of autocorrelation, various new methodologies and approaches such as double sampling, variable sample sizes and sampling intervals, etc. are suggested by various researchers. Researchers also used Markov chain, timeseries approach, MATLAB and artificial neural networks for the simulation of the data. This paper provides a survey and brief summary of the work on the development of the control charts for variables to monitor the mean and dispersion for autocorrelated data.},
  langid = {english},
  keywords = {todo}
}

@article{pratola2022,
  title = {Editorial: {{Special Issue}} on {{Industry}} 4.0},
  shorttitle = {Editorial},
  author = {Pratola, Matthew T.},
  year = {2022},
  journal = {Technometrics},
  volume = {64},
  number = {4},
  pages = {435--436},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2022.2125710},
  urldate = {2023-10-10},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Pratola_2022_Editorial.pdf}
}

@article{profiliart2,
  title = {Current Research on Profile Monitoring},
  author = {Woodall, William H},
  year = {2007},
  journal = {Production},
  volume = {17},
  pages = {420--425},
  publisher = {{SciELO Brasil}}
}

@article{qiao2022,
  title = {Optimal {{Design}} of {{One-Sided Exponential EWMA Charts Based}} on {{Median Run Length}} and {{Expected Median Run Length}}},
  author = {Qiao, YuLong and Sun, JinSheng and Castagliola, Philippe and Hu, XueLong},
  year = {2022},
  journal = {Communications in Statistics - Theory and Methods},
  volume = {51},
  number = {9},
  pages = {2887--2907},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0926},
  doi = {10.1080/03610926.2020.1782937},
  urldate = {2023-09-13},
  abstract = {Exponential type charts are useful tools to monitor the time between events in high-quality processes with a low defect rate. Most studies on exponential charts are designed with the average run length (ARL) metric. The only use of ARL in the design of control charts is sometimes criticized because the shape of the run length (RL) distribution of control charts changes with the shift size. In fact, the RL distribution of the exponential exponentially weighted moving average (EWMA) chart is skewed, especially when the process is in-control. Hence, the median run length (MRL) serves as a more meaningful indicator. Moreover, in some situations, the shift size in the process is unknown in advance. Under this case, the expected median run length (EMRL) can be used as the metric. In this paper, the RL properties of both the upper- and lower-sided exponential EWMA charts are studied through a Markov chain approach. Two optimal design procedures are developed for one-sided exponential EWMA charts based on the MRL and EMRL, respectively. The choices of reflecting boundaries for one-sided exponential EWMA charts are discussed through many numerical studies. The MRL and EMRL performances of the one-sided exponential EWMA charts are investigated.},
  keywords = {62K05,62P30,expected median run length,Exponential chart,exponentially weighted moving average chart,median run length,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qiao_et_al_2022_Optimal_design_of_one-sided_exponential_EWMA_charts_based_on_median_run_length.pdf}
}

@article{qiu2001,
  title = {A {{Rank-Based Multivariate CUSUM Procedure}}},
  author = {Qiu, Peihua and Hawkins, Douglas},
  year = {2001},
  journal = {Technometrics},
  volume = {43},
  number = {2},
  eprint = {1271026},
  eprinttype = {jstor},
  pages = {120--132},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  urldate = {2022-09-29},
  abstract = {We consider statistical process control when measurements are multivariate. A cumulative sum (CUSUM) procedure is suggested in detecting a shift in the mean vector of the measurements, which is based on the cross-sectional antiranks of the measurements. At each time point, the measurements are ordered and their antiranks, which are the indices of the order statistics, are recorded. When the process is in control and the joint distribution of the multivariate measurements satisfies some regularity conditions, the antirank vector at each time point has a given distribution. This distribution changes to some other distribution when the process is out of control and the components of the shift in the mean vector of the process are not all the same. This CUSUM can therefore detect shifts in all directions except the one in which the components of the shift in the mean vector are all the same but not 0. The shift with equal components, however, can be easily detected by another univariate CUSUM. The former CUSUM procedure is distribution free in the sense that all its properties depend on the distribution of the antirank vector only.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qiu_Hawkins_2001_A_Rank-Based_Multivariate_CUSUM_Procedure.pdf}
}

@article{qiu2008,
  title = {Distribution-{{Free Multivariate Process Control Based}} on {{Log-Linear Modeling}}},
  author = {Qiu, Peihua},
  year = {2008},
  journal = {IIE Transactions},
  volume = {40},
  number = {7},
  pages = {664--677},
  publisher = {{Taylor \& Francis}},
  issn = {0740-817X},
  doi = {10.1080/07408170701744843},
  urldate = {2022-08-27},
  abstract = {This paper considers Statistical Process Control (SPC) when the process measurement is multivariate. In the literature, most existing multivariate SPC procedures assume that the in-control distribution of the multivariate process measurement is known and it is a Gaussian distribution. In applications, however, the measurement distribution is usually unknown and it needs to be estimated from data. Furthermore, multivariate measurements often do not follow a Gaussian distribution (e.g., cases when some measurement components are discrete). We demonstrate that results from conventional multivariate SPC procedures are usually unreliable when the data are non-Gaussian. Existing statistical tools for describing multivariate non-Gaussian data, or transforming the multivariate non-Gaussian data to multivariate Gaussian data, are limited, making appropriate multivariate SPC difficult in such cases. In this paper, we suggest a methodology for estimating the in-control multivariate measurement distribution when a set of in-control data is available, which is based on log-linear modeling and which takes into account the association structure among the measurement components. Based on this estimated in-control distribution, a multivariate CUSUM procedure for detecting shifts in the location parameter vector of the measurement distribution is also suggested for Phase II SPC. This procedure does not depend on the Gaussian distribution assumption; thus, it is appropriate to use for most multivariate SPC problems.},
  keywords = {Discrete measurements,done,log-linear modeling,multivariate distribution,non-Gaussian data,nonparametric procedures,statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qiu_2008_Distribution-free_multivariate_process_control_based_on_log-linear_modeling.pdf}
}

@book{qiu2013,
  title = {Introduction to {{Statistical Process Control}}},
  author = {Qiu, Peihua},
  year = {2013},
  publisher = {{CRC Press}},
  address = {{Boca Raton, FL}},
  abstract = {A major tool for quality control and management, statistical process control (SPC) monitors sequential processes, such as production lines and Internet traffic, to ensure that they work stably and satisfactorily. Along with covering traditional methods, Introduction to Statistical Process Control describes many recent SPC methods that improve upon},
  googlebooks = {OYfSBQAAQBAJ},
  isbn = {978-1-4822-2041-4},
  langid = {english},
  keywords = {Business \& Economics / Quality Control,Mathematics / Probability \& Statistics / General,Technology \& Engineering / Quality Control}
}

@article{qiu2018,
  title = {Nonparametric {{Dynamic Curve Monitoring}}},
  author = {Qiu, Peihua and Zi, Xuemin and Zou, Changliang},
  year = {2018},
  journal = {Technometrics},
  volume = {60},
  number = {3},
  pages = {386--397},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2017.1361340},
  urldate = {2022-09-07},
  abstract = {Rapid sequential comparison between the longitudinal pattern of a given subject and a target pattern has become increasingly important in modern scientific research for detecting abnormal activities in many data-rich applications. This article focuses on this problem when observations are collected sequentially with uncorrelated or correlated noise involved. A dynamic monitoring procedure is developed after connecting the curve monitoring problem to curve comparison. Under the framework of generalized likelihood ratio testing, we suggest a new exponentially weighted moving average (EWMA) control chart that can accommodate unequally spaced design points. An adaptive parameter selection feature is built in the proposed control chart so that the chart can detect a wide range of longitudinal pattern shifts effectively. To furnish fast computation, recursive formulas are derived for computing the charting statistic. Numerical studies show that the proposed method can deliver a satisfactory performance, and it outperforms existing methods in various cases. An example from the semiconductor manufacturing industry is used for the illustration of its implementation. Supplementary materials for this article are available online.},
  keywords = {Adaptive test,Curve comparison,done,Generalized likelihood ratio test,Model checking,Nadaraya\textendash Watson kernel estimation,Statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qiu_et_al_2018_Nonparametric_Dynamic_Curve_Monitoring.pdf;/home/dede/Documents/MEGA/universita/zotero-pdf/Qiu_et_al_2018_Nonparametric_Dynamic_Curve_Monitoring2.pdf}
}

@article{qiu2021,
  title = {Transparent {{Sequential Learning}} for {{Statistical Process Control}} of {{Serially Correlated Data}}},
  author = {Qiu, Peihua and Xie, Xiulin},
  year = {2021},
  journal = {Technometrics},
  volume = {64},
  number = {4},
  pages = {487--501},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2021.1929493},
  urldate = {2021-10-18},
  abstract = {Machine learning methods have been widely used in different applications, including process control and monitoring. For handling statistical process control (SPC) problems, conventional supervised machine learning methods (e.g., artificial neural networks and support vector machines) would have some difficulties. For instance, a training dataset containing both in-control and out-of-control (OC) process observations is required by a supervised machine learning method, but it is rarely available in SPC applications. Furthermore, many machine learning methods work like black boxes. It is often difficult to interpret their learning mechanisms and the resulting decision rules in the context of an application. In the SPC literature, there have been some existing discussions on how to handle the lack of OC observations in the training data, using the one-class classification, artificial contrast, real-time contrast, and some other novel ideas. However, these approaches have their own limitations to handle SPC problems. In this article, we extend the self-starting process monitoring idea that has been employed widely in modern SPC research to a general learning framework for monitoring processes with serially correlated data. Under the new framework, process characteristics to learn are well specified in advance, and process learning is sequential in the sense that the learned process characteristics keep being updated during process monitoring. The learned process characteristics are then incorporated into a control chart for detecting process distributional shift based on all available data by the current observation time. Numerical studies show that process monitoring based on the new learning framework is more reliable and effective than some representative existing machine learning SPC approaches.},
  keywords = {Data correlation,done,Machine learning,Recursive computation,Self-starting charts,Sequential learning,Statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qiu_Xie_2021_Transparent_Sequential_Learning_for_Statistical_Process_Control_of_Serially.pdf;/home/dede/Documents/MEGA/universita/zotero-pdf/Qiu_Xie_2021_Transparent_Sequential_Learning_for_Statistical_Process_Control_of_Serially2.pdf;/home/dede/Zotero/storage/627E5QRV/00401706.2021.html}
}

@article{qiu2022,
  title = {Dynamic {{Disease Screening}} by {{Joint Modelling}} of {{Survival}} and {{Longitudinal Data}}},
  author = {Qiu, Peihua and You, Lu},
  year = {2022},
  volume = {71},
  number = {5},
  pages = {1158-1180},
  journal = {Journal of the Royal Statistical Society C (Applied Statistics)},
  issn = {1467-9876},
  doi = {10.1111/rssc.12573},
  urldate = {2022-09-07},
  abstract = {Sequential monitoring of dynamic processes is an active research area because of its broad applications in different industries and scientific research projects, including disease screening in medical research. In the literature, it has been shown that dynamic screening system (DySS) is a powerful tool for sequential monitoring of dynamic processes. To detect a disease (e.g. stroke) for a patient, existing DySS methods first estimate the regular longitudinal pattern of certain disease predictors (e.g. blood pressure, cholesterol level) from an in-control (IC) dataset that contains observations of a group of non-diseased people, and then compare the longitudinal pattern of the observed disease predictors of the given patient with the estimated regular longitudinal pattern. A signal of disease occurrence is triggered if their cumulative difference exceeds a certain level, facilitated by a built-in control chart. In practice, a dataset containing longitudinal observations of the disease predictors of both non-diseased and diseased people is often available in advance, from which it is possible to explore the relationship between the disease occurrence and the longitudinal pattern of the disease predictors. This relationship should be helpful for disease screening. In this paper, a new DySS method is suggested based on this idea. Numerical studies confirm that it can improve the existing DySS methods for disease screening.},
  langid = {english},
  keywords = {disease predictors,doing,dynamic processes,joint modelling,longitudinal data,survival analysis,time to event},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qiu_You_2022_Dynamic_disease_screening_by_joint_modelling_of_survival_and_longitudinal_data.pdf;/home/dede/Zotero/storage/6JEPTY2L/rssc.html}
}

@book{ramsay2002,
  title = {Applied {{Functional Data Analysis}}: {{Methods}} and {{Case Studies}}},
  shorttitle = {Applied {{Functional Data Analysis}}},
  author = {Ramsay, J. O. and Silverman, B. W.},
  year = {2002},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b98886},
  urldate = {2021-04-19},
  abstract = {Almost as soon as we had completed our previous book Functional Data Analysis in 1997, it became clear that potential interest in the ?eld was far wider than the audience for the thematic presentation we had given there. At the same time, both of us rapidly became involved in relevant new research involving many colleagues in ?elds outside statistics. This book treats the ?eld in a di?erent way, by considering case st- ies arising from our own collaborative research to illustrate how functional data analysis ideas work out in practice in a diverse range of subject areas. These include criminology, economics, archaeology, rheumatology, psych- ogy, neurophysiology, auxology (the study of human growth), meteorology, biomechanics, and education\textemdash and also a study of a juggling statistician. Obviously such an approach will not cover the ?eld exhaustively, and in any case functional data analysis is not a hard-edged closed system of thought. Nevertheless we have tried to give a ?avor of the range of meth- ology we ourselves have considered. We hope that our personal experience, including the fun we had working on these projects, will inspire others to extend ``functional'' thinking to many other statistical contexts. Of course, manyofourcasestudiesrequireddevelopmentofexistingmethodology,and readersshouldgaintheabilitytoadaptmethodstotheirownproblemstoo.},
  isbn = {978-0-387-95414-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ramsay_Silverman_2002_Applied_Functional_Data_Analysis.pdf;/home/dede/Zotero/storage/3ULIIKB7/9780387954141.html}
}

@book{ramsay2005,
  title = {Functional {{Data Analysis}}},
  author = {Ramsay, James and Silverman, B. W.},
  year = {2005},
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {2},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b98888},
  urldate = {2021-04-19},
  abstract = {Scientists and others today often collect samples of curves and other functional observations. This monograph presents many ideas and techniques for such data. Included are expressions in the functional domain of such classics as linear regression, principal components analysis, linear modeling, and canonical correlation analysis, as well as specifically functional techniques such as curve registration and principal differential analysis. Data arising in real applications are used throughout for both motivation and illustration, showing how functional approaches allow us to see new things, especially by exploiting the smoothness of the processes generating the data. The data sets exemplify the wide scope of functional data analysis; they are drawn from growth analysis, meteorology, biomechanics, equine science, economics, and medicine. The book presents novel statistical technology, much of it based on the authors' own research work, while keeping the mathematical level widely accessible. It is designed to appeal to students, to applied data analysts, and to experienced researchers; it will have value both within statistics and across a broad spectrum of other fields. This second edition is aimed at a wider range of readers, and especially those who would like to apply these techniques to their research problems. It complements the authors' other recent volume Applied Functional Data Analysis: Methods and Case Studies. In particular, there is an extended coverage of data smoothing and other matters arising in the preliminaries to a functional data analysis. The chapters on the functional linear model and modeling of the dynamics of systems through the use of differential equations and principal differential analysis have been completely rewritten and extended to include new developments. Other chapters have been revised substantially, often to give more weight to examples and practical considerations. Jim Ramsay is Professor of Psychology at McGill University and is an international authority on many aspects of multivariate analysis. He was President of the Statistical Society of Canada in 2002-3 and holds the Society's Gold Medal for his work in functional data analysis. Bernard Silverman is Master of St Peter's College and Professor of Statistics at Oxford University. He was President of the Institute of Mathematical Statistics in 2000\textendash 1. He is a Fellow of the Royal Society. His main specialty is in computational statistics, and he is the author or editor of several highly regarded books in this area.},
  isbn = {978-0-387-40080-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ramsay_Silverman_2005_Functional_Data_Analysis.pdf;/home/dede/Zotero/storage/6FCI636S/9780387400808.html}
}

@article{ranger1996,
  title = {Choosing Principal Components for Multivariate Statistical Process Control},
  author = {Ranger, George C. and Alt, Frank B.},
  year = {1996},
  journal = {Communications in Statistics - Theory and Methods},
  volume = {25},
  number = {5},
  pages = {909--922},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0926},
  doi = {10.1080/03610929608831739},
  urldate = {2023-10-23},
  abstract = {Principal components are useful for multivariate process control. Typically, the principal component variables are often selected to summarize the variation in the process data. We provide an analysis to select the principal component variables to be included in a multivariate control chart that incorporates the unique aspects of the process control problem (rather than using traditional principal component guidelines).},
  keywords = {chi-square chart,quality control,selection of variables,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ranger_Alt_1996_Choosing_principal_components_for_multivariate_statistical_process_control.pdf}
}

@article{rao2015,
  title = {Online {{Real-Time Quality Monitoring}} in {{Additive Manufacturing Processes Using Heterogeneous Sensors}}},
  author = {Rao, Prahalad K. and Liu, Jia (Peter) and Roberson, David and Kong, Zhenyu (James) and Williams, Christopher},
  year = {2015},
  journal = {Journal of Manufacturing Science and Engineering},
  volume = {137},
  number = {061007},
  issn = {1087-1357},
  doi = {10.1115/1.4029823},
  urldate = {2023-10-13},
  abstract = {The objective of this work is to identify failure modes and detect the onset of process anomalies in additive manufacturing (AM) processes, specifically focusing on fused filament fabrication (FFF). We accomplish this objective using advanced Bayesian nonparametric analysis of in situ heterogeneous sensor data. Experiments are conducted on a desktop FFF machine instrumented with a heterogeneous sensor array including thermocouples, accelerometers, an infrared (IR) temperature sensor, and a real-time miniature video borescope. FFF process failures are detected online using the nonparametric Bayesian Dirichlet process (DP) mixture model and evidence theory (ET) based on the experimentally acquired sensor data. This sensor data-driven defect detection approach facilitates real-time identification and correction of FFF process drifts with an accuracy and precision approaching 85\% (average F-score). In comparison, the F-score from existing approaches, such as probabilistic neural networks (PNN), na\"ive Bayesian clustering, support vector machines (SVM), and quadratic discriminant analysis (QDA), was in the range of 55\textendash 75\%.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rao_et_al_2015_Online_Real-Time_Quality_Monitoring_in_Additive_Manufacturing_Processes_Using.pdf;/home/dede/Zotero/storage/WK7RNWU4/Online-Real-Time-Quality-Monitoring-in-Additive.html}
}

@manual{rcoreteam2023,
  type = {Manual},
  title = {\proglang{R}: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2023},
  address = {{Vienna, Austria}},
  institution = {{R Foundation for Statistical Computing}},
  keywords = {todo}
}

@article{reuter2009,
  title = {Laplace\textendash{{Beltrami}} Eigenvalues and Topological Features of Eigenfunctions for Statistical Shape Analysis},
  author = {Reuter, Martin and Wolter, Franz-Erich and Shenton, Martha and Niethammer, Marc},
  year = {2009},
  journal = {Computer-Aided Design},
  series = {Selected {{Papers}} from the 2007 {{New Advances}} in {{Shape Analysis}} and {{Geometric Modeling Workshop}}},
  volume = {41},
  number = {10},
  pages = {739--755},
  issn = {0010-4485},
  doi = {10.1016/j.cad.2009.02.007},
  urldate = {2023-10-10},
  abstract = {This paper proposes the use of the surface-based Laplace\textendash Beltrami and the volumetric Laplace eigenvalues and eigenfunctions as shape descriptors for the comparison and analysis of shapes. These spectral measures are isometry invariant and therefore allow for shape comparisons with minimal shape pre-processing. In particular, no registration, mapping, or remeshing is necessary. The discriminatory power of the 2D surface and 3D solid methods is demonstrated on a population of female caudate nuclei (a subcortical gray matter structure of the brain, involved in memory function, emotion processing, and learning) of normal control subjects and of subjects with schizotypal personality disorder. The behavior and properties of the Laplace\textendash Beltrami eigenvalues and eigenfunctions are discussed extensively for both the Dirichlet and Neumann boundary condition showing advantages of the Neumann vs. the Dirichlet spectra in 3D. Furthermore, topological analyses employing the Morse\textendash Smale complex (on the surfaces) and the Reeb graph (in the solids) are performed on selected eigenfunctions, yielding shape descriptors, that are capable of localizing geometric properties and detecting shape differences by indirectly registering topological features such as critical points, level sets and integral lines of the gradient field across subjects. The use of these topological features of the Laplace\textendash Beltrami eigenfunctions in 2D and 3D for statistical shape analysis is novel.},
  keywords = {Brain structure,Caudate nucleus,Eigenfunctions,Eigenvalues,Laplace\textendash Beltrami spectra,Morse\textendash Smale complex,Nodal domains,Reeb graph,Schizotypal personality disorder},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Reuter_et_al_2009_Laplace–Beltrami_eigenvalues_and_topological_features_of_eigenfunctions_for.pdf;/home/dede/Zotero/storage/CC4SDKEG/S0010448509000463.html}
}

@article{reynolds2008,
  title = {Combinations of {{Multivariate Shewhart}} and {{MEWMA Control Charts}} for {{Monitoring}} the {{Mean Vector}} and {{Covariance Matrix}}},
  author = {Reynolds, Marion R. and Stoumbos, Zachary G.},
  year = {2008},
  journal = {Journal of Quality Technology},
  volume = {40},
  number = {4},
  pages = {381--393},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2008.11917744},
  urldate = {2023-09-10},
  abstract = {When monitoring a process that has multivariate normal variables, the Shewhart-type control chart traditionally used for monitoring the process mean vector is effective for detecting large shifts, but for detecting small shifts, it is more effective to use the multivariate exponentially weighted moving average (MEWMA) control chart. It has been proposed that better overall performance in detecting small and large shifts in the mean can be obtained by using the MEWMA chart in combination with the Shewhart chart. Here we investigate the performance of this combination in the context of the more general problem of detecting changes in the mean or increases in variability. Recent investigations of combinations of the MEWMA chart for the mean and MEWMA-type charts based on the squared deviations of the observations from the target has shown that these combinations have excellent performance in detecting sustained shifts in the mean or in variability. Here we consider both sustained and transient shifts and show that a combination of two MEWMA charts has better overall performance than the combination of the MEWMA and Shewhart charts. We also consider a three-chart combination consisting of the MEWMA chart for the mean, an MEWMA-type chart of the squared deviations from target, and the Shewhart chart. When the sample size is n = 1, this three-chart combination does not seem to have better overall performance than the combination of the two MEWMA charts. When n \$gt 1, the three-chart combination has significantly better performance for some mean shifts, but somewhat worse performance for shifts in variability.},
  keywords = {Average Time to Signal,Multivariate Exponentially Weighted Moving Average Control Chart,Regression Adjustment of Variables,Shewhart Chart,Squared Deviations from Target,Statistical Process Control,Steady-State Average Time to Signal,Surveillance,todo,Transient Shift},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Reynolds_Stoumbos_2008_Combinations_of_Multivariate_Shewhart_and_MEWMA_Control_Charts_for_Monitoring.pdf}
}

@article{riaz2021,
  title = {Advanced Multivariate Cumulative Sum Control Charts Based on Principal Component Method with Application},
  author = {Riaz, Muhammad and Zaman, Babar and Mehmood, Rashid and Abbas, Nasir and Abujiya, Mu'azu},
  year = {2021},
  journal = {Quality and Reliability Engineering International},
  volume = {37},
  number = {6},
  pages = {2760--2789},
  issn = {1099-1638},
  doi = {10.1002/qre.2889},
  urldate = {2023-10-23},
  abstract = {Existing multivariate cumulative sum (MCUSUM) control charts involve entire associated variables of a process to monitor variations in the mean vector. In this study, we have offered MCUSUM control charts with principal component method (PCM). The proposed MCUSUM control charts with PCM capture the whole process variations using fewer latent variables (principal components) while preserving as much data variability as possible. To show the significance of proposed MCUSUM control charts with PCM, various performance measures are considered including average run length, extra quadratic loss, relative average run length, and performance comparison index. Furthermore, performance measures are calculated through advanced Monte Carlo simulation method to explore the behavior of proposed MCUSUM control charts and to conduct comparative analysis with existing models. Results revealed that proposed MCUSUM control charts with PCM are efficient to detect variations timely by involving smaller number of principal components instead of considering entire associated variables. Also, proposed MCUSUM control charts have the ability to accommodate the features of existing control charts, which are illustrated as the special cases. Besides, to highlight the implementation mechanism and advantages of proposed MCUSUM control charts with PCM, a real-life example from wind turbine process is included.},
  copyright = {\textcopyright{} 2021 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {average run length,control charts,Monte Carlo simulation,multivariate CUSUM,principle component,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Riaz_et_al_2021_Advanced_multivariate_cumulative_sum_control_charts_based_on_principal.pdf;/home/dede/Zotero/storage/6ZHY3RYM/qre.html}
}

@article{rigdon1995,
  title = {An Integral Equation for the In-Control Average Run Length of a Multivariate Exponentially Weighted Moving Average Control Chart},
  author = {Rigdon, Steven E.},
  year = {1995},
  journal = {Journal of Statistical Computation and Simulation},
  volume = {52},
  number = {4},
  pages = {351--365},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10.1080/00949659508811685},
  urldate = {2023-05-17},
  abstract = {An integral equation is given for the in-control average run length of a multivariate exponentially weighted moving average control chart. This integral equation is used to determine the appropriate upper control limits for various values of the smoothing constant r, the dimension p of the measured quality characteristic, and the desired in-control average run length L o Tables are given which allow a user to select the appropriate value of the upper control limit given these three conditions.},
  keywords = {Multivariate quality control,Radau quadrature,Secant method}
}

@article{rigdon1995a,
  title = {A Double-Integral Equation for the Average Run Length of a Multivariate Exponentially Weighted Moving Average Control Chart},
  author = {Rigdon, Steven E},
  year = {1995},
  journal = {Statistics \& Probability Letters},
  volume = {24},
  number = {4},
  pages = {365--373},
  issn = {0167-7152},
  doi = {10.1016/0167-7152(94)00196-F},
  urldate = {2023-05-17},
  abstract = {The multivariate exponentially weighted moving average control chart is a control charting scheme that uses weighted averages of previously observed random vectors. This scheme, which is defined using Z0 = {$\mu$}0, Zi = rXi + (1 - r)Zi - 1 (i {$\geqslant$} 1), where X1, X2, \ldots{} denote the vector-valued output of a process, can be used to detect shifts in the process mean vector more quickly, on the average, than the usual Hotelling T2 chart. We prove that for the special case {$\mu$} = 0, {$\Sigma$} = I, the average run length (ARL) depends on the initial value z0 for the MEWMA statistic only through its magnitude and the angle it makes with the mean vector. This theorem is then used to derive an integral equation of the ARL. This integral equation involves a double integral, and the unknown function is a function of two variables. ARLs can be obtained by approximating the solution to the integral equation. Previously, simulation was needed to approximate the ARLs.},
  langid = {english},
  keywords = {Hotelling  chart,Iterated kernels,Multivariate quality control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rigdon_1995_A_double-integral_equation_for_the_average_run_length_of_a_multivariate.pdf}
}

@article{robbins1951,
  title = {A {{Stochastic Approximation Method}}},
  author = {Robbins, Herbert and Monro, Sutton},
  year = {1951},
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {3},
  pages = {400--407},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729586},
  urldate = {2021-04-08},
  abstract = {Let \$M(x)\$ denote the expected value at level \$x\$ of the response to a certain experiment. \$M(x)\$ is assumed to be a monotone function of \$x\$ but is unknown to the experimenter, and it is desired to find the solution \$x = \textbackslash theta\$ of the equation \$M(x) = \textbackslash alpha\$, where \$\textbackslash alpha\$ is a given constant. We give a method for making successive experiments at levels \$x\_1,x\_2,\textbackslash cdots\$ in such a way that \$x\_n\$ will tend to \$\textbackslash theta\$ in probability.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Robbins_Monro_1951_A_Stochastic_Approximation_Method.pdf;/home/dede/Zotero/storage/HGYIJNZP/1177729586.html}
}

@article{roberts1959,
  title = {Control {{Chart Tests Based}} on {{Geometric Moving Averages}}},
  author = {Roberts, S. W.},
  year = {1959},
  journal = {Technometrics},
  volume = {1},
  number = {3},
  pages = {239--250},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1959.10489860},
  urldate = {2023-01-23},
  abstract = {A geometrical moving average gives the most recent observation the greatest weight, and all previous observations weights decreasing in geometric progression from the most recent back to the first. A graphical procedure for generating geometric moving averages is described in which the most recent observation is assigned a weight r. The properties of control chart tests based on geometric moving averages are compared to tests based on ordinary moving averages.},
  keywords = {done}
}

@article{roberts1966,
  title = {A {{Comparison}} of {{Some Control Chart Procedures}}},
  author = {Roberts, S. W.},
  year = {1966},
  journal = {Technometrics},
  volume = {8},
  number = {3},
  pages = {411--430},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1966.10490374},
  urldate = {2023-09-06},
  abstract = {This paper unifies and extends previously published characterizations of moving average, geometric moving average, and cumulative sum control chart procedures. It presents comparable characterizations of two procedures based on tests described but not evaluated in earlier papers. One of these procedures is based on a test devised by Girshick and Rubin that is optimal under a particular set of idealized conditions. The other procedure is based on run sum tests, which are generalizations of the type of run test that counts the number of consecutive points that exceed a limit, the generalization taking into account the extent that points in such a run exceed the limit.},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Roberts_1966_A_Comparison_of_Some_Control_Chart_Procedures.pdf}
}

@article{ross2015,
  title = {Parametric and Nonparametric Sequential Change Detection in {{\proglang{R}}}: {{The}} {{\pkg{cpm}}} Package},
  author = {Ross, Gordon J.},
  year = {2015},
  journal = {Journal of Statistical Software},
  url = {https://www.jstatsoft.org/v66/i03/},
  volume = {66},
  number = {3},
  pages = {1--20},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ross_2015_Parametric_and_nonparametric_sequential_change_detection_in_R.pdf}
}

@incollection{ruppert1991,
  title = {Stochastic {{Approximation}}},
  booktitle = {Handbook of {{Sequential Analysis}}},
  author = {Ruppert, D.},
  editor = {Ghosh, B. K. and Sen, P. K.},
  year = {1991},
  pages = {503--529},
  address = {{New York, NY}}
}

@inproceedings{rusinkiewicz2001,
  title = {Efficient Variants of the {{ICP}} Algorithm},
  booktitle = {Proceedings {{Third International Conference}} on 3-{{D Digital Imaging}} and {{Modeling}}},
  author = {Rusinkiewicz, S. and Levoy, M.},
  year = {2001},
  pages = {145--152},
  doi = {10.1109/IM.2001.924423},
  urldate = {2023-10-13},
  abstract = {The ICP (Iterative Closest Point) algorithm is widely used for geometric alignment of three-dimensional models when an initial estimate of the relative pose is known. Many variants of ICP have been proposed, affecting all phases of the algorithm from the selection and matching of points to the minimization strategy. We enumerate and classify many of these variants, and evaluate their effect on the speed with which the correct alignment is reached. In order to improve convergence for nearly-flat meshes with small features, such as inscribed surfaces, we introduce a new variant based on uniform sampling of the space of normals. We conclude by proposing a combination of ICP variants optimized for high speed. We demonstrate an implementation that is able to align two range images in a few tens of milliseconds, assuming a good initial guess. This capability has potential application to real-time 3D model acquisition and model-based tracking.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rusinkiewicz_Levoy_2001_Efficient_variants_of_the_ICP_algorithm.pdf;/home/dede/Zotero/storage/F5B9SFU9/924423.html}
},

@Article{meyer2017,
  author = {Sebastian Meyer and Leonhard Held and Michael Höhle},
  title = {Spatio-Temporal Analysis of Epidemic Phenomena Using the \proglang{R} Package \pkg{surveillance}},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {77},
  number = {11},
  pages = {1--55},
  doi = {10.18637/jss.v077.i11},
}

@article{salmon2016,
  title = {Monitoring {{Count Time Series}} in {{\proglang{R}}}: {{Aberration Detection}} in {{Public Health Surveillance}}},
  shorttitle = {Monitoring {{Count Time Series}} in {{R}}},
  author = {Salmon, Ma{\"e}lle and Schumacher, Dirk and H{\"o}hle, Michael},
  year = {2016},
  journal = {Journal of Statistical Software},
  volume = {70},
  pages = {1--35},
  issn = {1548-7660},
  doi = {10.18637/jss.v070.i10},
  urldate = {2023-11-07},
  abstract = {Public health surveillance aims at lessening disease burden by, e.g., timely recognizing emerging outbreaks in case of infectious diseases. Seen from a statistical perspective, this implies the use of appropriate methods for monitoring time series of aggregated case reports. This paper presents the tools for such automatic aberration detection offered by the \proglang{R} package surveillance. We introduce the functionalities for the visualization, modeling and monitoring of surveillance time series. With respect to modeling we focus on univariate time series modeling based on generalized linear models (GLMs), multivariate GLMs, generalized additive models and generalized additive models for location, shape and scale. Applications of such modeling include illustrating implementational improvements and extensions of the well-known Farrington algorithm, e.g., by spline-modeling or by treating it in a Bayesian context. Furthermore, we look at categorical time series and address overdispersion using beta-binomial or Dirichlet-multinomial modeling. With respect to monitoring we consider detectors based on either a Shewhart-like single timepoint comparison between the observed count and the predictive distribution or by likelihoodratio based cumulative sum methods. Finally, we illustrate how surveillance can support aberration detection in practice by integrating it into the monitoring workflow of a public health institution. Altogether, the present article shows how well surveillance can support automatic aberration detection in a public health surveillance context.},
  copyright = {Copyright (c) 2016 Ma\"elle Salmon, Dirk Schumacher, Michael H\"ohle},
  langid = {english},
  keywords = {outbreak detection,R,statistical process control,surveillance,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Salmon_et_al_2016_Monitoring_Count_Time_Series_in_R.pdf}
}

@book{santos-fernandez2012,
  title = {Multivariate {{Statistical Quality Control Using \proglang{R}}}},
  author = {{Santos-Fern{\'a}ndez}, Edgar},
  year = {2012},
  volume = {14},
  doi = {10.1007/978-1-4614-5453-3},
  abstract = {The intensive use of automatic data acquisition system and the use of cloud computing for process monitoring have led to an increased occurrence of industrial processes that utilize statistical process control and capability analysis. These analyses are performed almost exclusively with multivariate methodologies. The aim of this Brief is to present the most important MSQC techniques developed in R language. The book is divided into two parts. The first part contains the basic R elements, an introduction to statistical procedures, and the main aspects related to Statistical Quality Control (SQC). The second part covers the construction of multivariate control charts, the calculation of Multivariate Capability Indices.},
  isbn = {978-1-4614-5453-3},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Santos-Fernandez_2012_Multivariate_Statistical_Quality_Control_Using_R.pdf}
}

@article{savitzky1964a,
  title = {Smoothing and {{Differentiation}} of {{Data}} by {{Simplified Least Squares Procedures}}.},
  author = {Savitzky, {\relax Abraham}. and Golay, M. J. E.},
  year = {1964},
  journal = {Analytical Chemistry},
  volume = {36},
  number = {8},
  pages = {1627--1639},
  publisher = {{American Chemical Society}},
  issn = {0003-2700},
  doi = {10.1021/ac60214a047},
  urldate = {2023-10-21},
  file = {/home/dede/Zotero/storage/7NRJT9EB/ac60214a047.html}
}

@inproceedings{sbriglia2016,
  title = {Embedding {{Sensors}} in {{FDM Plastic Parts During Additive Manufacturing}}},
  booktitle = {Topics in {{Modal Analysis}} \& {{Testing}}, {{Volume}} 10},
  author = {Sbriglia, Lexey R. and Baker, Andrew M. and Thompson, James M. and Morgan, Robert V. and Wachtor, Adam J. and Bernardin, John D.},
  editor = {Mains, Michael},
  year = {2016},
  series = {Conference {{Proceedings}} of the {{Society}} for {{Experimental Mechanics Series}}},
  pages = {205--214},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-30249-2_17},
  abstract = {In additive manufacturing, there is a necessity to qualify both the geometrical and material characteristics of the fabricated part, because both are being created simultaneously as the part is built up layer by layer. Increased availability of open source fused deposition modeling  machines has expanded the parameter space for which the user has control during the build process. This work quantifies the effects of operator choices, such as print speed, printer head and build plate temperatures, layering thickness, or building in a thermally controlled or fully open environment, on the quality and reproducibility of the build. Modal analyses were performed on completed builds using an electrodynamic shaker and integrated circuit piezoelectric accelerometers embedded in the parts during the build process. Experimental measurements of the fused deposition modeled parts were benchmarked against eigenvalue analysis results for an idealized part with homogenous material properties to gauge the suitability of such analysis to fused deposition modeling additive manufacturing. Follow on work will use this embedded technique for state-of-health monitoring in deployed systems and real-time diagnostics and control of the build process.},
  isbn = {978-3-319-30249-2},
  langid = {english},
  keywords = {Additive manufacturing,Embedded sensors,FDM,Fused deposition modeling,Modal analysis}
}

@article{scholkopf1998,
  title = {Nonlinear {{Component Analysis}} as a {{Kernel Eigenvalue Problem}}},
  author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander and M{\"u}ller, Klaus-Robert},
  year = {1998},
  journal = {Neural Computation},
  volume = {10},
  number = {5},
  pages = {1299--1319},
  issn = {0899-7667},
  doi = {10.1162/089976698300017467},
  urldate = {2023-10-11},
  abstract = {A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map\textemdash for instance, the space of all possible five-pixel products in 16 \texttimes{} 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Scholkopf_et_al_1998_Nonlinear_Component_Analysis_as_a_Kernel_Eigenvalue_Problem.pdf;/home/dede/Zotero/storage/4NDCK8N5/Nonlinear-Component-Analysis-as-a-Kernel.html}
}

@article{scimone2022,
  title = {Statistical {{Modeling}} and {{Monitoring}} of {{Geometrical Deviations}} in {{Complex Shapes With Application}} to {{Additive Manufacturing}}},
  author = {Scimone, Riccardo and Taormina, Tommaso and Colosimo, Bianca Maria and Grasso, Marco and Menafoglio, Alessandra and Secchi, Piercesare},
  year = {2022},
  journal = {Technometrics},
  volume = {64},
  number = {4},
  pages = {437--456},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2021.1961870},
  urldate = {2023-07-12},
  abstract = {The industrial development of new production processes like additive manufacturing (AM) is making available novel types of complex shapes that go beyond traditionally manufactured geometries and 2.5D free-form surfaces. New challenges must be faced to characterize, model and monitor the natural variability of such complex shapes, since previously proposed methods based on parametric models are not applicable. The present study proposes a methodology that applies to complex shapes represented in the form of triangulated meshes, which is the current standard for AM data format. The method combines a novel bi-directional way to model the deviation between the reconstructed geometry (e.g., via X-ray computed tomography) and the nominal geometry (i.e., the originating 3D model) with a profile monitoring approach for the detection of out-of-control shapes. A paradigmatic example consisting of an egg-shaped trabecular shell representative of real parts produced via AM is used to illustrate the methodology and to test its effectiveness in detecting real geometrical distortions.},
  keywords = {Additive manufacturing,Complex shape,Geometrical defects,Industry 4.0,Statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Scimone_et_al_2022_Statistical_Modeling_and_Monitoring_of_Geometrical_Deviations_in_Complex_Shapes.pdf}
}

@article{scranton1996,
  title = {Efficient Shift Detection Using Multivariate Exponentially-Weighted Moving Average Control Charts and Principal Components},
  author = {Scranton, Richard and Runger, George C. and Keats, J. Bert and Montgomery, Douglas C.},
  year = {1996},
  journal = {Quality and Reliability Engineering International},
  volume = {12},
  number = {3},
  pages = {165--171},
  issn = {1099-1638},
  doi = {10.1002/(SICI)1099-1638(199605)12:3<165::AID-QRE990>3.0.CO;2-Q},
  urldate = {2023-10-23},
  abstract = {This paper demonstrates the use of principal components in conjunction with the multivariate exponentially-weighted moving average (MEWMA) control procedure for process monitoring. It is demonstrated that the number of variables to be monitored is reduced through this approach, and that the average run length to detect process shifts or upsets is substantially reduced as well. The performance of the MEWMA applied to all the variables may be related to the MEWMA control chart that uses principal components through the non-centrality parameter. An average run length table demonstrates the advantages of the principal components MEWMA over the procedure that uses all of the variables. An illustrative example is provided.},
  copyright = {Copyright \textcopyright{} 1996 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {average run length,exponentially-weighted moving average,multivariate quality control,statistical process control,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Scranton_et_al_1996_Efficient_shift_detection_using_multivariate_exponentially-weighted_moving.pdf;/home/dede/Zotero/storage/DVU6I639/(SICI)1099-1638(199605)123165AID-QRE9903.0.html}
}

@article{scrucca2004,
  title = {\pkg{qcc}: An {{\proglang{R}}} Package for Quality Control Charting and Statistical Process Control},
  shorttitle = {Qcc},
  author = {Scrucca, Luca and Snow, Greg and Bloomfield, Peter},
  year = {2004},
  journal = {\proglang{R} News},
  volume = {4/1},
  pages = {11--17},
  urldate = {2023-04-29},
  abstract = {Shewhart quality control charts for continuous, attribute and count data. Cusum and EWMA charts. Operating characteristic curves. Process capability analysis. Pareto chart and cause-and-effect chart. Multivariate control charts.},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {todo}
}

@article{shang2014,
  title = {A Survey of Functional Principal Component Analysis},
  author = {Shang, Han Lin},
  year = {2014},
  journal = {AStA Advances in Statistical Analysis},
  volume = {98},
  number = {2},
  pages = {121--142},
  issn = {1863-818X},
  doi = {10.1007/s10182-013-0213-1},
  urldate = {2023-10-14},
  abstract = {Advances in data collection and storage have tremendously increased the presence of functional data, whose graphical representations are curves, images or shapes. As a new area of statistics, functional data analysis extends existing methodologies and theories from the realms of functional analysis, generalized linear model, multivariate data analysis, nonparametric statistics, regression models and many others. From both methodological and practical viewpoints, this paper provides a review of functional principal component analysis, and its use in explanatory analysis, modeling and forecasting, and classification of functional data.},
  langid = {english},
  keywords = {Dimension reduction,Explanatory analysis,Functional data clustering,Functional data forecasting,Functional data modeling},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shang_2014_A_survey_of_functional_principal_component_analysis.pdf}
}

@article{shen2013,
  title = {Monitoring Poisson Count Data with Probability Control Limits When Sample Sizes Are Time Varying},
  author = {Shen, Xiaobei and Zou, Changliang and Jiang, Wei and Tsung, Fugee},
  year = {2013},
  journal = {Naval Research Logistics (NRL)},
  volume = {60},
  number = {8},
  pages = {625--636},
  issn = {1520-6750},
  doi = {10.1002/nav.21557},
  urldate = {2022-01-19},
  abstract = {This article considers the problem of monitoring Poisson count data when sample sizes are time varying without assuming a priori knowledge of sample sizes. Traditional control charts, whose control limits are often determined before the control charts are activated, are constructed based on perfect knowledge of sample sizes. In practice, however, future sample sizes are often unknown. Making an inappropriate assumption of the distribution function could lead to unexpected performance of the control charts, for example, excessive false alarms in the early runs of the control charts, which would in turn hurt an operator's confidence in valid alarms. To overcome this problem, we propose the use of probability control limits, which are determined based on the realization of sample sizes online. The conditional probability that the charting statistic exceeds the control limit at present given that there has not been a single alarm before can be guaranteed to meet a specified false alarm rate. Simulation studies show that our proposed control chart is able to deliver satisfactory run length performance for any time-varying sample sizes. The idea presented in this article can be applied to any effective control charts such as the exponentially weighted moving average or cumulative sum chart. \textcopyright{} 2013 Wiley Periodicals, Inc. Naval Research Logistics 60: 625\textendash 636, 2013},
  langid = {english},
  keywords = {average run length,done,exponentially weighted moving average,false alarm rate,healthcare,run length distribution,statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shen_et_al_2013_Monitoring_poisson_count_data_with_probability_control_limits_when_sample_sizes.pdf;/home/dede/Zotero/storage/XZKUMNA4/nav.html}
}

@article{shen2016,
  title = {Self-{{Starting Monitoring Scheme}} for {{Poisson Count Data With Varying Population Sizes}}},
  author = {Shen, Xiaobei and Tsui, Kwok-Leung and Zou, Changliang and Woodall, William H.},
  year = {2016},
  journal = {Technometrics},
  volume = {58},
  number = {4},
  pages = {460--471},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2015.1075423},
  urldate = {2020-11-19},
  abstract = {In this article, we consider the problem of monitoring Poisson rates when the population sizes are time-varying and the nominal value of the process parameter is unavailable. Almost all previous control schemes for the detection of increases in the Poisson rate in Phase II are constructed based on assumed knowledge of the process parameters, for example, the expectation of the count of a rare event when the process of interest is in control. In practice, however, this parameter is usually unknown and not able to be estimated with a sufficiently large number of reference samples. A self-starting exponentially weighted moving average (EWMA) control scheme based on a parametric bootstrap method is proposed. The success of the proposed method lies in the use of probability control limits, which are determined based on the observations during rather than before monitoring. Simulation studies show that our proposed scheme has good in-control and out-of-control performance under various situations. In particular, our proposed scheme is useful in rare event studies during the start-up stage of a monitoring process. Supplementary materials for this article are available online.},
  keywords = {Average run length,done,Healthcare surveillance,Poisson process,Probability control limits.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shen_et_al_2016_Self-Starting_Monitoring_Scheme_for_Poisson_Count_Data_With_Varying_Population.pdf;/home/dede/Zotero/storage/AX2M2MN3/00401706.2015.html}
}

@book{shewhart1931,
  title = {Economic {{Control}} of {{Quality Of Manufactured Product}}},
  author = {Shewhart, Walter A.},
  year = {1931},
  publisher = {{D. Van Nostrand Company}},
  address = {{New York, NY}},
  abstract = {2015 Reprint of 1931 Edition. Full Facsimile of the original edition. Not reproduced with Optical Recognition Software. The father of modern quality control, Walter A. Shewhart brought together the disciplines of statistics, engineering, and economics in a simple but highly effective tool: the control chart. This technique, and the principles behind it, has played a key role in economic development from the 1940's through to the present day. Most of Shewhart's professional career was spent at Western Electric as an engineer from 1918 to 1924 and at Bell Telephone Laboratories from 1925 until his retirement in 1956. In addition, he served for more than 20 years as the first editor of the Mathematical Statistics Series published by John Wiley \& Sons.},
  langid = {english}
}

@inproceedings{shiryaev1961,
  title = {The Problem of the Most Rapid Detection of a Disturbance in a Stationary Process},
  booktitle = {Soviet Math. {{Dokl}}},
  author = {Shiryaev, Albert N},
  year = {1961},
  volume = {2},
  pages = {103},
  keywords = {todo}
}

@article{shiryaev1963,
  title = {On {{Optimum Methods}} in {{Quickest Detection Problems}}},
  author = {Shiryaev, A. N.},
  year = {1963},
  journal = {Theory of Probability \& Its Applications},
  volume = {8},
  number = {1},
  pages = {22--46},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0040-585X},
  doi = {10.1137/1108002},
  urldate = {2023-11-03},
  abstract = {This paper is an introduction to the thematic issue devoted to the optimal and asymptotically optimal methods of decision making in problems of the quickest detection of changes of probability characteristics of observed processes (the ``disorder" problem), as well as some general problems of the optimal stopping theory on which the decision of these problems is based. This paper's introductory purpose is twofold: on the one hand it gives a general model covering a variate of schemes describing the appearance of disorder, and on the other hand it describes briefly the specific models and general problems concerning the optimal stopping theory which the papers of this issue contain. (The content of this issue, the list of authors, and the selection of papers were completely formed by A. N. Shiryaev\textemdash Ed.)},
  keywords = {todo}
}

@article{shu2008,
  title = {A {{Weighted CUSUM Chart}} for {{Detecting Patterned Mean Shifts}}},
  author = {Shu, Lianjie and Jiang, Wei and Tsui, Kwok-Leung},
  year = {2008},
  journal = {Journal of Quality Technology},
  volume = {40},
  number = {2},
  pages = {194--213},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2008.11917725},
  urldate = {2022-01-18},
  abstract = {Most conventional control charts focus on detecting a constant mean shift. In reality, however, it is often important to deal with situations where the mean of the monitoring sequence has a dynamic shift pattern, e.g., residuals from a time series. In these cases, the conventional control charts may perform poorly, as they do not consider the information due to what is known as ``forecast recovery'' contained in the patterned mean shift. This paper proposes a new weighted cumulative sum (WCUSUM) procedure for monitoring a sequence with a patterned mean shift. This method first estimates the dynamic mean of the sequence, then uses the estimates for weighting the incremental in the conventional CUSUM chart. Guidelines for designing the WCUSUM chart are proposed, and the performance is compared with that of the conventional CUSUM chart and other alternatives for detecting patterned mean shifts. It is found that the WCUSUM chart performs far superior to other charts for detecting small to moderate shifts when forecast recovery is present and performs competitively otherwise.},
  keywords = {Automatic Process Control,Change-Point Detection,done,EWMA,Statistical Process Control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shu_et_al_2008_A_Weighted_CUSUM_Chart_for_Detecting_Patterned_Mean_Shifts.pdf;/home/dede/Zotero/storage/HHJMF4Q3/00224065.2008.html}
}

@article{sidak1967,
  title = {Rectangular {{Confidence Regions}} for the {{Means}} of {{Multivariate Normal Distributions}}},
  author = {{\v S}id{\'a}k, Zbyn{\v e}k},
  year = {1967},
  journal = {Journal of the American Statistical Association},
  volume = {62},
  number = {318},
  eprint = {2283989},
  eprinttype = {jstor},
  pages = {626--633},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2283989},
  urldate = {2023-10-16},
  abstract = {For rectangular confidence regions for the mean values of multivariate normal distributions the following conjecture of O. J. Dunn [3], [4] is proved: Such a confidence region constructed for the case of independent coordinates is, at the same time, a conservative confidence region for any case of dependent coordinates. This result is based on an inequality for the probabilities of rectangles in normal distributions, which permits one to factor out the probability for any single coordinate.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sidak_1967_Rectangular_Confidence_Regions_for_the_Means_of_Multivariate_Normal.pdf}
}

@misc{silva2023,
  title = {{{\pkg{PySpc}}}: Statistical Process Control Charts Library for Humans},
  author = {Silva, Carlos},
  year = {2023},
  urldate = {2023-10-18},
  abstract = {Statistical Process Control Charts Library for Humans},
  copyright = {GPL-3.0},
  keywords = {control-chart,python,spc},
  url = {https://github.com/carlosqsilva/pyspc}
}

@book{sladek2016,
  title = {Coordinate {{Metrology}}},
  author = {S{\l}adek, Jerzy A.},
  year = {2016},
  series = {Springer {{Tracts}} in {{Mechanical Engineering}}},
  publisher = {{Springer-Verlag}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-48465-4},
  urldate = {2023-10-10},
  isbn = {978-3-662-48463-0 978-3-662-48465-4},
  keywords = {Accuracy of Measuring Systems,Calibration and measurement Capabilities CMC,Calibration of Laser Trackers,Contact and Noncontact Measurement Systems,Coordinate Measuring Machine (CMM),Maximum Permissible Errors,Neural Network Models of CMM,Uncertainty of Measurement,Virtual Articulated Arm CMA,Virtual CMM}
}

@article{sood2011,
  title = {Optimization of Process Parameters in Fused Deposition Modeling Using Weighted Principal Component Analysis},
  author = {Sood, Anoop Kumar and Chaturvedi, Vedansh and Datta, Saurav and Mahapatra, Siba Sankar},
  year = {2011},
  journal = {Journal of Advanced Manufacturing Systems},
  volume = {10},
  number = {02},
  pages = {241--259},
  publisher = {{World Scientific Publishing Co.}},
  issn = {0219-6867},
  doi = {10.1142/S0219686711002181},
  urldate = {2023-10-13},
  abstract = {Fused deposition modeling (FDM) is a process by which functional parts can be produced rapidly through deposition of fused layers of material according to a numerically defined cross-sectional geometry. Literature suggests that process parameters largely influence on quality characteristics of rapid prototyping (RP) parts. A functional part is subjected to different loading conditions in actual practice. Therefore, process parameters need to be determined in such a way that they collectively optimize more than one response simultaneously. To address this issue, effect of important process parameters viz., layer thickness, orientation, raster angle, raster width, and air gap have been studied. The responses considered in this study are mechanical property of FDM produced parts such as tensile, bending and impact strength. The multiple responses are converted into a single response using principal component analysis (PCA) so that influence of correlation among the responses can be eliminated. Resulting single response is nothing but the weighted sum of three principal components that explain almost hundred percent of variation. The experiments have been conducted in accordance with Taguchi's orthogonal array to reduce the experimental runs. The results indicate that all the factors such as layer thickness, orientation, raster angle, raster width and air gap and interaction between layer thickness and orientation significantly influence the response. Optimum parameter settings have been identified to simultaneously optimize three responses. The mechanism of failure is explained with the help of SEM micrographs.},
  keywords = {ANOVA,Fused deposition modeling (FDM),signal-to-noise (S/N) ratio,Taguchi method,weighted principal component analysis}
}

@article{sparks2000,
  title = {{{CUSUM Charts}} for {{Signalling Varying Location Shifts}}},
  author = {Sparks, Ross S.},
  year = {2000},
  journal = {Journal of Quality Technology},
  volume = {32},
  number = {2},
  pages = {157--171},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2000.11979987},
  urldate = {2023-03-09},
  abstract = {The conventional cumulative sum (CUSUM) with k = 0.5 is often used as the default CUSUM statistic when future shifts are unknown. In this paper, CUSUM procedures are designed to be efficient at signalling a range of future expected but unknown location shifts. Two approaches are advocated. The first uses three simultaneous conventional CUSUM statistics with different resetting boundaries. This results in a procedure that has, on average, several levels of memory, and thus signals a broader range of location shifts more efficiently than the conventional CUSUM with k = 0.5. The second uses an adaptive CUSUM statistic that continually adjusts its form to be efficient for signalling a one-step-ahead forecast in deviation from its target value. Average run length (ARL) is used to compare the relative performance of procedures. Several applications are used to illustrate procedures.},
  keywords = {Average Run Length,done,Exponentially Weighted Moving Average,Robust Control Charts},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sparks_2000_CUSUM_Charts_for_Signalling_Varying_Location_Shifts.pdf}
}

@article{stankus2019a,
  title = {An {{Improved}} Multivariate Generalised Likelihood Ratio Control Chart for the Monitoring of Point Clouds from {{3D}} Laser Scanners},
  author = {Stankus, Sue E. and {Castillo-Villar}, Krystel K.},
  year = {2019},
  journal = {International Journal of Production Research},
  volume = {57},
  number = {8},
  pages = {2344--2355},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7543},
  doi = {10.1080/00207543.2018.1518600},
  urldate = {2023-10-10},
  abstract = {Statistical quality control techniques are crucial for manufacturing companies with tight tolerances but high-volume data generated from laser scanners has pushed the limits of traditional control charts. In a previous work, multivariate generalised likelihood ratio control (MGLR) chart was used to identify process shifts and locate defects on artefacts by converting 3D point cloud data to a 2D image. This paper presents a 3D MGLR control chart that retains the 3D nature of the point cloud data and uses a Fourier transform of the point errors. The average run length (ARL1) of the proposed 3D MGLR was tested using a designed experiment with ten replications and varying the number of past scans and number of Regions of Interest (ROIs). The designed experiment was repeated using three defects: incorrect surface curvature, surface scratch, and surface dent. The proposed methodology identified the dent while the prior methodology never identified it. In addition, the proposed methodology had a significantly shorter ARL1 than the prior methodology for the scratch and no significant difference in the ARL1 for the incorrect surface curvature. The proposed 3D MGLR control chart enabled the usage of 3D data without needing to convert it to a 2D image.},
  keywords = {3D laser scanners,control charts,non-contact metrology systems,spatiotemporal monitoring,statistical design of experiments}
}

@article{steiner1999,
  title = {{{EWMA Control Charts}} with {{Time-Varying Control Limits}} and {{Fast Initial Response}}},
  author = {Steiner, Stefan H.},
  year = {1999},
  journal = {Journal of Quality Technology},
  volume = {31},
  number = {1},
  pages = {75--86},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.1999.11979899},
  urldate = {2023-10-20},
  abstract = {The control limits of an exponentially weighted moving average (EWMA) control chart should vary with time, approaching asymptotic limits as time increases. However, previous analyses of EWMA charts consider only asymptotic control limits. In this article, the run length properties of EWMA charts with time-varying control limits are approximated using non-homogeneous Markov chains. Comparing the average run lengths (ARL's) of EWMA charts with time-varying control limits and results previously obtained for asymptotic EWMA charts shows that using time-varying control limits is akin to the fast initial response (FIR) feature suggested for cumulative sum charts. The ARL of the EWMA scheme with time-varying limits is substantially more sensitive to early process shifts, especially when the EWMA weight is small. An additional improvement in FIR performance can be achieved by further narrowing the control limits for the first twenty observations. The methodology is illustrated assuming a normal process with known standard deviation where we wish to detect shifts in the mean.},
  keywords = {Average Run Length,Cumulative Sum,Exponentially Weighted Moving Average,Fast Initial Response,Markov Chains}
}

@article{steiner2000,
  title = {Monitoring Surgical Performance Using Risk-Adjusted Cumulative Sum Charts},
  author = {Steiner, Stefan H. and Cook, Richard J. and Farewell, Vern T. and Treasure, Tom},
  year = {2000},
  journal = {Biostatistics},
  volume = {1},
  number = {4},
  pages = {441--452},
  publisher = {{Oxford Academic}},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/1.4.441},
  urldate = {2020-11-18},
  abstract = {Abstract. The cumulative sum (CUSUM) procedure is a graphical method that is widely used for quality monitoring in industrial settings. More recently it has bee},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Steiner_et_al_2000_Monitoring_surgical_performance_using_risk-adjusted_cumulative_sum_charts3.pdf;/home/dede/Zotero/storage/9UTBJ2FL/238348.html}
}

@book{stroustrup2013,
  title = {The \proglang{{{C}}++} {{Programming Language}}},
  author = {Stroustrup, Bjarne},
  year = {2013},
  edition = {4th},
  publisher = {{Addison-Wesley Professional}},
  address = {{Westport, Conn}},
  abstract = {This book features an enhanced, layflat binding, which allows the book to stay open more easily when placed on a flat surface. This special binding method\textemdash noticeable by a small space inside the spine\textemdash also increases durability. C++11 has arrived: thoroughly master it, with the definitive new guide from C++ creator Bjarne Stroustrup, C++ Programming Language, Fourth Edition! The brand-new edition of the worlds most trusted and widely read guide to C++, it has been comprehensively updated for the long-awaited C++11 standard. Extensively rewritten to present the C++11 language, standard library, and key design techniques as an integrated whole, Stroustrup thoroughly addresses changes that make C++11 feel like a whole new language, offering definitive guidance for leveraging its improvements in performance, reliability, and clarity. C++ programmers around the world recognize Bjarne Stoustrup as the go-to expert for the absolutely authoritative and exceptionally useful information they need to write outstanding C++ programs. Now, as C++11 compilers arrive and development organizations migrate to the new standard, they know exactly where to turn once more: Stoustrup C++ Programming Language, Fourth Edition.},
  isbn = {978-0-275-96730-7},
  langid = {english}
}

@article{tenenbaum2000,
  title = {A {{Global Geometric Framework}} for {{Nonlinear Dimensionality Reduction}}},
  author = {Tenenbaum, Joshua B. and de Silva, Vin and Langford, John C.},
  year = {2000},
  journal = {Science},
  volume = {290},
  number = {5500},
  pages = {2319--2323},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.290.5500.2319},
  urldate = {2023-10-10},
  abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs\textemdash 30,000 auditory nerve fibers or 106 optic nerve fibers\textemdash a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.}
}

@misc{themathworksinc.2023,
  title = {{{\proglang{MATLAB}}} Version: 23.2.0 ({{R2023b}})},
  author = {{The Mathworks Inc.}},
  year = {2023},
  address = {{Natick, Massachusetts, United States}},
  howpublished = {The MathWorks Inc.}
}

@incollection{tratt2009,
  title = {Dynamically {{Typed Languages}}},
  booktitle = {Advances in {{Computers}}},
  author = {Tratt, Laurence},
  year = {2009},
  volume = {77},
  pages = {149--184},
  publisher = {{Elsevier}},
  doi = {10.1016/S0065-2458(09)01205-4},
  urldate = {2023-10-18},
  abstract = {Dynamically typed languages such as Python and Ruby have experienced a rapid grown in popularity in recent times. However, there is much confusion as to what makes these languages interesting relative to statically typed languages, and little knowledge of their rich history. In this chapter, I explore the general topic of dynamically typed languages, how they differ from statically typed languages, their history, and their defining features.},
  file = {/home/dede/Zotero/storage/LKSH265I/S0065245809012054.html}
}

@article{vandenboogaart2014,
  title = {Bayes {{Hilbert Spaces}}},
  author = {{van den Boogaart}, Karl Gerald and Egozcue, Juan Jos{\'e} and {Pawlowsky-Glahn}, Vera},
  year = {2014},
  journal = {Australian \& New Zealand Journal of Statistics},
  volume = {56},
  number = {2},
  pages = {171--194},
  issn = {1467-842X},
  doi = {10.1111/anzs.12074},
  urldate = {2023-10-14},
  abstract = {A Bayes linear space is a linear space of equivalence classes of proportional {$\sigma$}-finite measures, including probability measures. Measures are identified with their density functions. Addition is given by Bayes' rule and substraction by Radon\textendash Nikodym derivatives. The present contribution shows the subspace of square-log-integrable densities to be a Hilbert space, which can include probability and infinite measures, measures on the whole real line or discrete measures. It extends the ideas from the Hilbert space of densities on a finite support towards Hilbert spaces on general measure spaces. It is also a generalisation of the Euclidean structure of the simplex, the sample space of random compositions. In this framework, basic notions of mathematical statistics get a simple algebraic interpretation. A key tool is the centred-log-ratio transformation, a generalization of that used in compositional data analysis, which maps the Hilbert space of measures into a subspace of square-integrable functions. As a consequence of this structure, distances between densities, orthonormal bases, and Fourier series representing measures become available. As an application, Fourier series of normal distributions and distances between them are derived, and an example related to grain size distributions is presented. The geometry of the sample space of random compositions, known as Aitchison geometry of the simplex, is obtained as a particular case of the Hilbert space when the measures have discrete and finite support.},
  copyright = {\textcopyright{} 2014 Australian Statistical Publishing Association Inc. Published by Wiley Publishing Asia Pty Ltd.},
  langid = {english},
  keywords = {Aitchison geometry of the simplex,distance between measures,Fourier coefficients,infinite measures,normal distribution,perturbation,probability measures.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/van_den_Boogaart_et_al_2014_Bayes_Hilbert_Spaces.pdf;/home/dede/Zotero/storage/IWTDK2UM/anzs.html}
}

@book{vanrossum1995,
  title = {\proglang{Python} Reference Manual},
  author = {Van Rossum, Guido and Drake Jr, Fred L},
  year = {1995},
  publisher = {{Centrum voor Wiskunde en Informatica Amsterdam}}
}

@incollection{vitale1975,
  title = {A {{Bernstein Polynomial Approach}} to {{Density Function Estimation}}},
  booktitle = {Statistical {{Inference}} and {{Related Topics}}},
  author = {Vitale, Richard A.},
  editor = {Puri, Madan Lal},
  year = {1975},
  pages = {87--99},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-568002-8.50011-2},
  urldate = {2023-10-10},
  abstract = {This chapter presents a Bernstein polynomial approach to density function estimation and describes the Bernstein polynomial estimate of f(x). The form is one of a linear combination of beta densities with random coefficients. The order of convergence of the Bernstein polynomial estimate is comparable to that of other methods. The estimate chapter has a significant practical advantage as it is well adapted to computation.},
  isbn = {978-0-12-568002-8},
  file = {/home/dede/Zotero/storage/8T89VCRQ/B9780125680028500112.html}
}

@article{waldmann1986,
  title = {Bounds for the {{Distribution}} of the {{Run Length}} of {{Geometric Moving Average Charts}}},
  author = {Waldmann, K.-H.},
  year = {1986},
  journal = {Journal of the Royal Statistical Society C (Applied Statistics)},
  volume = {35},
  number = {2},
  eprint = {2347265},
  eprinttype = {jstor},
  pages = {151--158},
  publisher = {{[John Wiley \& Sons, Royal Statistical Society]}},
  issn = {0035-9254},
  doi = {10.2307/2347265},
  urldate = {2023-09-13},
  abstract = {Upper and lower bounds are derived for the distribution of the run length N of one-sided and two-sided geometric moving average charts. By considering the iterates {$<$}latex{$>\$$}P(N {$>$} 0), P(N {$>$} 1),\textbackslash ldots,\${$<$}/latex{$>$} it is shown that {$<$}latex{$>\$$}(m\^-\_n)\^iP(N {$>$} n) \textbackslash leqslant P(N {$>$} n + i) \textbackslash leqslant (m\^+\_n)\^iP(N {$>$} n)\${$<$}/latex{$>$} for each n and all i = 1, 2,... with constants 0 {$\leqslant$} m\textsuperscript{-}\textsubscript{n} {$\leqslant$} m\textsuperscript{+}\textsubscript{n} {$\leqslant$} 1 suitably chosen. The bounds converge monotonically and, under some mild and natural assumptions, m\textsuperscript{-}\textsubscript{n} and m\textsuperscript{+}\textsubscript{n} have the same positive limit as n \textrightarrow{} {$\infty$}. Bounds are also presented for the percentage points of the distribution function of N, for the first two moments of N, and for the probability mass function of N. Some numerical results are displayed to demonstrate the efficiency of the method.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Waldmann_1986_Bounds_for_the_Distribution_of_the_Run_Length_of_Geometric_Moving_Average_Charts.pdf}
}

@article{waldmann1986a,
  title = {Bounds for the {{Distribution}} of the {{Run Length}} of {{One-Sided}} and {{Two-Sided CUSUM Quality Control Schemes}}},
  author = {Waldmann, K.-H.},
  year = {1986},
  journal = {Technometrics},
  volume = {28},
  number = {1},
  eprint = {1269604},
  eprinttype = {jstor},
  pages = {61--67},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1269604},
  urldate = {2023-09-13},
  abstract = {Upper and lower bounds are derived for the distribution of the run length N of both the one-sided and two-sided CUSUM schemes. Based upon a sequence of iterates Pr(N {$>$} 0), Pr(N {$>$} 1),..., bounds are constructed such that {$<$}latex{$>\$$}(m\_\{n\}\^\{-\})\^\{i\}\{\textbackslash rm Pr\}(N{$>$}n)\textbackslash leq \{\textbackslash rm Pr\}(N{$>$}n+i)\textbackslash leq (m\_\{n\}\^\{+\})\^\{i\}\{\textbackslash rm Pr\}(N{$>$}n)\${$<$}/latex{$>$} holds for all n, i = 1, 2,..., with constants 0{$\leq$} m\textsubscript{n}\textsuperscript{-}{$\leq$} m\textsubscript{n}\textsuperscript{+}{$\leq$} 1 suitably chosen. The bounds converge monotonically in the sense that {$<$}latex{$>\$$}(m\_\{n\}\^\{-\})\^\{i+1\}\{\textbackslash rm Pr\}(N{$>$}n)\textbackslash leq (m\_\{n+1\}\^\{-\})\^\{i\}\${$<$}/latex{$>$} {$<$}latex{$>\$\lbrace\backslash$}rm Pr\vphantom\{\}(N{$>$}n+1),(m\_\{n\}\^\{+\})\^\{i+1\}\{\textbackslash rm Pr\}(N{$>$}n)\textbackslash geq (m\_\{n+1\}\^\{+\})\^\{i\}\{\textbackslash rm Pr\}(N{$>$}n+1)\${$<$}/latex{$>$}, and, under some mild and natural conditions, {$<$}latex{$>\$\lbrace\backslash$}rm lim\vphantom\{\}\_\{n\textbackslash rightarrow \textbackslash infty \}m\_\{n\}\^\{-\}=\{\textbackslash rm lim\}\_\{n\textbackslash rightarrow \textbackslash infty \}m\_\{n\}\^\{+\}{$>$}0\${$<$}/latex{$>$}. As a by-product, bounds are presented for the percentage points of the distribution function of N, for the average run length of the inspection scheme, for the standard deviation of N, and finally, for the probability mass function of N. Some numerical results are displayed to demonstrate the efficiency of the method.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Waldmann_1986_Bounds_for_the_Distribution_of_the_Run_Length_of_One-Sided_and_Two-Sided_CUSUM.pdf}
}

@article{wang2014b,
  title = {Statistical {{Surface Monitoring}} by {{Spatial-Structure Modeling}}},
  author = {Wang, Andi and Wang, Kaibo and Tsung, Fugee},
  year = {2014},
  journal = {Journal of Quality Technology},
  volume = {46},
  number = {4},
  pages = {359--376},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2014.11917977},
  urldate = {2023-10-10},
  abstract = {In some manufacturing processes, the quality characteristic is represented by a two-dimensional (2-D) surface. Surface data can generally be treated as a special profile with one response variable and two explanatory variables, for which spatial correlations are commonly observed. Existing parametric charts for profile monitoring are unable to adequately describe the spatial correlations among variables in 2-D surface data, and nonparametric charts cannot be applied to a 2-D data structure directly. In this study, we propose a new chart based on the Gaussian-Kriging model, in which the spatial correlations within the 2-D surface profile are represented by a parametric function. We construct a parametric model that considers three components of the surface\textemdash the global trend, the spatial correlations, and independent errors. Then we monitor the process by detecting changes in the estimated parameters. We utilize this method to monitor a wafer-manufacturing process and compare its performance with that of an existing profile-monitoring method through simulation.},
  keywords = {Gaussian-Kriging Model,Profile Monitoring,Spatial Correlation,Statistical Process Control,Surface Data}
}

@article{wang2017,
  title = {Multivariate {{Ordinal Categorical Process Control Based}} on {{Log-Linear Modeling}}},
  author = {Wang, Junjie and Li, Jian and Su, Qin},
  year = {2017},
  journal = {Journal of Quality Technology},
  volume = {49},
  number = {2},
  pages = {108--122},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2017.11917983},
  urldate = {2022-09-12},
  abstract = {In many applications, the quality of products or services tends to be measured by multiple categorical characteristics, each of which is classified into attribute levels such as good, marginal, and bad. Here there is usually natural order among these attribute levels. However, traditional monitoring techniques ignore such order among them. By assuming that each ordinal categorical quality characteristic is determined by a latent continuous variable, this paper incorporates the ordinal information into an extended log-linear model and proposes a multivariate ordinal categorical control chart based on a generalized likelihood-ratio test. The proposed chart is efficient in detecting location shifts and dependence shifts in the corresponding latent continuous variables of ordinal categorical characteristics based on merely the attribute-level counts of the ordinal characteristics.},
  keywords = {Contingency Table,Dependence Shift,done,Latent Variable,Location Shift,Statistical Process Control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wang_et_al_2017_Multivariate_Ordinal_Categorical_Process_Control_Based_on_Log-Linear_Modeling.pdf}
}

@article{wang2018,
  title = {A {{Spatial-Adaptive Sampling Procedure}} for {{Online Monitoring}} of {{Big Data Streams}}},
  author = {Wang, Andi and Xian, Xiaochen and Tsung, Fugee and Liu, Kaibo},
  year = {2018},
  journal = {Journal of Quality Technology},
  volume = {50},
  number = {4},
  pages = {329--343},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2018.1507560},
  urldate = {2021-10-20},
  abstract = {With the improvement of data-acquisition technology, big data streams that involve continuous observations with high dimensionality and large volume frequently appear in modern applications, which poses significant challenges for statistical process control. In this article we consider the problem of online monitoring a class of big data streams where each data stream is associated with a spatial location. Our goal is to quickly detect shifts occurring in such big data streams when only partial information can be observed at each time and the out-of-control variables are clustered in a small and unknown region. To achieve this goal, we propose a novel spatial-adaptive sampling and monitoring (SASAM) procedure that aims to leverage the spatial information of the data streams for quick change detection. Specifically, the proposed sampling strategy will adaptively and intelligently integrate two seemingly contradictory ideas: (1) random sampling that quickly searches for possible out-of-control variables; and (2) directional sampling that focuses on highly suspicious out-of-control variables that may cluster in a small region. Simulation and real case studies show that the proposed method significantly outperforms the existing sampling strategy without taking the spatial information of the data streams into consideration.},
  keywords = {big data streams,cumulative-sum statistics,done,high-dimensional and high-frequency data,partial information,scalable monitoring schemes,statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wang_et_al_2018_A_spatial-adaptive_sampling_procedure_for_online_monitoring_of_big_data_streams.pdf;/home/dede/Zotero/storage/Q7TV4IQ9/00224065.2018.html}
}

@article{wells2013a,
  title = {Statistical Process Monitoring Approach for High-Density Point Clouds},
  author = {Wells, Lee J. and Megahed, Fadel M. and Niziolek, Cory B. and Camelio, Jaime A. and Woodall, William H.},
  year = {2013},
  journal = {Journal of Intelligent Manufacturing},
  volume = {24},
  number = {6},
  pages = {1267--1279},
  issn = {1572-8145},
  doi = {10.1007/s10845-012-0665-2},
  urldate = {2023-10-10},
  abstract = {Statistical process control (SPC) methods have been extensively applied to monitor the quality performance of manufacturing processes to quickly detect and correct out-of-control conditions. As sensor and measurement technologies advance, there is a continual need to adapt and refine SPC methods to effectively and efficiently use these new data-sets. One of the most state-of-the-art dimensional measurement technologies currently being implemented in industry is the 3D laser scanner, which rapidly provides millions of data points to represent an entire manufactured part's surface. Consequently, this data has a great potential to detect unexpected faults, i.e., faults that are not captured by measuring a small number of predefined dimensions. However, in order for this potential to be realized, SPC methods capable of handling these large data-sets need to be developed. This paper presents an approach to performing SPC using point clouds obtained through a 3D laser scanner. The proposed approach transforms high-dimensional point clouds into linear profiles through the use of Q\textendash Q plots, which can be monitored by well established profile monitoring techniques. In this paper point clouds are simulated to determine the performance of the proposed approach under varying fault scenarios. In addition, experimental studies were performed to determine the effectiveness of the proposed approach using actual point cloud data. The results of these experiments show that the proposed approach can significantly improve the monitoring capabilities for manufacturing parts that are characterized by complex surface geometries.},
  langid = {english},
  keywords = {Coordinate measuring machines,High-density data,Noncontact scanning systems,Phase II control charts,Profile monitoring,Statistical quality control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wells_et_al_2013_Statistical_process_monitoring_approach_for_high-density_point_clouds2.pdf}
}

@article{wiel1996,
  title = {Monitoring {{Processes That Wander Using Integrated Moving Average Models}}},
  author = {Wiel, S. A. Vander},
  year = {1996},
  journal = {Technometrics},
  volume = {38},
  number = {2},
  eprint = {1270407},
  eprinttype = {jstor},
  pages = {139--151},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1270407},
  urldate = {2023-10-21},
  abstract = {Often the least appropriate assumption in traditional control-charting technology is that process data constitute a random sample. In reality most process data are correlated-either temporally, spatially, or due to nested sources of variation. One approach to monitoring temporally correlated data uses a control chart on the forecast errors from a time series model of the process with, possibly, a transfer-function term to model compensatory adjustments. If the time series term is an integrated moving average, then a sudden level shift in the process results in a patterned shift in the mean of forecast errors. Initially the mean shifts by the same amount as the process level, but then it decays geometrically back to 0 corresponding to the ability of the forecast to "recover" from the upset. We study four monitoring schemes-cumulative sums (CUSUM's), exponentially weighted moving averages, Shewhart individuals charts, and a likelihood ratio scheme. Comparisons of signaling probabilities and average run lengths show that CUSUM's can be designed to perform at least as well as, and often better than, any of the other schemes. Shewhart individuals charts often perform much worse than the others. Graphical aids are provided for designing CUSUM's in this context.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wiel_1996_Monitoring_Processes_That_Wander_Using_Integrated_Moving_Average_Models.pdf}
}

@article{woodall2000,
  title = {Controversies and {{Contradictions}} in {{Statistical Process Control}}},
  author = {Woodall, William H.},
  year = {2000},
  journal = {Journal of Quality Technology},
  volume = {32},
  number = {4},
  pages = {341--350},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2000.11980013},
  urldate = {2022-01-15},
  abstract = {Statistical process control (SPC) methods are widely used to monitor and improve manufacturing processes and service operations. Disputes over the theory and application of these methods are frequent and often very intense. Some of the controversies and issues discussed are the relationship between hypothesis testing and control charting, the role of theory and the modeling of control chart performance, the relative merits of competing methods, the relevance of research on SPC and even the relevance of SPC itself. One purpose of the paper is to offer a resolution of some of these disagreements in order to improve the communication between practitioners and researchers.},
  keywords = {Average Run Length,Control Charts,Cumulative Sum Control Charts,done,Exponentially Weighted Moving Average Control Charts},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Woodall_2000_Controversies_and_Contradictions_in_Statistical_Process_Control.pdf}
}

@article{woodall2004,
  title = {Using {{Control Charts}} to {{Monitor Process}} and {{Product Quality Profiles}}},
  author = {Woodall, William H. and Spitzner, Dan J. and Montgomery, Douglas C. and Gupta, Shilpa},
  year = {2004},
  journal = {Journal of Quality Technology},
  volume = {36},
  number = {3},
  pages = {309--320},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2004.11980276},
  urldate = {2023-10-13},
  abstract = {In most statistical process control (SPC) applications, it is assumed that the quality of a process or product can be adequately represented by the distribution of a univariate quality characteristic or by the general multivariate distribution of a vector consisting of several correlated quality characteristics. In many practical situations, however, the quality of a process or product is better characterized and summarized by a relationship between a response variable and one or more explanatory variables. Thus, at each sampling stage, one observes a collection of data points that can be represented by a curve (or profile). In some calibration applications, the profile can be represented adequately by a simple straight-line model, while in other applications, more complicated models are needed. In this expository paper, we discuss some of the general issues involved in using control charts to monitor such process- and product-quality profiles and review the SPC literature on the topic. We relate this application to functional data analysis and review applications involving linear profiles, nonlinear profiles, and the use of splines and wavelets. We strongly encourage research in profile monitoring and provide some research ideas.},
  keywords = {Calibration,Linear Regression,Multivariate Quality Control,Nonlinear Regression,Splines,Statistical Process Control,Wavelets},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Woodall_et_al_2004_Using_Control_Charts_to_Monitor_Process_and_Product_Quality_Profiles.pdf}
}

@article{woodall2006,
  title = {The {{Use}} of {{Control Charts}} in {{Health-Care}} and {{Public-Health Surveillance}}},
  author = {Woodall, William H.},
  year = {2006},
  journal = {Journal of Quality Technology},
  volume = {38},
  number = {2},
  pages = {89--104},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2006.11918593},
  urldate = {2023-06-27},
  abstract = {There are many applications of control charts in health-care monitoring and in public-health surveillance. We introduce these applications to industrial practitioners and discuss some of the ideas that arise that may be applicable in industrial monitoring. The advantages and disadvantages of the charting methods proposed in the health-care and public-health areas are considered. Some additional contributions in the industrial statistical process control literature relevant to this area are given. There are many application and research opportunities available in the use of control charts for health-related monitoring.},
  keywords = {Cluster detection,Cumulative sum chart,CUSUM,Exponentially weighted moving average,Risk-adjustment,Sequential probability ratio test,Sets method,SPC,Statistical process control,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Woodall_2006_The_Use_of_Control_Charts_in_Health-Care_and_Public-Health_Surveillance.pdf}
}

@article{wuest2014,
  title = {An Approach to Monitoring Quality in Manufacturing Using Supervised Machine Learning on Product State Data},
  author = {Wuest, Thorsten and Irgens, Christopher and Thoben, Klaus-Dieter},
  year = {2014},
  journal = {Journal of Intelligent Manufacturing},
  volume = {25},
  number = {5},
  pages = {1167--1180},
  issn = {1572-8145},
  doi = {10.1007/s10845-013-0761-y},
  urldate = {2023-10-13},
  abstract = {Increasing market demand towards higher product and process quality and efficiency forces companies to think of new and innovative ways to optimize their production. In the area of high-tech manufacturing products, even slight variations of the product state during production can lead to costly and time-consuming rework or even scrapage. Describing an individual product's state along the entire manufacturing programme, including all relevant information involved for utilization, e.g., in-process adjustments of process parameters, can be one way to meet the quality requirements and stay competitive. Ideally, the gathered information can be directly analyzed and in case of an identified critical trend or event, adequate action, such as an alarm, can be triggered. Traditional methods based on modelling of cause-effect relations reaches its limits due to the fast increasing complexity and high-dimensionality of modern manufacturing programmes. There is a need for new approaches that are able to cope with this complexity and high-dimensionality which, at the same time, are able to generate applicable results with reasonable effort. Within this paper, the possibility to generate such a system by applying a combination of Cluster Analysis and Supervised Machine Learning on product state data along the manufacturing programme will be presented. After elaborating on the different key aspects of the approach, the applicability on the identified problem in industrial environment will be discussed briefly.},
  langid = {english},
  keywords = {Cluster analysis,Data,Manufacturing,Process,Product state,Quality monitoring,Supervised machine learning},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wuest_et_al_2014_An_approach_to_monitoring_quality_in_manufacturing_using_supervised_machine.pdf}
}

@article{xian2018,
  title = {A {{Nonparametric Adaptive Sampling Strategy}} for {{Online Monitoring}} of {{Big Data Streams}}},
  author = {Xian, Xiaochen and Wang, Andi and Liu, Kaibo},
  year = {2018},
  journal = {Technometrics},
  volume = {60},
  number = {1},
  pages = {14--25},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2017.1317291},
  urldate = {2021-10-20},
  abstract = {With the rapid advancement of sensor technology, a huge amount of data is generated in various applications, which poses new and unique challenges for statistical process control (SPC). In this article, we propose a nonparametric adaptive sampling (NAS) strategy to online monitor nonnormal big data streams in the context of limited resources, where only a subset of observations are available at each acquisition time. In particular, this proposed method integrates a rank-based CUSUM scheme and an innovative idea that corrects the anti-rank statistics with partial observations, which can effectively detect a wide range of possible mean shifts when data streams are exchangeable and follow arbitrary distributions. Two theoretical properties on the sampling layout of the proposed NAS algorithm are investigated when the process is in control and out of control. Both simulations and case studies are conducted under different scenarios to illustrate and evaluate the performance of the proposed method. Supplementary materials for this article are available online.},
  keywords = {Distribution-free,done,Multivariate CUSUM procedure,Partial observations,Process change detection,Statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Xian_et_al_2018_A_Nonparametric_Adaptive_Sampling_Strategy_for_Online_Monitoring_of_Big_Data.pdf;/home/dede/Zotero/storage/EG4APGK8/00401706.2017.html}
}

@article{xian2019,
  title = {Online Monitoring of Big Data Streams: {{A}} Rank-Based Sampling Algorithm by Data Augmentation},
  shorttitle = {Online Monitoring of Big Data Streams},
  author = {Xian, Xiaochen and Zhang, Chen and Bonk, Scott and Liu, Kaibo},
  year = {2019},
  journal = {Journal of Quality Technology},
  volume = {53},
  number = {2},
  pages = {135--153},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2019.1681924},
  urldate = {2022-09-29},
  abstract = {In many applications of modern quality control, process monitoring involves a large number of process variables and quality characteristics. Practitioners are desired to attain complete information about the process in order to assure quick detection of shifts that may possibly occur at any variable. However, full information is not always available during online monitoring of big data streams due to limitations of monitoring resources in practice. In this paper, a rank-based monitoring and sampling algorithm based on data augmentation is proposed to quickly detect the mean shifts in a process when only a limited portion of observations are available online. Specifically, at each observation time, the proposed method will automatically augment information for unobservable variables based on the online observations, and then intelligently allocate the monitoring resources to the most suspicious data streams. Comparing to the existing literature, this method is able to accurately infer the status of all variables in a process based on a small number of observable variables and effectively construct a global monitoring statistic with the proposed augmented vector, which leads to a quick detection of the out-of-control status even if limited shifted variables are observed in real time. Simulation studies as well as a real case study on real-time solar flare detection are conducted to demonstrate the efficacy and applicability of the proposed method.},
  keywords = {cumulative sum (CUSUM),data augmentation,done,partial observations,rank-based monitoring,statistical process control (SPC)},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Xian_et_al_2021_Online_monitoring_of_big_data_streams.pdf}
}

@article{xie2022,
  title = {Robust {{Monitoring}} of {{Multivariate Processes With Short-Ranged Serial Data Correlation}}},
  author = {Xie, Xiulin and Qiu, Peihua},
  year = {2022},
  journal = {Quality and Reliability Engineering International},
  volume = {38},
  number = {8},
  pages = {4196--4209},
  issn = {1099-1638},
  doi = {10.1002/qre.3199},
  urldate = {2023-01-13},
  abstract = {Control charts are commonly used in practice for detecting distributional shifts of sequential processes. Traditional statistical process control (SPC) charts are based on the assumptions that process observations are independent and identically distributed and follow a parametric distribution when the process is in-control (IC). In practice, these assumptions are rarely valid, and it has been well demonstrated that these traditional control charts are unreliable to use when their model assumptions are invalid. To overcome this limitation, nonparametric SPC has become an active research area, and some nonparametric control charts have been developed. But, most existing nonparametric control charts are based on data ordering and/or data categorization of the original process observations, which would result in information loss in the observed data and consequently reduce the effectiveness of the related control charts. In this paper, we suggest a new multivariate online monitoring scheme, in which process observations are first sequentially decorrelated, the decorrelated data of each quality variable are then transformed using their estimated IC distribution so that the IC distribution of the transformed data would be roughly N(0, 1), and finally the conventional multivariate exponentially weighted moving average (MEWMA) chart is applied to the transformed data of all quality variables for online process monitoring. This chart is self-starting in the sense that estimates of all related IC quantities are updated recursively over time. It can well accommodate stationary short-range serial data correlation, and its design is relatively simple since its control limit can be determined in advance by a Monte Carlo simulation. Because information loss due to data ordering and/or data categorization is avoided in this approach, numerical studies show that it is reliable to use and effective for process monitoring in various cases considered.},
  langid = {english},
  keywords = {data decorrelation,doing,normalization,recursive computation,self-starting charts,sequential learning,transformation},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Xie_Qiu_2022_Robust_monitoring_of_multivariate_processes_with_short-ranged_serial_data.pdf;/home/dede/Zotero/storage/A4FL4KAK/qre.html}
}

@article{xie2023,
  title = {A {{General Framework}} for {{Robust Monitoring}} of {{Multivariate Correlated Processes}}},
  author = {Xie, Xiulin and Qiu, Peihua},
  year = {2023},
  journal = {Technometrics},
  pages = {1--22},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2023.2224411},
  urldate = {2023-06-20},
  abstract = {Statistical process control (SPC) charts provide an important analytic tool for online monitoring of sequential processes. Conventional SPC charts are designed for cases when in-control (IC) process observations are independent and identically distributed at different observation times and the IC process distribution belongs to a parametric (e.g., normal) family. In practice, however, these model assumptions are rarely valid. To address this issue, there have been some existing discussions in the SPC literature for handling cases when the IC process distribution cannot be described well by a parametric form, and some nonparametric SPC charts have been developed based on data ranking and/or data categorization. However, both data ranking and data categorization would lose information in the original process observations. Consequently, the effectiveness of the nonparametric SPC charts would be compromised. In this paper, we make another research effort to handle this problem by developing a general process monitoring framework that is robust to the IC process distribution and short-ranged serial correlation. The new method tries to preserve as much information in the original process observations as possible. Instead of using data ranking and/or data categorization, it is based on intensive data pre-processing, including data decorrelation, data transformation, and data integration. Because the distribution of the pre-processed data can be approximated well by a parametric distribution, the design and implementation of the new method is relatively simple. Numerical studies show that it is indeed robust to the IC process distribution and effective for online monitoring of multivariate processes with short-ranged serial correlation.},
  keywords = {Data decorrelation,doing,Recursive computation,Robustness,Self-starting charts,Sequential learning,toRead,Transformation},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Xie_Qiu_2023_A_General_Framework_for_Robust_Monitoring_of_Multivariate_Correlated_Processes.pdf}
}

@article{xie2023a,
  title = {Control {{Charts}} for {{Dynamic Process Monitoring}} with an {{Application}} to {{Air Pollution Surveillance}}},
  author = {Xie, Xiulin and Qiu, Peihua},
  year = {2023},
  journal = {The Annals of Applied Statistics},
  volume = {17},
  number = {1},
  pages = {47--66},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/22-AOAS1615},
  urldate = {2023-09-13},
  abstract = {Air pollution is a major global public health risk factor. Among all air pollutants, PM2.5 is especially harmful. It has been well demonstrated that chronic exposure to PM2.5 can cause many health problems, including asthma, lung cancer and cardiovascular diseases. To tackle problems caused by air pollution, governments have put a huge amount of resources to improve air quality and reduce the impact of air pollution on public health. In this effort it is extremely important to develop an air pollution surveillance system to constantly monitor the air quality over time and to give a signal promptly once the air quality is found to deteriorate so that a timely government intervention can be implemented. To monitor a sequential process, a major statistical tool is the statistical process control (SPC) chart. However, traditional SPC charts are based on the assumptions that process observations at different time points are independent and identically distributed. These assumptions are rarely valid in environmental data because seasonality and serial correlation are common in such data. To overcome this difficulty, we suggest a new control chart in this paper, which can properly accommodate dynamic temporal pattern and serial correlation in a sequential process. Thus, it can be used for effective air pollution surveillance. This method is demonstrated by an application to monitor the daily average PM2.5 levels in Beijing and shown to be effective and reliable in detecting the increase of PM2.5 levels.},
  keywords = {Data correlation,done,dynamic process monitoring,environmental protection,pollution surveillance,sequential learning,statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Xie_Qiu_2023_Control_charts_for_dynamic_process_monitoring_with_an_application_to_air.pdf}
}

@article{xue2021,
  title = {A {{Nonparametric CUSUM Chart}} for {{Monitoring Multivariate Serially Correlated Processes}}},
  author = {Xue, Li and Qiu, Peihua},
  year = {2021},
  journal = {Journal of Quality Technology},
  volume = {53},
  number = {4},
  pages = {396--409},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2020.1778430},
  urldate = {2021-11-03},
  abstract = {In applications, most processes for quality control and management are multivariate. Thus, multivariate statistical process control (MSPC) is an important research problem and has been discussed extensively in the literature. Early MSPC research is based on the assumptions that process observations at different time points are independent and they have a parametric distribution (e.g., Gaussian) when the process is in-control (IC). Recent MSPC research has lifted the ``parametric distribution'' assumption, and some nonparametric MSPC charts have been developed. These nonparametric MSPC charts, however, often requires the ``independent process observations'' assumption, which is rarely valid in practice because serial data correlation is common in a time series data. In the literature, it has been well demonstrated that a control chart who ignores serial data correlation would be unreliable to use when such data correlation exists. So far, we have not found any existing nonparametric MSPC charts that can accommodate serial data correlation properly. In this paper, we suggest a flexible nonparametric MSPC chart which can accommodate stationary serial data correlation properly. Numerical studies show that it performs well in different cases.},
  keywords = {data correlation,decorrelation,done,moment estimation,nonparametric charts,stationary data correlation,statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Xue_Qiu_2021_A_nonparametric_CUSUM_chart_for_monitoring_multivariate_serially_correlated.pdf;/home/dede/Zotero/storage/GW8B27P2/00224065.2020.html}
}

@article{yan2014,
  title = {Image-Based Process Monitoring Using Low-Rank Tensor Decomposition},
  author = {Yan, Hao and Paynabar, Kamran and Shi, Jianjun},
  year = {2014},
  journal = {IEEE Transactions on Automation Science and Engineering},
  volume = {12},
  number = {1},
  pages = {216--227},
  publisher = {{IEEE}},
  urldate = {2023-10-18},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yan_et_al_2014_Image-based_process_monitoring_using_low-rank_tensor_decomposition.pdf}
}

@article{yan2022,
  title = {Real-Time Detection of Clustered Events in Video-Imaging Data with Applications to Additive Manufacturing},
  author = {Yan, Hao and Grasso, Marco and Paynabar, Kamran and Colosimo, Bianca Maria},
  year = {2022},
  journal = {IISE Transactions},
  volume = {54},
  number = {5},
  pages = {464--480},
  publisher = {{Taylor \& Francis}},
  issn = {2472-5854},
  doi = {10.1080/24725854.2021.1882013},
  urldate = {2022-09-21},
  abstract = {The use of video-imaging data for in-line process monitoring applications has become popular in industry. In this framework, spatio-temporal statistical process monitoring methods are needed to capture the relevant information content and signal possible out-of-control states. Video-imaging data are characterized by a spatio-temporal variability structure that depends on the underlying phenomenon, and typical out-of-control patterns are related to events that are localized both in time and space. In this article, we propose an integrated spatio-temporal decomposition and regression approach for anomaly detection in video-imaging data. Out-of-control events are typically sparse, spatially clustered and temporally consistent. The goal is not only to detect the anomaly as quickly as possible (``when'') but also to locate it in space (``where''). The proposed approach works by decomposing the original spatio-temporal data into random natural events, sparse spatially clustered and temporally consistent anomalous events, and random noise. Recursive estimation procedures for spatio-temporal regression are presented to enable the real-time implementation of the proposed methodology. Finally, a likelihood ratio test procedure is proposed to detect when and where the anomaly happens. The proposed approach was applied to the analysis of high-sped video-imaging data to detect and locate local hot-spots during a metal additive manufacturing process.},
  keywords = {in-situ defect detection,laser powder bed fusion,metal additive manufacturing,Spatio-temporal regression,statistical process monitoring,todo,video-imaging data},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yan_et_al_2022_Real-time_detection_of_clustered_events_in_video-imaging_data_with_applications.pdf}
}

@article{yang2017,
  title = {Nonparametric {{Profile Monitoring}} Using {{Dynamic Probability Control Limits}}},
  author = {Yang, Wenwan and Zou, Changliang and Wang, Zhaojun},
  year = {2017},
  journal = {Quality and Reliability Engineering International},
  volume = {33},
  number = {5},
  pages = {1131--1142},
  issn = {1099-1638},
  doi = {10.1002/qre.2104},
  urldate = {2022-01-03},
  abstract = {This article focuses on monitoring nonparametric profile with time-varying sample sizes and random predictors. Traditional profile monitoring schemes, whose control limits are often determined before the monitoring initiates, are constructed based on perfect knowledge of profile sample sizes and predictors. In practice, however, our foreknowledge about future random sample sizes and predictors is seldom available. An inappropriate assumption or estimation of the sample sizes model and/or predictors distribution function may lead to unexpected performance of traditional control charts. To overcome this problem, we propose a kernel-based nonparametric profile monitoring scheme which integrates the multivariate exponentially weighted moving average procedure with the probability control limits. The success of the proposed chart lies in the use of dynamic control limits which are determined online, essentially aiming at guaranteeing the conditional probability that the charting statistic exceeds the control limit at present given that there is no alarm before the current time point to meet a pre-specified false alarm rate. The simulation studies show that the proposed control scheme has good in-control and out-of-control performances under various scenarios of time-varying sample sizes and random predictors. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {control charts,kernel estimation,multivariate statistics,profile monitoring,quality control,skimmed},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yang_et_al_2017_Nonparametric_Profile_Monitoring_using_Dynamic_Probability_Control_Limits.pdf;/home/dede/Zotero/storage/YVSVU2AZ/qre.html}
}

@article{yang2020,
  title = {Online Sequential Monitoring of Spatio-Temporal Disease Incidence Rates},
  author = {Yang, Kai and Qiu, Peihua},
  year = {2020},
  journal = {IISE Transactions},
  volume = {52},
  number = {11},
  pages = {1218--1233},
  publisher = {{Taylor \& Francis}},
  issn = {2472-5854},
  doi = {10.1080/24725854.2019.1696496},
  urldate = {2022-09-11},
  abstract = {Online sequential monitoring of the incidence rates of chronic or infectious diseases is critically important for public health. Governments have invested a great amount of money in building global, national and regional disease reporting and surveillance systems. In these systems, conventional control charts, such as the cumulative sum (CUSUM) and the exponentially weighted moving average (EWMA) charts, are usually included for disease surveillance purposes. However, these charts require many assumptions on the observed data, including the ones that the observed data should be independent at different places and/or times, and they should follow a parametric distribution when no disease outbreaks are present. These assumptions are rarely valid in practice, making the results from the conventional control charts unreliable. Motivated by an application to monitor the Florida influenza-like illness data, we develop a new sequential monitoring approach in this article, which can accommodate the dynamic nature of the observed disease incidence rates (i.e., the distribution of the observed disease incidence rates can change over time due to seasonality and other reasons), spatio-temporal data correlation, and arbitrary data distribution. It is shown that the new method is more reliable to use in practice than the commonly used conventional charts for sequential monitoring of disease incidence rates. Because of its generality, the proposed method should be useful for many other applications as well, including spatio-temporal monitoring of the air quality in a region or the sea-level pressure data collected in a region of an ocean.},
  keywords = {Change detection,correlation,disease surveillance,done,dynamic systems,early detection,nonparametric methods,process control,sequential monitoring},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yang_Qiu_2020_Online_sequential_monitoring_of_spatio-temporal_disease_incidence_rates.pdf}
}

@article{yashchin1993,
  title = {Performance of {{CUSUM Control Schemes}} for {{Serially Correlated Observations}}},
  author = {Yashchin, Emmanuel},
  year = {1993},
  journal = {Technometrics},
  volume = {35},
  number = {1},
  eprint = {1269288},
  eprinttype = {jstor},
  pages = {37--52},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1269288},
  urldate = {2023-04-15},
  abstract = {This article discusses situations in which one is interested in evaluating the run-length characteristics of a cumulative sum control scheme when the underlying data show the presence of serial correlation. In practical applications, situations of this type are common in problems associated with monitoring such characteristics of data as forecasting errors, measures of model adequacy, and variance components. The discussed problem is also relevant in situations in which data transformations are used to reduce the magnitude of serial correlation. The basic idea of analysis involves replacing the sequence of serially correlated observations by a sequence of independent and identically distributed observations for which the run-length characteristics of interest are roughly the same. Applications of the proposed method for several classes of processes arising in the area of statistical process control are discussed in detail, and it is shown that it leads to approximations that can be considered acceptable in many practical situations.},
  keywords = {todo}
}

@article{ye2022,
  title = {Online {{Nonparametric Monitoring}} of {{Heterogeneous Data Streams}} with {{Partial Observations Based}} on {{Thompson Sampling}}},
  author = {Ye, Honghan and Xian, Xiaochen and Cheng, Jing-Ru C. and Hable, Brock and Shannon, Robert W. and Elyaderani, Mojtaba Kadkhodaie and Liu, Kaibo},
  year = {2022},
  journal = {IISE Transactions},
  volume = {0},
  number = {0},
  pages = {1--13},
  publisher = {{Taylor \& Francis}},
  issn = {2472-5854},
  doi = {10.1080/24725854.2022.2039423},
  urldate = {2022-04-07},
  abstract = {With the rapid advancement of sensor technology driven by Internet-of-Things-enabled applications, tremendous amounts of measurements of heterogeneous data streams are frequently acquired for online process monitoring. Such massive data, involving a large number of data streams with high sampling frequency, incur high costs on data collection, transmission, and analysis in practice. As a result, the resource constraint often restricts the data observability to only a subset of data streams at each data acquisition time, posing significant challenges in many online monitoring applications. Unfortunately, existing methods do not provide a general framework for monitoring heterogeneous data streams with partial observations. In this article, we propose a nonparametric monitoring and sampling algorithm to quickly detect abnormalities occurring to heterogeneous data streams. In particular, an approximation framework is incorporated with an antirank-based CUSUM procedure to collectively estimate the underlying status of all data streams based on partially observed data. Furthermore, an intelligent sampling strategy based on Thompson sampling is proposed to dynamically observe the informative data streams and balance between exploration and exploitation to facilitate quick anomaly detection. Theoretical justification of the proposed algorithm is also investigated. Both simulations and case studies are conducted to demonstrate the superiority of the proposed method.},
  keywords = {antirank-based CUSUM procedure,done,Heterogeneous data streams,mean shift detection,partial observations,Thompson sampling},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ye_et_al_2022_Online_nonparametric_monitoring_of_heterogeneous_data_streams_with_partial.pdf;/home/dede/Zotero/storage/S6UZQSUP/24725854.2022.html}
}

@article{yi2019,
  title = {A {{\proglang{MATLAB}}} Toolbox for Data Pre-Processing and Multivariate Statistical Process Control},
  author = {Yi, Gang and Herdsman, Craig and Morris, Julian},
  year = {2019},
  journal = {Chemometrics and Intelligent Laboratory Systems},
  volume = {194},
  pages = {103863},
  issn = {0169-7439},
  doi = {10.1016/j.chemolab.2019.103863},
  urldate = {2023-07-02},
  abstract = {A Multivariate Statistical Data Pre-screening/Data Pre-processing Toolbox (Pre-Screen) has been designed and developed for use by practising process engineers and researchers who wish to pre-process process data prior to multivariate data analysis, process data modelling or building predictive and inferential models. Many commercial data analysis packages do not fully address the initial data cleaning and data conditioning tasks which can consume up to 80\% of the modelling time. The software toolkit has been developed specifically with the aim of focusing on the industrial needs for the initial data pre-screening of large industrial data sets. The core feature of Pre-Screen is that it has been specifically developed to make the analysis of large data sets as fast and visual as possible, and accessible for both process and control engineers, analytical scientists and academic R\&D without taking away the need for engineering science understanding. The toolbox builds on top of the MATLAB numerical computing environment, with powerful user interface procedures providing user friendly, mouse/menu driven software. The toolbox has been complied to allow use by those whom do not have access to MATLAB.},
  langid = {english},
  keywords = {Contribution plots,Data pre-processing,Data pre-screening,Fault diagnosis,GUI,Monitoring,Multivariate control charts,Multivariate data analysis,Multivariate statistical process control,Principal component analysis},
  file = {/home/dede/Zotero/storage/A98TRXMS/S0169743919304551.html}
}

@article{yi2021,
  title = {An Adaptive {{CUSUM}} Chart for Drift Detection},
  author = {Yi, Fan and Qiu, Peihua},
  year = {2021},
  journal = {Quality and Reliability Engineering International},
  volume = {38},
  number = {2},
  pages = {887--894},
  issn = {1099-1638},
  doi = {10.1002/qre.3020},
  urldate = {2022-09-07},
  abstract = {In practice, sequential processes often have gradual changes in their process distributions over time. This is related to the drift detection problem in statistical process control. In the literature, there have been some existing discussions on this problem. But, most existing methods are designed based on the assumption that the related drift is linear or have another specific pattern. In reality, however, such specified patterns may not be valid. In this paper, we suggest an adaptive cumulative sum (CUSUM) chart to handle the drift detection problem with a flexible drift pattern. The new method integrates the general framework to construct a CUSUM chart based on the generalized likelihood ratio statistic and estimation of a shift size by the exponentially weighted least square regression procedure. Simulation studies show that the proposed method is effective in various cases considered. The new method is also illustrated using an example about the exchange rates between Indian Rupees and US Dollars.},
  langid = {english},
  keywords = {adaptive CUSUM chart,done,drift detection,exponentially weighted least square,likelihood ratio,linear drift,statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yi_Qiu_2022_An_adaptive_CUSUM_chart_for_drift_detection.pdf;/home/dede/Zotero/storage/K9F2ICD8/qre.html}
}

@article{you2019,
  title = {Fast Computing for Dynamic Screening Systems When Analyzing Correlated Data},
  author = {You, Lu and Qiu, Peihua},
  year = {2019},
  journal = {Journal of Statistical Computation and Simulation},
  volume = {89},
  number = {3},
  pages = {379--394},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10.1080/00949655.2018.1552273},
  urldate = {2022-09-01},
  abstract = {In practice, we often need to identify individuals whose longitudinal behaviour is different from the behaviour of those well-functioning individuals, so that some unpleasant consequences (e.g. stroke) can be avoided or early detected. To handle such applications, a new statistical method, called dynamic screening system, has been developed in the literature. A recent version of this method can analyze correlated data. However, the computation involved is intensive. In this paper, we suggest a fast computing algorithm for the dynamic screening system. The algorithm can improve the effectiveness of the conventional dynamic screening system in certain cases. Numerical results show that the new algorithm works well in different cases.},
  keywords = {Computing time,correlation,data de-correlation,doing,sequential monitoring,sprint length,statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/You_Qiu_2019_Fast_computing_for_dynamic_screening_systems_when_analyzing_correlated_data.pdf}
}

@article{zang2018b,
  title = {Phase {{I Monitoring}} of {{Spatial Surface Data}} from {{3D Printing}}},
  author = {Zang, Yangyang and Qiu, Peihua},
  year = {2018},
  journal = {Technometrics},
  volume = {60},
  number = {2},
  pages = {169--180},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2017.1321585},
  urldate = {2023-10-10},
  abstract = {In recent years, 3D printing gets more and more popular in manufacturing industries. Quality control of 3D printing products thus becomes an important research problem. However, this problem is challenging due to the facts that (i) the surface of a product from 3D printing can have arbitrary shape, even when the 3D printing process is in-control, (ii) surface observations of the product obtained from a laser scanner may not have regularly spaced locations, and (iii) the overall geometric positions of 3D printing products might be all different, making proper comparison among different products difficult. In this article, we propose a Phase I control chart for monitoring products from 3D printing that addresses all these challenges. Numerical studies show that it works well in practice.},
  keywords = {Kernel smoothing,Nonparametric regression,Profile monitoring,Registration,Statistical process control,Surface estimation}
}

@article{zang2018c,
  title = {Phase {{II}} Monitoring of Free-Form Surfaces: {{An}} Application to {{3D}} Printing},
  shorttitle = {Phase {{II}} Monitoring of Free-Form Surfaces},
  author = {Zang, Yangyang and Qiu, Peihua},
  year = {2018},
  journal = {Journal of Quality Technology},
  volume = {50},
  number = {4},
  pages = {379--390},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2018.1508274},
  urldate = {2023-10-10},
  abstract = {Three-dimensional (3D) printing techniques have become popular in recent years. Monitoring the quality of its products is thus important. In the literature, there is little existing research on this topic now, partly because it is a challenging problem with complex data structures. In this article, we propose a nonparametric control chart for Phase II monitoring of the top surfaces of 3D printing products. The top surfaces are focused in this article because they are our major concern regarding the quality of 3D printing products in some applications. Such surfaces are often free-form surfaces. Our proposed method is based on local kernel estimation of free-form surfaces. Before Phase II monitoring, observed data from different products are first geometrically aligned to account for possible movement between the products and a laser scanner during the data acquisition stage. Numerical studies show that the proposed method works well in practice.},
  keywords = {image registration,local smoothing,nonparametric regression,process monitoring,statistical process control,surface estimation}
}

@article{zeileis2002,
  title = {\pkg{strucchange}: {{An \proglang{R} Package}} for {{Testing}} for {{Structural Change}} in {{Linear Regression Models}}},
  shorttitle = {Strucchange},
  author = {Zeileis, Achim and Leisch, Friedrich and Hornik, Kurt and Kleiber, Christian},
  year = {2002},
  journal = {Journal of Statistical Software},
  volume = {7},
  pages = {1--38},
  issn = {1548-7660},
  doi = {10.18637/jss.v007.i02},
  urldate = {2023-07-01},
  abstract = {This paper reviews tests for structural change in linear regression models from the generalized fluctuation test framework as well as from the F test (Chow test) framework. It introduces a unified approach for implementing these tests and presents how these ideas have been realized in an \proglang{R} package called strucchange. Enhancing the standard significance test approach the package contains methods to fit, plot and test empirical fluctuation processes (like CUSUM, MOSUM and estimates-based processes) and to compute, plot and test sequences of F statistics with the supF , aveF and expF test. Thus, it makes powerful tools available to display information about structural changes in regression relationships and to assess their significance. Furthermore, it is described how incoming data can be monitored.},
  copyright = {Copyright (c) 2001 Achim Zeileis, Friedrich Leisch, Kurt Hornik, Christian Kleiber},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zeileis_et_al_2002_strucchange.pdf}
}

@article{zhang2018,
  title = {Multiple Profiles Sensor-Based Monitoring and Anomaly Detection},
  author = {Zhang, Chen and Yan, Hao and Lee, Seungho and Shi, Jianjun},
  year = {2018},
  journal = {Journal of Quality Technology},
  volume = {50},
  number = {4},
  pages = {344--362},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2018.1508275},
  urldate = {2021-10-20},
  abstract = {Generally, in an advanced manufacturing system hundreds of sensors are deployed to measure key process variables in real time. Thus it is desirable to develop methodologies to use real-time sensor data for on-line system condition monitoring and anomaly detection. However, there are several challenges in developing an effective process monitoring system: (i) data streams generated by multiple sensors are high-dimensional profiles; (ii) sensor signals are affected by noise due to system-inherent variations; (iii) signals of different sensors have cluster-wise features; and (iv) an anomaly may cause only sparse changes of sensor signals. To address these challenges, this article presents a real-time multiple profiles sensor-based process monitoring system, which includes the following modules: (i) preprocessing sensor signals to remove inherent variations and conduct profile alignments, (ii) using multichannel functional principal component analysis (MFPCA)\textendash based methods to extract sensor features by considering cluster-wise between-sensor correlations, and (iii) constructing a monitoring scheme with the top-R strategy based on the extracted features, which has scalable detection power for different fault patterns. Finally, we implement and demonstrate the proposed framework using data from a real manufacturing system.},
  keywords = {data fusion,done,functional PCA,multichannel profile monitoring,statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhang_et_al_2018_Multiple_profiles_sensor-based_monitoring_and_anomaly_detection.pdf;/home/dede/Zotero/storage/DSGKKTLH/00224065.2018.html}
}

@article{zhao2005,
  title = {Dual {{CUSUM}} Control Schemes for Detecting a Range of Mean Shifts},
  author = {Zhao, Yi and Tsung, Fugee and Wang, Zhaojun},
  year = {2005},
  journal = {IIE Transactions},
  volume = {37},
  number = {11},
  pages = {1047--1057},
  publisher = {{Taylor \& Francis}},
  issn = {0740-817X},
  doi = {10.1080/07408170500232321},
  urldate = {2023-10-20},
  abstract = {Conventional quality control procedures, such as the CUmulative SUM (CUSUM) and exponentially weighted moving average charts are usually designed based on a mean shift with a given size. In practice, the exact value of the shift size is often unknown and can only be reasonably assumed to vary within a certain range. Such a range of shifts deteriorates the performance of existing control charts. In this paper, a quality control scheme, a Dual CUSUM (DCUSUM), is applied that combines two CUSUM charts to detect the range of shifts. The out-of-control signal is triggered if either one of the CUSUM statistics goes out of the DCUSUM control limits. In particular, a design procedure for the DCUSUM charts is developed and an analytical formula for the Average Run Length (ARL) calculation is obtained via the Markov chain method. The proposed DCUSUM charts are compared with the conventional CUSUM and combined Shewhart-CUSUM charts. Based on a proposed criterion, the integrated relative ARL, the proposed schemes show better performance in detecting a range of mean shifts.}
}

@article{zhao2021,
  title = {An {{Intrinsic Geometrical Approach}} for {{Statistical Process Control}} of {{Surface}} and {{Manifold Data}}},
  author = {Zhao, Xueqi and {del Castillo}, Enrique},
  year = {2021},
  journal = {Technometrics},
  volume = {63},
  number = {3},
  pages = {295--312},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2020.1772114},
  urldate = {2021-10-18},
  abstract = {We present a new method for statistical process control (SPC) of a discrete part manufacturing system based on intrinsic geometrical properties of the parts, estimated from three-dimensional sensor data. An intrinsic method has the computational advantage of avoiding the difficult part registration problem, necessary in previous SPC approaches of three-dimensional geometrical data, but inadequate if noncontact sensors are used. The approach estimates the spectrum of the Laplace\textendash Beltrami (LB) operator of the scanned parts and uses a multivariate nonparametric control chart for online process control. Our proposal brings SPC closer to computer vision and computer graphics methods aimed to detect large differences in shape (but not in size). However, the SPC problem differs in that small changes in either shape or size of the parts need to be detected, keeping a controllable false alarm rate and without completely filtering noise. An online or ``Phase II'' method and a scheme for starting up in the absence of prior data (``Phase I'') are presented. Comparison with earlier approaches that require registration shows the LB spectrum method to be more sensitive to rapidly detect small changes in shape and size, including the practical case when the sequence of part datasets is in the form of large, unequal size meshes. A post-alarm diagnostic method to investigate the location of defects on the surface of a part is also presented. While we focus in this article on surface (triangulation) data, the methods can also be applied to point cloud and voxel metrology data.},
  keywords = {Differential geometry,doing,Laplace\textendash Beltrami operator,Noncontact sensor,Permutation test,Spectral method},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhao_del_Castillo_2021_An_Intrinsic_Geometrical_Approach_for_Statistical_Process_Control_of_Surface.pdf;/home/dede/Zotero/storage/QD82JLZ9/00401706.2020.html}
}

@phdthesis{zhao2022a,
  title = {Intrinsic {{Geometrical Methods}} for {{Statistical Process Control}} of {{Complex Data Objects}}},
  author = {Zhao, Xueqi},
  year = {2022},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhao_2022_Intrinsic_Geometrical_Methods_for_Statistical_Process_Control_of_Complex_Data.pdf}
}

@article{zhao2022b,
  title = {Geometrical Deviation Modeling and Monitoring of {{3D}} Surface Based on Multi-Output {{Gaussian}} Process},
  author = {Zhao, Chen and Lv, Jun and Du, Shichang},
  year = {2022},
  journal = {Measurement},
  volume = {199},
  pages = {111569},
  issn = {0263-2241},
  doi = {10.1016/j.measurement.2022.111569},
  urldate = {2023-10-13},
  abstract = {Geometrical deviation is an important factor in determining the quality of a three-dimensional (3D) Surface. For 3D surfaces with complex shapes, the high-definition measurement (HDM) technology can provide detailed information on surface topography, which inspired new challenges in characterizing, modeling, and monitoring geometrical deviations. This paper proposes a spherical multi-output Gaussian process (S-MOGP) method to model and monitor 3D surfaces. Firstly, the surface in the 3D coordinate system is mapped to the spherical 2D parameter domain. Secondly, a state equation based on the multi-output Gaussian process is established to model the 3D surface. Finally, statistics are calculated and control charts are presented to monitor the geometrical deviation. The results of simulations and a case study show that the proposed method can effectively model 3D surfaces and monitor the geometrical deviations.},
  keywords = {Geometrical deviation,Multi-output Gaussian process,Spherical mapping,Three-dimensional surface,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhao_et_al_2022_Geometrical_deviation_modeling_and_monitoring_of_3D_surface_based_on.pdf;/home/dede/Zotero/storage/LCMVJHWQ/S0263224122007849.html}
}

@article{zhou2003,
  title = {State {{Space Modeling}} of {{Dimensional Variation Propagation}} in {{Multistage Machining Process Using Differential Motion Vectors}}},
  author = {Zhou, Shiyu and Huang, Qiang and Shi, Jianjun},
  year = {2003},
  journal = {IEEE Transactions on Robotics and Automation},
  volume = {19},
  number = {2},
  pages = {296--309},
  issn = {2374-958X},
  doi = {10.1109/TRA.2003.808852},
  abstract = {In this paper, a state space model is developed to describe the dimensional variation propagation of multistage machining processes. A complicated machining system usually contains multiple stages. When the workpiece passes through multiple stages, machining errors at each stage will be accumulated and transformed onto the workpiece. Differential motion vector, a concept from the robotics field, is used in this model as the state vector to represent the geometric deviation of the workpiece. The deviation accumulation and transformation are quantitatively described by the state transition in the state space model. A systematic procedure that builds the model is presented and an experimental validation is also conducted. The validation result is satisfactory. This model has great potential to be applied to fault diagnosis and process design evaluation for complicated machining processes.},
  keywords = {Fault diagnosis,Fixtures,Machining,Orbital robotics,Process design,Robot kinematics,Solid modeling,State-space methods,Symmetric matrices,Thermal force,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhou_et_al_2003_State_space_modeling_of_dimensional_variation_propagation_in_multistage.pdf;/home/dede/Zotero/storage/BS5CJVXX/1192159.html}
}

@article{zhu2007,
  title = {On-Line {{HPLC}} Combined with Multivariate Statistical Process Control for the Monitoring of Reactions},
  author = {Zhu, Lifeng and Brereton, Richard G. and Thompson, Duncan R. and Hopkins, Paul L. and Escott, Richard E. A.},
  year = {2007},
  journal = {Analytica Chimica Acta},
  volume = {584},
  number = {2},
  pages = {370--378},
  issn = {1873-4324},
  doi = {10.1016/j.aca.2006.11.045},
  abstract = {On-line high performance liquid chromatography is used to monitor a steady state reaction over 35.2 h, with 197 chromatograms recorded as the reaction progresses. For each chromatogram, peaks are detected, baseline corrected, aligned and integrated to provide a peak table consisting of the intensities of 19 peaks, two corresponding to the reactants, one to the product and one to the solvent, the remaining being impurities, by-products or intermediates. D-charts and Q-charts from multivariate statistical process control are applied to the data to determine which samples are out of control and also provide diagnostic insight into why these samples are problematic. The D-chart is best at looking at overall performance issues such as problems with mixing or difficulties with instrument operation, whereas the Q-charts are best at detecting impurities during the reaction.},
  langid = {english},
  pmid = {17386627},
  keywords = {{Chromatography, High Pressure Liquid},{Factor Analysis, Statistical},Online Systems}
}

@article{zhu2013,
  title = {\pkg{edcc}: {{An \proglang{R} Package}} for the {{Economic Design}} of the {{Control Chart}}},
  shorttitle = {Edcc},
  author = {Zhu, Weicheng and Park, Changsoon},
  year = {2013},
  journal = {Journal of Statistical Software},
  volume = {52},
  pages = {1--24},
  issn = {1548-7660},
  doi = {10.18637/jss.v052.i09},
  urldate = {2023-10-18},
  abstract = {The basic purpose of the economic design of the control charts is to find the optimum control charts parameters to minimize the process cost. In this paper, an \proglang{R} package, edcc (economic design of control charts), which provides a numerical method to find the optimum chart parameters is presented using the unified approach of the economic design. Also, some examples are given to illustrate how to use this package. The types of the control chart available in the edcc package are X?, CUSUM (cumulative sum), and EWMA (exponentially-weighted moving average) control charts.},
  copyright = {Copyright (c) 2012 Weicheng Zhu, Changsoon Park},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhu_Park_2013_edcc.pdf}
}

@article{zou2006,
  title = {The {{Adaptive Lasso}} and {{Its Oracle Properties}}},
  author = {Zou, Hui},
  year = {2006},
  journal = {Journal of the American Statistical Association},
  volume = {101},
  number = {476},
  pages = {1418--1429},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214506000000735},
  urldate = {2023-10-10},
  abstract = {The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the {$\mathscr{l}$}1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection.},
  keywords = {Asymptotic normality,Lasso,Minimax,Oracle inequality,Oracle procedure,Variable selection},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zou_2006_The_Adaptive_Lasso_and_Its_Oracle_Properties.pdf}
}

@article{zou2008,
  title = {Monitoring {{Profiles Based}} on {{Nonparametric Regression Methods}}},
  author = {Zou, Changliang and Tsung, Fugee and Wang, Zhaojun},
  year = {2008},
  journal = {Technometrics},
  volume = {50},
  number = {4},
  pages = {512--526},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/004017008000000433},
  urldate = {2020-12-09},
  abstract = {The use statistical process control (SPC) in monitoring and diagnosis of process and product quality profiles remains an important problem in various manufacturing industries. The SPC problem with a nonlinear profile is particularly challenging. This article proposes a novel scheme to monitor changes in both the regression relationship and the variation of the profile online. It integrates the multivariate exponentially weighted moving average procedure with the generalized likelihood ratio test based on nonparametric regression. The proposed scheme not only provides an effective SPC solution to handle nonlinear profiles, which are common in industrial practice, but it also resolves the latent problem in popular parametric monitoring methods of being unable to detect certain types of changes due to a misspecified, out-of-control model. Our simulation results demonstrate the effectiveness and efficiency of the proposed monitoring scheme. In addition, a systematic diagnostic approach is provided to locate the change point of the process and identify the type of change in the profile. Finally, a deep reactive ion-etching example from semiconductor manufacturing is used to illustrate the implementation of the proposed monitoring and diagnostic approach.},
  keywords = {done,Exponentially weighted moving average,Generalized likelihood ratio test,Lack-of-fit test,Local linear smoother,Nonlinear profile,Statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zou_et_al_2008_Monitoring_Profiles_Based_on_Nonparametric_Regression_Methods.pdf;/home/dede/Zotero/storage/JX4DANLV/004017008000000433.html}
}

@article{zou2010,
  title = {Likelihood {{Ratio-Based Distribution-Free EWMA Control Charts}}},
  author = {Zou, Changliang and Tsung, Fugee},
  year = {2010},
  journal = {Journal of Quality Technology},
  volume = {42},
  number = {2},
  pages = {174--196},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2010.11917815},
  urldate = {2023-09-29},
  abstract = {Nonparametric or distribution-free charts are useful in statistical process control when there is a lack of or limited knowledge about the underlying process distribution. Most existing approaches in the literature are for monitoring location parameters. They may not be effective with a change of distribution over time in many applications. This paper develops a new distribution-free control chart based on the integration of a powerful nonparametric goodness-of-fit test and the exponentially weighted moving-average (EWMA) control scheme. Benefiting from certain desirable properties of the test and the proposed charting statistic, our proposed control chart is computationally fast, convenient to use, and efficient in detecting potential shifts in location, scale, and shape. Thus, it offers robust protection against variation in various underlying distributions. Numerical studies and a real-data example show that the proposed approaches are quite effective in industrial applications, particularly in start-up and short-run situations.},
  keywords = {Anderson-Darling Test,Change Point,Goodness of Fit,Self-Starting,Statistical Process Control,todo,Weighted Empirical Distribution}
}

@article{zou2011a,
  title = {A {{Multivariate Sign EWMA Control Chart}}},
  author = {Zou, Changliang and Tsung, Fugee},
  year = {2011},
  journal = {Technometrics},
  volume = {53},
  number = {1},
  pages = {84--97},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/TECH.2010.09095},
  urldate = {2023-09-19},
  abstract = {Nonparametric control charts are useful in statistical process control (SPC) when there is a lack of or limited knowledge about the underlying process distribution, especially when the process measurement is multivariate. This article develops a new multivariate SPC methodology for monitoring location parameters. It is based on adapting a powerful multivariate sign test to online sequential monitoring. The weighted version of the sign test is used to formulate the charting statistic by incorporating the exponentially weighted moving average control (EWMA) scheme, which results in a nonparametric counterpart of the classical multivariate EWMA (MEWMA). It is affine-invariant and has a strictly distribution-free property over a broad class of population models. That is, the in-control (IC) run length distribution can attain (or is always very close to) the nominal one when using the same control limit designed for a multivariate normal distribution. Moreover, when the process distribution comes from the elliptical direction class, the IC average run length can be calculated via a one-dimensional Markov chain model. This control chart possesses some other favorable features: it is fast to compute with a similar computational effort to the MEWMA chart; it is easy to implement because only the multivariate median and the associated transformation matrix need to be specified (estimated) from the historical data before monitoring; it is also very efficient in detecting process shifts, particularly small or moderate shifts when the process distribution is heavy tailed or skewed. Two real-data examples from manufacturing show that it performs quite well in applications. This article has supplementary material online.},
  keywords = {Affine invariant,Distribution free,MEWMA,Multivariate median,Nonparametric procedure,Robustness,Statistical process control,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zou_Tsung_2011_A_Multivariate_Sign_EWMA_Control_Chart.pdf}
}

@article{zou2012a,
  title = {A {{Spatial Rank-Based Multivariate EWMA Control Chart}}},
  author = {Zou, Changliang and Wang, Zhaojun and Tsung, Fugee},
  year = {2012},
  journal = {Naval Research Logistics (NRL)},
  volume = {59},
  number = {2},
  pages = {91--110},
  issn = {1520-6750},
  doi = {10.1002/nav.21475},
  urldate = {2023-09-19},
  abstract = {Nonparametric control charts are useful in statistical process control when there is a lack of or limited knowledge about the underlying process distribution, especially when the process measurement is multivariate. This article develops a new multivariate self-starting methodology for monitoring location parameters. It is based on adapting the multivariate spatial rank to on-line sequential monitoring. The weighted version of the rank-based test is used to formulate the charting statistic by incorporating the exponentially weighted moving average control scheme. It is robust to non-normally distributed data, easy to construct, fast to compute and also very efficient in detecting multivariate process shifts, especially small or moderate shifts which occur when the process distribution is heavy-tailed or skewed. As it avoids the need for a lengthy data-gathering step before charting and it does not require knowledge of the underlying distribution, the proposed control chart is particularly useful in start-up or short-run situations. A real-data example from white wine production processes shows that it performs quite well. \textcopyright{} 2012 Wiley Periodicals, Inc. Naval Research Logistics 59: 91\textendash 110, 2012},
  copyright = {Copyright \textcopyright{} 2012 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {distribution-free,multivariate EWMA,nonparametric procedure,robustness,self-starting,spatial rank,statistical process control,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zou_et_al_2012_A_spatial_rank-based_multivariate_EWMA_control_chart.pdf;/home/dede/Zotero/storage/AESW53X5/nav.html}
}

@book{shumway2017,
  title = {Time {{Series Analysis}} and {{Its Applications}}: {{With \proglang{R} Examples}}},
  shorttitle = {Time {{Series Analysis}} and {{Its Applications}}},
  author = {Shumway, Robert H. and Stoffer, David S.},
  year = {2017},
  edition = {4th ed.},
  publisher = {{Springer-Verlag}},
  address = {{New York, NY}},
  abstract = {The fourth edition of this popular graduate textbook, like its predecessors, presents a balanced and comprehensive treatment of both time and frequency domain methods with accompanying theory. Numerous examples using nontrivial data illustrate solutions to problems such as discovering natural and anthropogenic climate change, evaluating pain perception experiments using functional magnetic resonance imaging, and monitoring a nuclear test ban treaty.The book is designed as a textbook for graduate level students in the physical, biological, and social sciences and as a graduate level text in statistics. Some parts may also serve as an undergraduate introductory course. Theory and methodology are separated to allow presentations on different levels. In addition to coverage of classical methods of time series regression, ARIMA models, spectral analysis and state-space models, the text includes modern developments including categorical time series analysis, multivariate spectral methods, long memory series, nonlinear models, resampling techniques, GARCH models, ARMAX models, stochastic volatility, wavelets, and Markov chain Monte Carlo integration methods.This edition includes R code for each numerical example in addition to Appendix R, which provides a reference for the data sets and R scripts used in the text in addition to a tutorial on basic R commands and R time series.~An additional file is available on the book's website for download, making all the data sets and scripts easy to load into R.},
  isbn = {978-3-319-52451-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shumway_Stoffer_2017_Time_Series_Analysis_and_Its_Applications.pdf}
}

@InCollection{capizzi2018,
  title = {Phase I Distribution-Free Analysis with the {R} Package {dfphase1}},
  author = {Giovanna Capizzi and Guido Masarotto},
  booktitle = {Frontiers in Statistical Quality Control 12},
  year = {2018},
  editor = {{Knoth} and {Sven} and {Schmid} and {Wolfgang}},
  publisher = {Springer},
  pages = {3--19},
  doi = {10.1007/978-3-319-75295-2_1},
}

